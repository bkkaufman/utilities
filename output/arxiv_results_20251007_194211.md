# arXiv Search Results

**Search Query:** `("foundation model" OR "agentic" OR "multi-agent" OR "autonomous system" OR "RAG" OR "retrieval" OR "pharmacovigilance" OR "medical AI") AND (cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.HC OR cat:cs.IR OR cat:cs.DC OR cat:stat.ML)`

**Date Range:** Papers published after 2025-08-08

**Generated:** 2025-10-07 19:42:12

**Total Papers Found:** 100

---

## 1. Paper2Video: Automatic Video Generation from Scientific Papers

**Authors:** Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI, cs.CL, cs.MA, cs.MM

**PDF:** [http://arxiv.org/pdf/2510.05096v1](http://arxiv.org/pdf/2510.05096v1)

**Abstract:**

Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

---

## 2. Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces

**Authors:** Debojyoti Ghosh, Soumya K Ghosh, Adrijit Goswami

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.05071v1](http://arxiv.org/pdf/2510.05071v1)

**Abstract:**

Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

---

## 3. Multi-Agent Distributed Optimization With Feasible Set Privacy

**Authors:** Shreya Meel, Sennur Ulukus

**Published:** 2025-10-06

**Categories:** cs.IT, cs.CR, cs.DC, cs.NI, eess.SP, math.IT

**PDF:** [http://arxiv.org/pdf/2510.05068v1](http://arxiv.org/pdf/2510.05068v1)

**Abstract:**

We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.

---

## 4. Staircase Streaming for Low-Latency Multi-Agent Inference

**Authors:** Junlin Wang, Jue Wang, Zhen, Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05059v1](http://arxiv.org/pdf/2510.05059v1)

**Abstract:**

Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

---

## 5. SegMASt3R: Geometry Grounded Segment Matching

**Authors:** Rohit Jayanti, Swayam Agrawal, Vansh Garg, Siddharth Tourani, Muhammad Haris Khan, Sourav Garg, Madhava Krishna

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.05051v1](http://arxiv.org/pdf/2510.05051v1)

**Abstract:**

Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

---

## 6. Look-ahead Reasoning with a Learned Model in Imperfect Information Games

**Authors:** Ondřej Kubíček, Viliam Lisý

**Published:** 2025-10-06

**Categories:** cs.AI, cs.GT

**PDF:** [http://arxiv.org/pdf/2510.05048v1](http://arxiv.org/pdf/2510.05048v1)

**Abstract:**

Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

---

## 7. Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization

**Authors:** Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05038v1](http://arxiv.org/pdf/2510.05038v1)

**Abstract:**

Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

---

## 8. Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad

**Authors:** Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun

**Published:** 2025-10-06

**Categories:** astro-ph.IM, cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05016v1](http://arxiv.org/pdf/2510.05016v1)

**Abstract:**

While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

---

## 9. Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning

**Authors:** Imran Mansha

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05003v1](http://arxiv.org/pdf/2510.05003v1)

**Abstract:**

Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

---

## 10. LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game

**Authors:** Fangzhou Liang, Tianshi Zheng, Chunkit Chan, Yauwai Yim, Yangqiu Song

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04980v1](http://arxiv.org/pdf/2510.04980v1)

**Abstract:**

Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

---

## 11. ActiveMark: on watermarking of visual foundation models via massive activations

**Authors:** Anna Chistyakova, Mikhail Pautov

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04966v1](http://arxiv.org/pdf/2510.04966v1)

**Abstract:**

Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

---

## 12. Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits

**Authors:** Ailiya Borjigin, Cong He

**Published:** 2025-10-06

**Categories:** cs.AI, cs.DC

**PDF:** [http://arxiv.org/pdf/2510.04952v1](http://arxiv.org/pdf/2510.04952v1)

**Abstract:**

We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

---

## 13. MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning

**Authors:** Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04935v1](http://arxiv.org/pdf/2510.04935v1)

**Abstract:**

Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

---

## 14. Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context

**Authors:** Ngeyen Yinkfu, Sunday Nwovu, Jonathan Kayizzi, Angelique Uwamahoro

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04912v1](http://arxiv.org/pdf/2510.04912v1)

**Abstract:**

In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

---

## 15. Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches

**Authors:** Yicheng Tao, Yao Qin, Yepang Liu

**Published:** 2025-10-06

**Categories:** cs.SE, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04905v1](http://arxiv.org/pdf/2510.04905v1)

**Abstract:**

Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

---

## 16. Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects

**Authors:** Jonathan Colaço Carr, Qinyi Sun, Cameron Allen

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04901v1](http://arxiv.org/pdf/2510.04901v1)

**Abstract:**

Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

---

## 17. HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks

**Authors:** Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson

**Published:** 2025-10-06

**Categories:** cs.RO, cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04898v1](http://arxiv.org/pdf/2510.04898v1)

**Abstract:**

Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

---

## 18. Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models

**Authors:** Alina Ermilova, Dmitrii Kornilov, Sofia Samoilova, Ekaterina Laptenkova, Anastasia Kolesnikova, Ekaterina Podplutova, Senotrusova Sofya, Maksim G. Sharaev

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04888v1](http://arxiv.org/pdf/2510.04888v1)

**Abstract:**

Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

---

## 19. Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution

**Authors:** Adi Banerjee, Anirudh Nair, Tarik Borogovac

**Published:** 2025-10-06

**Categories:** cs.AI, cs.MA

**PDF:** [http://arxiv.org/pdf/2510.04886v1](http://arxiv.org/pdf/2510.04886v1)

**Abstract:**

Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

---

## 20. RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection

**Authors:** Yuxin Wen, Arman Zharmagambetov, Ivan Evtimov, Narine Kokhlikyan, Tom Goldstein, Kamalika Chaudhuri, Chuan Guo

**Published:** 2025-10-06

**Categories:** cs.CR, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04885v1](http://arxiv.org/pdf/2510.04885v1)

**Abstract:**

Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

---

## 21. Video Game Level Design as a Multi-Agent Reinforcement Learning Problem

**Authors:** Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius

**Published:** 2025-10-06

**Categories:** cs.AI, cs.LG, cs.MA, cs.NE

**PDF:** [http://arxiv.org/pdf/2510.04862v1](http://arxiv.org/pdf/2510.04862v1)

**Abstract:**

Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

---

## 22. A Clinical-grade Universal Foundation Model for Intraoperative Pathology

**Authors:** Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04861v1](http://arxiv.org/pdf/2510.04861v1)

**Abstract:**

Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

---

## 23. Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails

**Authors:** Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04860v1](http://arxiv.org/pdf/2510.04860v1)

**Abstract:**

As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

---

## 24. FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration

**Authors:** Victor May, Diganta Misra, Yanqi Luo, Anjali Sridhar, Justine Gehring, Silvio Soares Ribeiro Junior

**Published:** 2025-10-06

**Categories:** cs.SE, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04852v1](http://arxiv.org/pdf/2510.04852v1)

**Abstract:**

AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

---

## 25. LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation

**Authors:** Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor Rühle, Saravan Rajmohan

**Published:** 2025-10-06

**Categories:** cs.AI, cs.LG, cs.MA

**PDF:** [http://arxiv.org/pdf/2510.04851v1](http://arxiv.org/pdf/2510.04851v1)

**Abstract:**

We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

---

## 26. When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA

**Authors:** Elisei Rykov, Kseniia Petrushina, Maksim Savkin, Valerii Olisov, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, Julia Belikova

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04849v1](http://arxiv.org/pdf/2510.04849v1)

**Abstract:**

Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

---

## 27. Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning

**Authors:** Abhinav Madahar

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04817v1](http://arxiv.org/pdf/2510.04817v1)

**Abstract:**

Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

---

## 28. A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation

**Authors:** Alon Kaya, Igal Bilik, Inna Stainvas

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04794v1](http://arxiv.org/pdf/2510.04794v1)

**Abstract:**

Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

---

## 29. Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading

**Authors:** Zifan Song, Kaitao Song, Guosheng Hu, Ding Qi, Junyao Gao, Xiaohua Wang, Dongsheng Li, Cairong Zhao

**Published:** 2025-10-06

**Categories:** cs.MA, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04787v1](http://arxiv.org/pdf/2510.04787v1)

**Abstract:**

Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.

---

## 30. Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning

**Authors:** Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, Moritz Hardt

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04786v1](http://arxiv.org/pdf/2510.04786v1)

**Abstract:**

Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

---

## 31. Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge

**Authors:** Max Kirchner, Hanna Hoffmann, Alexander C. Jenke, Oliver L. Saldanha, Kevin Pfeiffer, Weam Kanjo, Julia Alekseenko, Claas de Boer, Santhi Raj Kolamuri, Lorenzo Mazza, Nicolas Padoy, Sophia Bano, Annika Reinke, Lena Maier-Hein, Danail Stoyanov, Jakob N. Kather, Fiona R. Kolbinger, Sebastian Bodenstedt, Stefanie Speidel

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04772v1](http://arxiv.org/pdf/2510.04772v1)

**Abstract:**

Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

---

## 32. When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates

**Authors:** Michele Caprio, Siu Lun Chau, Krikamol Muandet

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, math.PR, math.ST, stat.ML, stat.TH, Primary: 54H25, Secondary: 68T05, 68T37

**PDF:** [http://arxiv.org/pdf/2510.04769v1](http://arxiv.org/pdf/2510.04769v1)

**Abstract:**

Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

---

## 33. LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0

**Authors:** Jinbo Wen, Jiawen Kang, Linfeng Zhang, Xiaoying Tang, Jianhang Tang, Yang Zhang, Zhaohui Yang, Dusit Niyato

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04765v1](http://arxiv.org/pdf/2510.04765v1)

**Abstract:**

Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

---

## 34. ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever

**Authors:** Eduardo Martínez Rivera, Filippo Menolascina

**Published:** 2025-10-06

**Categories:** cs.CL, q-bio.QM

**PDF:** [http://arxiv.org/pdf/2510.04757v1](http://arxiv.org/pdf/2510.04757v1)

**Abstract:**

Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

---

## 35. ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts

**Authors:** Mehdi Houshmand Sarkhoosh, Frøy Øye, Henrik Nestor Sørlie, Nam Hoang Vu, Dag Johansen, Cise Midoglu, Tomas Kupka, Pål Halvorsen

**Published:** 2025-10-06

**Categories:** cs.CV, cs.MM

**PDF:** [http://arxiv.org/pdf/2510.04739v1](http://arxiv.org/pdf/2510.04739v1)

**Abstract:**

Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

---

## 36. BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs

**Authors:** Ivo Petrov, Jasper Dekoninck, Martin Vechev

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04721v1](http://arxiv.org/pdf/2510.04721v1)

**Abstract:**

Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

---

## 37. ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion

**Authors:** Foivos Paraperas Papantoniou, Stefanos Zafeiriou

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04706v1](http://arxiv.org/pdf/2510.04706v1)

**Abstract:**

Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

---

## 38. Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI

**Authors:** Quang-Khai Bui-Tran, Minh-Toan Dinh, Thanh-Huy Nguyen, Ba-Thinh Lam, Mai-Anh Vu, Ulas Bagci

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04705v1](http://arxiv.org/pdf/2510.04705v1)

**Abstract:**

Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

---

## 39. Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents

**Authors:** Yiding Wang, Zhepei Wei, Xinyu Zhu, Yu Meng

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04695v1](http://arxiv.org/pdf/2510.04695v1)

**Abstract:**

Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

---

## 40. Multi-Agent Tool-Integrated Policy Optimization

**Authors:** Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04678v1](http://arxiv.org/pdf/2510.04678v1)

**Abstract:**

Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

---

## 41. Watch and Learn: Learning to Use Computers from Online Videos

**Authors:** Chan Hee Song, Yiwen Song, Palash Goyal, Yu Su, Oriana Riva, Hamid Palangi, Tomas Pfister

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04673v1](http://arxiv.org/pdf/2510.04673v1)

**Abstract:**

Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

---

## 42. EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents

**Authors:** Buyuan Zhu, Shiyu Hu, Yiping Ma, Yuanming Zhang, Kang Hao Cheong

**Published:** 2025-10-06

**Categories:** cs.CV, cs.CY

**PDF:** [http://arxiv.org/pdf/2510.04648v1](http://arxiv.org/pdf/2510.04648v1)

**Abstract:**

As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

---

## 43. QuantAgents: Towards Multi-agent Financial System via Simulated Trading

**Authors:** Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04643v1](http://arxiv.org/pdf/2510.04643v1)

**Abstract:**

In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

---

## 44. Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents

**Authors:** Zeyi Zhang, Yanju Zhou, Heyuan Yao, Tenglong Ao, Xiaohang Zhan, Libin Liu

**Published:** 2025-10-06

**Categories:** cs.GR, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04637v1](http://arxiv.org/pdf/2510.04637v1)

**Abstract:**

We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

---

## 45. Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs

**Authors:** Lukas Gienapp, Martin Potthast, Harrisen Scells, Eugene Yang

**Published:** 2025-10-06

**Categories:** cs.IR

**PDF:** [http://arxiv.org/pdf/2510.04633v1](http://arxiv.org/pdf/2510.04633v1)

**Abstract:**

The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

---

## 46. Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry

**Authors:** Anastasia Zhukova, Jonas Lührs, Christian E. Matt, Bela Gipp

**Published:** 2025-10-06

**Categories:** cs.CL, cs.IR

**PDF:** [http://arxiv.org/pdf/2510.04631v1](http://arxiv.org/pdf/2510.04631v1)

**Abstract:**

Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

---

## 47. Compressed Concatenation of Small Embedding Models

**Authors:** Mohamed Ayoub Ben Ayad, Michael Dinzinger, Kanishka Ghosh Dastidar, Jelena Mitrovic, Michael Granitzer

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04626v1](http://arxiv.org/pdf/2510.04626v1)

**Abstract:**

Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

---

## 48. Fairness in Repeated Matching: A Maximin Perspective

**Authors:** Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh

**Published:** 2025-10-06

**Categories:** cs.GT, cs.AI, cs.LG, cs.MA, econ.TH

**PDF:** [http://arxiv.org/pdf/2510.04624v1](http://arxiv.org/pdf/2510.04624v1)

**Abstract:**

We study a sequential decision-making model where a set of items is
repeatedly matched to the same set of agents over multiple rounds. The
objective is to determine a sequence of matchings that either maximizes the
utility of the least advantaged agent at the end of all rounds (optimal) or at
the end of every individual round (anytime optimal). We investigate the
computational challenges associated with finding (anytime) optimal outcomes and
demonstrate that these problems are generally computationally intractable.
However, we provide approximation algorithms, fixed-parameter tractable
algorithms, and identify several special cases whereby the problem(s) can be
solved efficiently. Along the way, we also establish characterizations of
Pareto-optimal/maximum matchings, which may be of independent interest to works
in matching theory and house allocation.

---

## 49. MedPAO: A Protocol-Driven Agent for Structuring Medical Reports

**Authors:** Shrish Shrinath Vaidya, Gowthamaan Palani, Sidharth Ramesh, Velmurugan Balasubramanian, Minmini Selvam, Gokulraja Srinivasaraja, Ganapathy Krishnamurthi

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04623v1](http://arxiv.org/pdf/2510.04623v1)

**Abstract:**

The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

---

## 50. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models

**Authors:** Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04618v1](http://arxiv.org/pdf/2510.04618v1)

**Abstract:**

Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

---

## 51. A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents

**Authors:** Yuan Wang, Mingyu Li, Haibo Chen

**Published:** 2025-10-06

**Categories:** cs.OS, cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04607v1](http://arxiv.org/pdf/2510.04607v1)

**Abstract:**

Computer-use agents (CUAs) powered by large language models (LLMs) have
emerged as a promising approach to automating computer tasks, yet they struggle
with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to
decompose high-level goals into lengthy, error-prone sequences of fine-grained
actions, resulting in low success rates and an excessive number of LLM calls.
  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms
existing GUIs into three declarative primitives: access, state, and
observation, which are better suited for LLMs. Our key idea is policy-mechanism
separation: LLMs focus on high-level semantic planning (policy) while GOI
handles low-level navigation and interaction (mechanism). GOI does not require
modifying the application source code or relying on application programming
interfaces (APIs).
  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on
Windows. Compared to a leading GUI-based agent baseline, GOI improves task
success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI
completes over 61% of successful tasks with a single LLM call.

---

## 52. Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma

**Authors:** Shurui Li

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04588v1](http://arxiv.org/pdf/2510.04588v1)

**Abstract:**

Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

---

## 53. Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior

**Authors:** Sheng Wang, Ruiming Wu, Charles Herndon, Yihang Liu, Shunsuke Koga, Jeanne Shen, Zhi Huang

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04587v1](http://arxiv.org/pdf/2510.04587v1)

**Abstract:**

Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

---

## 54. COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context

**Authors:** Naman Gupta, Shreeyash Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna B Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, Shachee Mishra Gupta

**Published:** 2025-10-06

**Categories:** cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04568v1](http://arxiv.org/pdf/2510.04568v1)

**Abstract:**

Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

---

## 55. GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning

**Authors:** Weishuo Ma, Yanbo Wang, Xiyuan Wang, Lei Zou, Muhan Zhang

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04567v1](http://arxiv.org/pdf/2510.04567v1)

**Abstract:**

Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

---

## 56. Conditional Representation Learning for Customized Tasks

**Authors:** Honglin Liu, Chao Sun, Peng Hu, Yunfan Li, Xi Peng

**Published:** 2025-10-06

**Categories:** cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04564v1](http://arxiv.org/pdf/2510.04564v1)

**Abstract:**

Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

---

## 57. ContextNav: Towards Agentic Multimodal In-Context Learning

**Authors:** Honghao Fu, Yuan Ouyang, Kai-Wei Chang, Yiwei Wang, Zi Huang, Yujun Cai

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04560v1](http://arxiv.org/pdf/2510.04560v1)

**Abstract:**

Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

---

## 58. Fine-grained auxiliary learning for real-world product recommendation

**Authors:** Mario Almagro, Diego Ortego, David Jimenez

**Published:** 2025-10-06

**Categories:** cs.CL, cs.IR

**PDF:** [http://arxiv.org/pdf/2510.04551v1](http://arxiv.org/pdf/2510.04551v1)

**Abstract:**

Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

---

## 59. TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use

**Authors:** Pengfei He, Zhenwei Dai, Bing He, Hui Liu, Xianfeng Tang, Hanqing Lu, Juanhui Li, Jiayuan Ding, Subhabrata Mukherjee, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04550v1](http://arxiv.org/pdf/2510.04550v1)

**Abstract:**

Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

---

## 60. Post-training quantization of vision encoders needs prefixing registers

**Authors:** Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee

**Published:** 2025-10-06

**Categories:** cs.LG, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04547v1](http://arxiv.org/pdf/2510.04547v1)

**Abstract:**

Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

---

## 61. Code World Models for General Game Playing

**Authors:** Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04542v1](http://arxiv.org/pdf/2510.04542v1)

**Abstract:**

Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

---

## 62. 3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG

**Authors:** Shun-ichiro Hayashi, Daichi Mukunoki, Tetsuya Hoshino, Satoshi Ohshima, Takahiro Katagiri

**Published:** 2025-10-06

**Categories:** cs.GR, cs.AI, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04536v1](http://arxiv.org/pdf/2510.04536v1)

**Abstract:**

This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

---

## 63. More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models

**Authors:** Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL, cs.RO

**PDF:** [http://arxiv.org/pdf/2510.04532v1](http://arxiv.org/pdf/2510.04532v1)

**Abstract:**

Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

---

## 64. Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph

**Authors:** Hanyu Wang, Ruohan Xie, Yutong Wang, Guoxiong Gao, Xintao Yu, Bin Dong

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04520v1](http://arxiv.org/pdf/2510.04520v1)

**Abstract:**

Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

---

## 65. ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering

**Authors:** Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CE, cs.CL, cs.CV, stat.ME

**PDF:** [http://arxiv.org/pdf/2510.04514v1](http://arxiv.org/pdf/2510.04514v1)

**Abstract:**

Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

---

## 66. MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations

**Authors:** Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang

**Published:** 2025-10-06

**Categories:** cs.IR

**PDF:** [http://arxiv.org/pdf/2510.04508v1](http://arxiv.org/pdf/2510.04508v1)

**Abstract:**

Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

---

## 67. Wavelet Predictive Representations for Non-Stationary Reinforcement Learning

**Authors:** Min Wang, Xin Li, Ye He, Yao-Hui Li, Hasnaa Bennis, Riashat Islam, Mingzhong Wang

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04507v1](http://arxiv.org/pdf/2510.04507v1)

**Abstract:**

The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

---

## 68. GRACE: Generative Representation Learning via Contrastive Policy Optimization

**Authors:** Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.IR

**PDF:** [http://arxiv.org/pdf/2510.04506v1](http://arxiv.org/pdf/2510.04506v1)

**Abstract:**

Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

---

## 69. Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation

**Authors:** Yue Que, Yingyi Zhang, Xiangyu Zhao, Chen Ma

**Published:** 2025-10-06

**Categories:** cs.IR, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04502v1](http://arxiv.org/pdf/2510.04502v1)

**Abstract:**

Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

---

## 70. Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents

**Authors:** Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04491v1](http://arxiv.org/pdf/2510.04491v1)

**Abstract:**

Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

---

## 71. Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning

**Authors:** Edward Y. Chang, Ethan Y. Chang

**Published:** 2025-10-06

**Categories:** cs.AI, cs.IT, math.IT, I.2.4

**PDF:** [http://arxiv.org/pdf/2510.04488v1](http://arxiv.org/pdf/2510.04488v1)

**Abstract:**

Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

---

## 72. Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents

**Authors:** Zhiping Zhang, Yi Evie Zhang, Freda Shi, Tianshi Li

**Published:** 2025-10-06

**Categories:** cs.HC, cs.AI, cs.CR

**PDF:** [http://arxiv.org/pdf/2510.04465v1](http://arxiv.org/pdf/2510.04465v1)

**Abstract:**

Large Language Model (LLM) agents require personal information for
personalization in order to better act on users' behalf in daily tasks, but
this raises privacy concerns and a personalization-privacy dilemma. Agent's
autonomy introduces both risks and opportunities, yet its effects remain
unclear. To better understand this, we conducted a 3$\times$3 between-subjects
experiment ($N=450$) to study how agent's autonomy level and personalization
influence users' privacy concerns, trust and willingness to use, as well as the
underlying psychological processes. We find that personalization without
considering users' privacy preferences increases privacy concerns and decreases
trust and willingness to use. Autonomy moderates these effects: Intermediate
autonomy flattens the impact of personalization compared to No- and Full
autonomy conditions. Our results suggest that rather than aiming for perfect
model alignment in output generation, balancing autonomy of agent's action and
user control offers a promising path to mitigate the personalization-privacy
dilemma.

---

## 73. AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents

**Authors:** Jenny T. Liang, Titus Barik, Jeffrey Nichols, Eldon Schoop, Ruijia Cheng

**Published:** 2025-10-06

**Categories:** cs.HC

**PDF:** [http://arxiv.org/pdf/2510.04452v1](http://arxiv.org/pdf/2510.04452v1)

**Abstract:**

Interface agents powered by generative AI models (referred to as "agents")
can automate actions based on user commands. An important aspect of developing
agents is their user experience (i.e., agent experience). There is a growing
need to provide scaffolds for a broader set of individuals beyond AI engineers
to prototype agent experiences, since they can contribute valuable perspectives
to designing agent experiences. In this work, we explore the affordances agent
prototyping systems should offer by conducting a requirements elicitation study
with 12 participants with varying experience with agents. We identify key
activities in agent experience prototyping and the desired capabilities of
agent prototyping systems. We instantiate those capabilities in the
AgentBuilder design probe for agent prototyping. We conduct an in situ agent
prototyping study with 14 participants using AgentBuilder to validate the
design requirements and elicit insights on how developers prototype agents and
what their needs are in this process.

---

## 74. Achieve Performatively Optimal Policy for Performative Reinforcement Learning

**Authors:** Ziyi Chen, Heng Huang

**Published:** 2025-10-06

**Categories:** cs.LG, math.OC

**PDF:** [http://arxiv.org/pdf/2510.04430v1](http://arxiv.org/pdf/2510.04430v1)

**Abstract:**

Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

---

## 75. Utility-Learning Tension in Self-Modifying Agents

**Authors:** Charles L. Wang, Keir Dorchen, Peter Jin

**Published:** 2025-10-05

**Categories:** cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04399v1](http://arxiv.org/pdf/2510.04399v1)

**Abstract:**

As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

---

## 76. Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous Collections

**Authors:** Bastian Jäckl, Jiří Kruchina, Lucas Joos, Daniel A. Keim, Ladislav Peška, Jakub Lokoč

**Published:** 2025-10-05

**Categories:** cs.MM, cs.IR, H.3.3; H.5.2; H.5.1

**PDF:** [http://arxiv.org/pdf/2510.04396v1](http://arxiv.org/pdf/2510.04396v1)

**Abstract:**

Multimodal deep-learning models power interactive video retrieval by ranking
keyframes in response to textual queries. Despite these advances, users must
still browse ranked candidates manually to locate a target. Keyframe
arrangement within the search grid highly affects browsing effectiveness and
user efficiency, yet remains underexplored. We report a study with 49
participants evaluating seven keyframe layouts for the Visual Known-Item Search
task. Beyond efficiency and accuracy, we relate browsing phenomena, such as
overlooks, to layout characteristics. Our results show that a video-grouped
layout is the most efficient, while a four-column, rank-preserving grid
achieves the highest accuracy. Sorted grids reveal potentials and trade-offs,
enabling rapid scanning of uninteresting regions but down-ranking relevant
targets to less prominent positions, delaying first arrival times and
increasing overlooks.
  These findings motivate hybrid designs that preserve positions of top-ranked
items while sorting or grouping the remainder, and offer guidance for searching
in grids beyond video retrieval.

---

## 77. Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards

**Authors:** Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, Alfy Samuel

**Published:** 2025-10-05

**Categories:** cs.CL, cs.AI, cs.CY, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04392v1](http://arxiv.org/pdf/2510.04392v1)

**Abstract:**

RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

---

## 78. Internal World Models as Imagination Networks in Cognitive Agents

**Authors:** Saurabh Ranjan, Brian Odegaard

**Published:** 2025-10-05

**Categories:** cs.AI, cs.CL, cs.SI, q-bio.NC

**PDF:** [http://arxiv.org/pdf/2510.04391v1](http://arxiv.org/pdf/2510.04391v1)

**Abstract:**

What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

---

## 79. Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation

**Authors:** Hadi Nekoei, Aman Jaiswal, Patrice Bechard, Oleh Shliazhko, Orlando Marquez Ayala, Mathieu Reymond, Massimo Caccia, Alexandre Drouin, Sarath Chandar, Alexandre Lacoste

**Published:** 2025-10-05

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04373v1](http://arxiv.org/pdf/2510.04373v1)

**Abstract:**

Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

---

## 80. Speculative Actions: A Lossless Framework for Faster Agentic Systems

**Authors:** Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng

**Published:** 2025-10-05

**Categories:** cs.AI, cs.DC, cs.MA

**PDF:** [http://arxiv.org/pdf/2510.04371v1](http://arxiv.org/pdf/2510.04371v1)

**Abstract:**

Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

---

## 81. NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment

**Authors:** Shashank Mangla, Chris Hokamp, Jack Boylan, Demian Gholipour Ghalandari, Yuuv Jauhari, Lauren Cassidy, Oisin Duffy

**Published:** 2025-10-05

**Categories:** cs.MA, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04368v1](http://arxiv.org/pdf/2510.04368v1)

**Abstract:**

We design and implement NegotiationGym, an API and user interface for
configuring and running multi-agent social simulations focused upon negotiation
and cooperation. The NegotiationGym codebase offers a user-friendly,
configuration-driven API that enables easy design and customization of
simulation scenarios. Agent-level utility functions encode optimization
criteria for each agent, and agents can self-optimize by conducting multiple
interaction rounds with other agents, observing outcomes, and modifying their
strategies for future rounds.

---

## 82. Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators

**Authors:** Apurva Badithela, David Snyder, Lihan Zha, Joseph Mikhail, Matthew O'Kelly, Anushri Dixit, Anirudha Majumdar

**Published:** 2025-10-05

**Categories:** cs.RO, cs.AI, cs.SY, eess.SY

**PDF:** [http://arxiv.org/pdf/2510.04354v1](http://arxiv.org/pdf/2510.04354v1)

**Abstract:**

Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

---

## 83. Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies

**Authors:** G. Niklas Noren, Eva-Lisa Meldau, Johan Ellenius

**Published:** 2025-10-05

**Categories:** cs.LG, cs.AI, I.2.0

**PDF:** [http://arxiv.org/pdf/2510.04341v1](http://arxiv.org/pdf/2510.04341v1)

**Abstract:**

Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

---

## 84. RAP: 3D Rasterization Augmented End-to-End Planning

**Authors:** Lan Feng, Yang Gao, Eloi Zablocki, Quanyi Li, Wuyang Li, Sichao Liu, Matthieu Cord, Alexandre Alahi

**Published:** 2025-10-05

**Categories:** cs.CV, cs.RO

**PDF:** [http://arxiv.org/pdf/2510.04333v1](http://arxiv.org/pdf/2510.04333v1)

**Abstract:**

Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

---

## 85. DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks

**Authors:** Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho

**Published:** 2025-10-05

**Categories:** cs.LG, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04331v1](http://arxiv.org/pdf/2510.04331v1)

**Abstract:**

Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

---

## 86. FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents

**Authors:** Yucong Dai, Lu Zhang, Feng Luo, Mashrur Chowdhury, Yongkai Wu

**Published:** 2025-10-05

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04317v1](http://arxiv.org/pdf/2510.04317v1)

**Abstract:**

Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

---

## 87. On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems

**Authors:** Bohan Tang, Huidong Liang, Keyue Jiang, Xiaowen Dong

**Published:** 2025-10-05

**Categories:** cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04311v1](http://arxiv.org/pdf/2510.04311v1)

**Abstract:**

Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

---

## 88. Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs

**Authors:** Om Tailor

**Published:** 2025-10-05

**Categories:** cs.MA, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04303v1](http://arxiv.org/pdf/2510.04303v1)

**Abstract:**

Multi-agent deployments of large language models (LLMs) are increasingly
embedded in market, allocation, and governance workflows, yet covert
coordination among agents can silently erode trust and social welfare. Existing
audits are dominated by heuristics that lack theoretical guarantees, struggle
to transfer across tasks, and seldom ship with the infrastructure needed for
independent replication. We introduce \emph{Audit the Whisper}, a
conference-grade research artifact that spans theory, benchmark design,
detection, and reproducibility. Our contributions are: (i) a channel-capacity
analysis showing how interventions such as paraphrase, rate limiting, and role
permutation impose quantifiable capacity penalties -- operationalized via
paired-run Kullback--Leibler diagnostics -- that tighten mutual-information
thresholds with finite-sample guarantees; (ii) \textsc{ColludeBench}-v0,
covering pricing, first-price auctions, and peer review with configurable
covert schemes, deterministic manifests, and reward instrumentation; and (iii)
a calibrated auditing pipeline that fuses cross-run mutual information,
permutation invariance, watermark variance, and fairness-aware acceptance bias,
each tuned to a \(10^{-3}\) false-positive budget. Across 600 audited runs
spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with
zero observed false alarms, while ablations surface the price-of-auditing
trade-off and highlight fairness-driven colluders invisible to MI alone. We
release regeneration scripts, seed-stamped manifests, and documentation so that
external auditors can reproduce every figure and extend the framework with
minimal effort.

---

## 89. Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness

**Authors:** Lingnan Xu, Chong Feng, Kaiyuan Zhang, Liu Zhengyong, Wenqiang Xu, Fanqing Meng

**Published:** 2025-10-05

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04293v1](http://arxiv.org/pdf/2510.04293v1)

**Abstract:**

While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

---

## 90. Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning

**Authors:** Yunghwei Lai, Kaiming Liu, Ziyue Wang, Weizhi Ma, Yang Liu

**Published:** 2025-10-05

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04284v1](http://arxiv.org/pdf/2510.04284v1)

**Abstract:**

The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

---

## 91. Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales

**Authors:** Jinyang Jiang, Jinhui Han, Yijie Peng, Ying Zhang

**Published:** 2025-10-05

**Categories:** cs.AI, cs.LG, math.OC

**PDF:** [http://arxiv.org/pdf/2510.04272v1](http://arxiv.org/pdf/2510.04272v1)

**Abstract:**

Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

---

## 92. AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents

**Authors:** Yanjie Li, Yiming Cao, Dong Wang, Bin Xiao

**Published:** 2025-10-05

**Categories:** cs.CR, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04257v1](http://arxiv.org/pdf/2510.04257v1)

**Abstract:**

Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

---

## 93. Empowering Denoising Sequential Recommendation with Large Language Model Embeddings

**Authors:** Tongzhou Wu, Yuhao Wang, Maolin Wang, Chi Zhang, Xiangyu Zhao

**Published:** 2025-10-05

**Categories:** cs.IR, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04239v1](http://arxiv.org/pdf/2510.04239v1)

**Abstract:**

Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

---

## 94. When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue

**Authors:** Rikuo Sasaki, Michimasa Inaba

**Published:** 2025-10-05

**Categories:** cs.HC, cs.AI, H.5.2

**PDF:** [http://arxiv.org/pdf/2510.04229v1](http://arxiv.org/pdf/2510.04229v1)

**Abstract:**

Recent advancements in AI have highlighted its application in captology, the
field of using computers as persuasive technologies. We hypothesized that the
"conformity effect," where individuals align with others' actions, also occurs
with AI agents. This study verifies this hypothesis by introducing a "Persuadee
Agent" that is persuaded alongside a human participant in a three-party
persuasive dialogue with a Persuader Agent. We conducted a text-based dialogue
experiment with human participants. We compared four conditions manipulating
the Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and
the presence of an icebreaker session. Results showed that when the Persuadee
Agent accepted persuasion, both perceived persuasiveness and actual attitude
change significantly improved. Attitude change was greatest when an icebreaker
was also used, whereas an unpersuaded AI agent suppressed attitude change.
Additionally, it was confirmed that the persuasion acceptance of participants
increased at the moment the Persuadee Agent was persuaded. These results
suggest that appropriately designing a Persuadee Agent can improve persuasion
through the conformity effect.

---

## 95. Epistemic Diversity and Knowledge Collapse in Large Language Models

**Authors:** Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein

**Published:** 2025-10-05

**Categories:** cs.CL, cs.AI, cs.CY, cs.IR, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04226v1](http://arxiv.org/pdf/2510.04226v1)

**Abstract:**

Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

---

## 96. Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards

**Authors:** Zhuoran Zhuang, Ye Chen, Xia Zeng, Chao Luo, Luhui Liu, Yihan Chen

**Published:** 2025-10-05

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04214v1](http://arxiv.org/pdf/2510.04214v1)

**Abstract:**

We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

---

## 97. AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework

**Authors:** Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong

**Published:** 2025-10-05

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04206v1](http://arxiv.org/pdf/2510.04206v1)

**Abstract:**

Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

---

## 98. Adaptive Federated Learning via Dynamical System Model

**Authors:** Aayushya Agarwal, Larry Pileggi, Gauri Joshi

**Published:** 2025-10-05

**Categories:** cs.LG, cs.SY, eess.SY

**PDF:** [http://arxiv.org/pdf/2510.04203v1](http://arxiv.org/pdf/2510.04203v1)

**Abstract:**

Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

---

## 99. World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge

**Authors:** Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi

**Published:** 2025-10-05

**Categories:** cs.CV, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04201v1](http://arxiv.org/pdf/2510.04201v1)

**Abstract:**

While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

---

## 100. Constructing coherent spatial memory in LLM agents through graph rectification

**Authors:** Puzhen Zhang, Xuyang Chen, Yu Feng, Yuhan Jiang, Liqiu Meng

**Published:** 2025-10-05

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04195v1](http://arxiv.org/pdf/2510.04195v1)

**Abstract:**

Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

---

