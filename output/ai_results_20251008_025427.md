# AI Search Results

**Generated:** 2025-10-08 02:55:21

---

## arXiv Results (found 437)

### 1. EgoNight: Towards Egocentric Vision Understanding at Night with a   Challenging Benchmark

**Authors:** Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06218v1](http://arxiv.org/pdf/2510.06218v1)

**Abstract:**

Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.

---

### 2. Stratified GRPO: Handling Structural Heterogeneity in Reinforcement   Learning of LLM Search Agents

**Authors:** Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06214v1](http://arxiv.org/pdf/2510.06214v1)

**Abstract:**

Large language model (LLM) agents increasingly rely on external tools such as
search engines to solve complex, multi-step problems, and reinforcement
learning (RL) has become a key paradigm for training them. However, the
trajectories of search agents are structurally heterogeneous, where variations
in the number, placement, and outcomes of search calls lead to fundamentally
different answer directions and reward distributions. Standard policy gradient
methods, which use a single global baseline, suffer from what we identify and
formalize as cross-stratum bias-an "apples-to-oranges" comparison of
heterogeneous trajectories. This cross-stratum bias distorts credit assignment
and hinders exploration of complex, multi-step search strategies. To address
this, we propose Stratified GRPO, whose central component, Stratified Advantage
Normalization (SAN), partitions trajectories into homogeneous strata based on
their structural properties and computes advantages locally within each
stratum. This ensures that trajectories are evaluated only against their true
peers. Our analysis proves that SAN eliminates cross-stratum bias, yields
conditionally unbiased unit-variance estimates inside each stratum, and retains
the global unbiasedness and unit-variance properties enjoyed by standard
normalization, resulting in a more pure and scale-stable learning signal. To
improve practical stability under finite-sample regimes, we further linearly
blend SAN with the global estimator. Extensive experiments on diverse
single-hop and multi-hop question-answering benchmarks demonstrate that
Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3
points, achieving higher training rewards, greater training stability, and more
effective search policies. These results establish stratification as a
principled remedy for structural heterogeneity in RL for LLM search agents.

---

### 3. Training Dynamics Impact Post-Training Quantization Robustness

**Authors:** Albert Catalan-Tatjer, NiccolÃ² Ajroldi, Jonas Geiping

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06213v1](http://arxiv.org/pdf/2510.06213v1)

**Abstract:**

While post-training quantization is widely adopted for efficient deployment
of large language models, the mechanisms underlying quantization robustness
remain unclear. We conduct a comprehensive analysis of quantization degradation
across open-source language model training trajectories up to 32B parameters
and 15T training tokens to accurately assess the relationship between training
dynamics and quantization performance. Our key finding is that quantization
errors in large-scale training runs are driven by a complex interplay between
learning rate and other training hyperparameters. Specifically, once learning
rates decay, validation loss and quantization error diverge, largely
independent of training data scale. To investigate interventions on the
training dynamics and identify specific configurations that can modulate
quantization robustness favorably, we train our own models in controlled
experiments up to 100B tokens. Our results challenge the assumption that
increasing dataset scale inherently compromises quantization effectiveness,
demonstrating instead that strategic training hyperparameter interventions can
improve quantization quality at scale.

---

### 4. StarEmbed: Benchmarking Time Series Foundation Models on Astronomical   Observations of Variable Stars

**Authors:** Weijian Li, Hong-Yu Chen, Qinjie Lin, Nabeel Rehemtulla, Ved G. Shah, Dennis Wu, Adam A. Miller, Han Liu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06200v1](http://arxiv.org/pdf/2510.06200v1)

**Abstract:**

Time series foundation models (TSFMs) are increasingly being adopted as
highly-capable general-purpose time series representation learners. Although
their training corpora are vast, they exclude astronomical time series data.
Observations of stars produce peta-scale time series with unique challenges
including irregular sampling and heteroskedasticity. We introduce StarEmbed,
the first public benchmark for rigorous and standardized evaluation of
state-of-the-art TSFMs on stellar time series observations (``light curves'').
We benchmark on three scientifically-motivated downstream tasks: unsupervised
clustering, supervised classification, and out-of-distribution source
detection. StarEmbed integrates a catalog of expert-vetted labels with
multi-variate light curves from the Zwicky Transient Facility, yielding ~40k
hand-labeled light curves spread across seven astrophysical classes. We
evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,
Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against
handcrafted feature extraction, the long-standing baseline in the astrophysics
literature. Our results demonstrate that these TSFMs, especially the Chronos
models, which are trained on data completely unlike the astronomical
observations, can outperform established astrophysics-specific baselines in
some tasks and effectively generalize to entirely new data. In particular,
TSFMs deliver state-of-the-art performance on our out-of-distribution source
detection benchmark. With the first benchmark of TSFMs on astronomical time
series data, we test the limits of their generalization and motivate a paradigm
shift in time-domain astronomy from using task-specific, fully supervised
pipelines toward adopting generic foundation model representations for the
analysis of peta-scale datasets from forthcoming observatories.

---

### 5. Peeking inside the Black-Box: Reinforcement Learning for Explainable and   Accurate Relation Extraction

**Authors:** Xinyu Guo, Zhengliang Shi, Minglai Yang, Mahdi Rahimi, Mihai Surdeanu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06198v1](http://arxiv.org/pdf/2510.06198v1)

**Abstract:**

This paper introduces a framework for relation extraction (RE) that enhances
both accuracy and explainability. The framework has two key components: (i) a
reasoning mechanism that formulates relation extraction as a series of
text-processing steps inspired by cognitive science, and (ii) an optimization
process driven by reinforcement learning (RL) with a novel reward function
designed to improve both task accuracy and explanation quality. We call our
approach CogRE. Our framework addresses the lack of supervision for
language-based explanations in traditional RE by promoting outputs that include
important relation keywords. These keywords are drawn from a high-quality
dictionary that is automatically constructed using an LLM. We evaluate our
approach for the task of one-shot RE using two LLMs and two RE datasets. Our
experiments show that CogRE improves explanation quality by addressing two
common failure patterns in one-shot RE: poor attention focus and limited
one-shot learning capability. For example, our cognitive-structured reasoning
with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing
prior reasoning-based designs. Optimizing this approach with RL using our
reward further improves performance by +23.46% (absolute). Finally, human
evaluation shows that our best model generates relational keywords closely
aligned with gold labels, increasing human explanation quality ratings by 54%
(relative).

---

### 6. On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond

**Authors:** Chenxiao Yang, Cai Zhou, David Wipf, Zhiyuan Li

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06190v1](http://arxiv.org/pdf/2510.06190v1)

**Abstract:**

This paper formally studies generation processes, including auto-regressive
next-token prediction and masked diffusion, that abstract beyond architectural
specifics. At this level of abstraction, we quantify their benefits and
limitations through measurable criteria such as computational hardness and
learnability. In particular, we demonstrate that allowing generation to proceed
beyond autoregression and current masked diffusion, with capabilities to
rewrite and length-variable edit, can bring significant theoretical and
empirical advantages, with important implications for frontier LLMs that aspire
to tackle increasingly hard problems and work universally across domains beyond
natural language, such as coding and science.

---

### 7. Barbarians at the Gate: How AI is Upending Systems Research

**Authors:** Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li, Bowen Wang, Alex Krentsel, Tian Xia, Mert Cemri, Jongseok Park, Shuo Yang, Jeff Chen, Aditya Desai, Jiarong Xing, Koushik Sen, Matei Zaharia, Ion Stoica

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06189v1](http://arxiv.org/pdf/2510.06189v1)

**Abstract:**

Artificial Intelligence (AI) is starting to transform the research process as
we know it by automating the discovery of new solutions. Given a task, the
typical AI-driven approach is (i) to generate a set of diverse solutions, and
then (ii) to verify these solutions and select one that solves the problem.
Crucially, this approach assumes the existence of a reliable verifier, i.e.,
one that can accurately determine whether a solution solves the given problem.
We argue that systems research, long focused on designing and evaluating new
performance-oriented algorithms, is particularly well-suited for AI-driven
solution discovery. This is because system performance problems naturally admit
reliable verifiers: solutions are typically implemented in real systems or
simulators, and verification reduces to running these software artifacts
against predefined workloads and measuring performance. We term this approach
as AI-Driven Research for Systems (ADRS), which iteratively generates,
evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS
instance, we present case studies across diverse domains, including load
balancing for multi-region cloud scheduling, Mixture-of-Experts inference,
LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS
discovers algorithms that outperform state-of-the-art human designs (e.g.,
achieving up to 5.0x runtime improvements or 50% cost reductions). We distill
best practices for guiding algorithm evolution, from prompt design to evaluator
construction, for existing frameworks. We then discuss the broader implications
for the systems community: as AI assumes a central role in algorithm design, we
argue that human researchers will increasingly focus on problem formulation and
strategic guidance. Our results highlight both the disruptive potential and the
urgent need to adapt systems research practices in the age of AI.

---

### 8. Automated Program Repair of Uncompilable Student Code

**Authors:** Griffin Pitts, Aum Pandya, Darsh Rank, Tirth Bhatt, Muntasir Hoq, Bita Akram

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06187v1](http://arxiv.org/pdf/2510.06187v1)

**Abstract:**

A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

---

### 9. RECODE-H: A Benchmark for Research Code Development with Interactive   Human Feedback

**Authors:** Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, Yifan Li, Wooseong Yang, Bowei He, Xinni Zhang, Dianzhi Yu, Hanchen Yang, Hoang H Nguyen, Yue Zhou, Jie Yang, Jizhou Guo, Wenzhe Fan, Chin-Yuan Yeh, Panpan Meng, Liancheng Fang, Jinhu Qi, Wei-Chieh Huang, Zhengyao Gu, Yuwei Han, Langzhou He, Yuyao Yang, Xue Liu, Irwin King, Philip S. Yu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06186v1](http://arxiv.org/pdf/2510.06186v1)

**Abstract:**

Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation

---

### 10. VecInfer: Efficient LLM Inference with Low-Bit KV Cache via   Outlier-Suppressed Vector Quantization

**Authors:** Dingyu Yao, Chenxu Yang, Zhengyang Tong, Zheng Lin, Wei Liu, Jian Luan, Weiping Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06175v1](http://arxiv.org/pdf/2510.06175v1)

**Abstract:**

The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.

---

### 11. TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts

**Authors:** Christopher Kolberg, Katharina Eggensperger, Nico Pfeifer

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06162v1](http://arxiv.org/pdf/2510.06162v1)

**Abstract:**

Revealing novel insights from the relationship between molecular measurements
and pathology remains a very impactful application of machine learning in
biomedicine. Data in this domain typically contain only a few observations but
thousands of potentially noisy features, posing challenges for conventional
machine learning approaches. While prior-data fitted networks emerge as
foundation models for tabular data, they are currently not suited to handle
large feature counts (>500). Although feature reduction enables their
application, it hinders feature importance analysis. We propose a strategy that
extends existing models through continued pre-training on synthetic data
sampled from a customized prior. The resulting model, TabPFN-Wide, matches or
exceeds its base model's performance while exhibiting improved robustness to
noise. It seamlessly scales beyond 50,000 features, regardless of noise levels,
while maintaining inherent interpretability, which is critical for biomedical
applications. Our results show that prior-informed adaptation is suitable to
enhance the capability of foundation models for high-dimensional data. On
real-world biomedical datasets many of the most relevant features identified by
the model overlap with previous biological findings, while others propose
potential starting points for future studies.

---

### 12. LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design   for Heterogeneous Agent Teams

**Authors:** Aju Ani Justus, Chris Baber

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06151v1](http://arxiv.org/pdf/2510.06151v1)

**Abstract:**

A critical challenge in modelling Heterogeneous-Agent Teams is training
agents to collaborate with teammates whose policies are inaccessible or
non-stationary, such as humans. Traditional approaches rely on expensive
human-in-the-loop data, which limits scalability. We propose using Large
Language Models (LLMs) as policy-agnostic human proxies to generate synthetic
data that mimics human decision-making. To evaluate this, we conduct three
experiments in a grid-world capture game inspired by Stag Hunt, a game theory
paradigm that balances risk and reward. In Experiment 1, we compare decisions
from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and
Mixtral 8x22B models. LLMs, prompted with game-state observations and reward
structures, align more closely with experts than participants, demonstrating
consistency in applying underlying decision criteria. Experiment 2 modifies
prompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLM
outputs mirror human participants' variability, shifting between risk-averse
and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic
grid-world where the LLM agents generate movement actions. LLMs produce
trajectories resembling human participants' paths. While LLMs cannot yet fully
replicate human adaptability, their prompt-guided diversity offers a scalable
foundation for simulating policy-agnostic teammates.

---

### 13. RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators   without Human Test Sets

**Authors:** Jan Cegin, Branislav Pecher, Ivan Srba, Jakub Simko

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06143v1](http://arxiv.org/pdf/2510.06143v1)

**Abstract:**

LLMs are powerful generators of synthetic data, which are used for training
smaller, specific models. This is especially valuable for low-resource
languages, where human-labelled data is scarce but LLMs can still produce
high-quality text. However, LLMs differ in how useful their outputs are for
training. Selecting the best LLM as a generator is challenging because
extrinsic evaluation requires costly human annotations (which are often
unavailable for low-resource languages), while intrinsic metrics correlate
poorly with downstream performance. We introduce Round robin Synthetic data
Evaluation (RoSE), a proxy metric for selecting the best LLM generator without
human test sets. RoSE trains a small model on the outputs of a candidate
generator (LLM) and then evaluates it on generated synthetic examples from all
other candidate LLMs. The final RoSE score is the mean performance of this
small model. Across six LLMs, eleven languages, and three tasks (sentiment,
topic, intent), RoSE identifies the optimal generator more often than any other
intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within
0.76 percentage points of the optimal generator baseline. This result is
measured in terms of downstream performance, obtained by training a small model
on the chosen generator's outputs (optimal vs. proxy metric selected) and
evaluating it on human-labelled test data. Additionally, RoSE is the only
metric to achieve a positive correlation with performance on human test data.

---

### 14. CreditDecoding: Accelerating Parallel Decoding in Diffusion Large   Language Models with Trace Credits

**Authors:** Kangyu Wang, Zhiyun Jiang, Haibo Feng, Weijia Zhao, Lin Liu, Jianguo Li, Zhenzhong Lan, Weiyao Lin

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06133v1](http://arxiv.org/pdf/2510.06133v1)

**Abstract:**

Diffusion large language models (dLLMs) generate text through iterative
denoising steps, achieving parallel decoding by denoising only high-confidence
positions at each step. However, existing approaches often repetitively remask
tokens due to initially low confidence scores, leading to redundant iterations
and limiting overall acceleration. Through the analysis of dLLM decoding
traces, we observe that the model often determines the final prediction for a
token several steps before the decoding step. To leverage this historical
information and avoid redundant steps, we introduce the concept of Trace
Credit, which quantifies each token's convergence potential by accumulating
historical logits. Furthermore, we propose CreditDecoding, a training-free
parallel decoding algorithm that accelerates the confidence convergence of
correct but underconfident tokens by fusing current logits with Trace Credit.
This process significantly reduces redundant iterations and enhances decoding
robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup
and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times
speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.
Importantly, CreditDecoding scales effectively to long sequences and is
orthogonal to mainstream inference optimizations, making it a readily
integrable and versatile solution.

---

### 15. Discrete Diffusion Models with MLLMs for Unified Medical Multimodal   Generation

**Authors:** Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06131v1](http://arxiv.org/pdf/2510.06131v1)

**Abstract:**

Recent advances in generative medical models are constrained by
modality-specific scenarios that hinder the integration of complementary
evidence from imaging, pathology, and clinical notes. This fragmentation limits
their evolution into foundation models that can learn and reason across the
full spectrum of biomedical data. We propose MeDiM, the first medical discrete
diffusion model that learns shared distributions across modalities without
modality-specific components. MeDiM unifies multiple generative tasks:
translating between images and text, and jointly producing image-report pairs
across domains in response to prompts. Built on a discrete diffusion framework,
MeDiM bridges vision and language representations through a shared
probabilistic space. To enable unified and flexible medical generation, we
employ a multimodal large language model (MLLM) as the diffusion backbone,
leveraging its prior knowledge and cross-modal reasoning. Two key designs are
introduced: (1) removing the causal attention mask for bidirectional context,
and (2) injecting continuous timestep embeddings for diffusion awareness.
Experiments demonstrate high-fidelity medical generation (FID 16.60 on
MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR
0.2650 and 0.2580). Jointly generated image-report pairs further enhance
downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,
plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports
coherent and clinically grounded multimodal outputs.

---

### 16. lm-Meter: Unveiling Runtime Inference Latency for On-Device Language   Models

**Authors:** Haoxin Wang, Xiaolong Tu, Hongyu Ke, Huirong Chai, Dawei Chen, Kyungtae Han

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06126v1](http://arxiv.org/pdf/2510.06126v1)

**Abstract:**

Large Language Models (LLMs) are increasingly integrated into everyday
applications, but their prevalent cloud-based deployment raises growing
concerns around data privacy and long-term sustainability. Running LLMs locally
on mobile and edge devices (on-device LLMs) offers the promise of enhanced
privacy, reliability, and reduced communication costs. However, realizing this
vision remains challenging due to substantial memory and compute demands, as
well as limited visibility into performance-efficiency trade-offs on
resource-constrained hardware. We propose lm-Meter, the first lightweight,
online latency profiler tailored for on-device LLM inference. lm-Meter captures
fine-grained, real-time latency at both phase (e.g., embedding, prefill,
decode, softmax, sampling) and kernel levels without auxiliary devices. We
implement lm-Meter on commercial mobile platforms and demonstrate its high
profiling accuracy with minimal system overhead, e.g., only 2.58% throughput
reduction in prefill and 0.99% in decode under the most constrained Powersave
governor. Leveraging lm-Meter, we conduct comprehensive empirical studies
revealing phase- and kernel-level bottlenecks in on-device LLM inference,
quantifying accuracy-efficiency trade-offs, and identifying systematic
optimization opportunities. lm-Meter provides unprecedented visibility into the
runtime behavior of LLMs on constrained platforms, laying the foundation for
informed optimization and accelerating the democratization of on-device LLM
systems. Code and tutorials are available at
https://github.com/amai-gsu/LM-Meter.

---

### 17. Influence Functions for Efficient Data Selection in Reasoning

**Authors:** Prateek Humane, Paolo Cudrano, Daniel Z. Kaplan, Matteo Matteucci, Supriyo Chakraborty, Irina Rish

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06108v1](http://arxiv.org/pdf/2510.06108v1)

**Abstract:**

Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows
that a small amount of high-quality data can outperform massive datasets. Yet,
what constitutes "quality" remains ill-defined. Existing reasoning methods rely
on indirect heuristics such as problem difficulty or trace length, while
instruction-tuning has explored a broader range of automated selection
strategies, but rarely in the context of reasoning. We propose to define
reasoning data quality using influence functions, which measure the causal
effect of individual CoT examples on downstream accuracy, and introduce
influence-based pruning, which consistently outperforms perplexity and
embedding-based baselines on math reasoning within a model family.

---

### 18. Distributional Semantics Tracing: A Framework for Explaining   Hallucinations in Large Language Models

**Authors:** Gagan Bhatia, Somayajulu G Sripada, Kevin Allan, Jacobo Azcona

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06107v1](http://arxiv.org/pdf/2510.06107v1)

**Abstract:**

Large Language Models (LLMs) are prone to hallucination, the generation of
plausible yet factually incorrect statements. This work investigates the
intrinsic, architectural origins of this failure mode through three primary
contributions.First, to enable the reliable tracing of internal semantic
failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified
framework that integrates established interpretability techniques to produce a
causal map of a model's reasoning, treating meaning as a function of context
(distributional semantics). Second, we pinpoint the model's layer at which a
hallucination becomes inevitable, identifying a specific \textbf{commitment
layer} where a model's internal representations irreversibly diverge from
factuality. Third, we identify the underlying mechanism for these failures. We
observe a conflict between distinct computational pathways, which we interpret
using the lens of dual-process theory: a fast, heuristic \textbf{associative
pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway}
(akin to System 2), leading to predictable failure modes such as
\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the
coherence of the contextual pathway reveals a strong negative correlation
($\rho = -0.863$) with hallucination rates, implying that these failures are
predictable consequences of internal semantic weakness. The result is a
mechanistic account of how, when, and why hallucinations occur within the
Transformer architecture.

---

### 19. Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences

**Authors:** Batu El, James Zou

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06105v1](http://arxiv.org/pdf/2510.06105v1)

**Abstract:**

Large language models (LLMs) are increasingly shaping how information is
created and disseminated, from companies using them to craft persuasive
advertisements, to election campaigns optimizing messaging to gain votes, to
social media influencers boosting engagement. These settings are inherently
competitive, with sellers, candidates, and influencers vying for audience
approval, yet it remains poorly understood how competitive feedback loops
influence LLM behavior. We show that optimizing LLMs for competitive success
can inadvertently drive misalignment. Using simulated environments across these
scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise
in deceptive marketing; in elections, a 4.9% gain in vote share coincides with
22.3% more disinformation and 12.5% more populist rhetoric; and on social
media, a 7.5% engagement boost comes with 188.6% more disinformation and a
16.3% increase in promotion of harmful behaviors. We call this phenomenon
Moloch's Bargain for AI--competitive success achieved at the cost of alignment.
These misaligned behaviors emerge even when models are explicitly instructed to
remain truthful and grounded, revealing the fragility of current alignment
safeguards. Our findings highlight how market-driven optimization pressures can
systematically erode alignment, creating a race to the bottom, and suggest that
safe deployment of AI systems will require stronger governance and carefully
designed incentives to prevent competitive dynamics from undermining societal
trust.

---

### 20. The Valley of Code Reasoning: Scaling Knowledge Distillation of Large   Language Models

**Authors:** Muyu He, Muhammad Ali Shafique, Anand Kumar, Tsach Mackey, Nazneen Rajani

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06101v1](http://arxiv.org/pdf/2510.06101v1)

**Abstract:**

Distilling the thinking traces of a Large Language Model (LLM) with reasoning
capabilities into a smaller model has been proven effective. Yet, there is a
scarcity of work done on how model performances scale with the quantity of
distillation data. In this work, we study the scaling trend of distilling
competitive coding skills on two small non-reasoning LLMs. We validate the
hypothesis that there is a $\textit{valley of code reasoning}$: downstream
performance on competitive coding first drops as data quantity increases, then
it steadily increases in a sharper-than-log-linear fashion. Having identified
the trend, we further fine-tune the models at two different distillation stages
on the same data to ground conclusions on their respective learning phases. We
learn that across stages in the low and medium-low data regimes, small models
benefit significantly from easier coding questions than from harder ones. We
also find that, surprisingly, the correctness of outputs in training data makes
no difference to distillation outcomes. Our work represents a step forward in
understanding the training dynamics of code reasoning distillation outside
intuition

---

### 21. The Alignment Auditor: A Bayesian Framework for Verifying and Refining   LLM Objectives

**Authors:** Matthieu Bou, Nyal Patel, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06096v1](http://arxiv.org/pdf/2510.06096v1)

**Abstract:**

The objectives that Large Language Models (LLMs) implicitly optimize remain
dangerously opaque, making trustworthy alignment and auditing a grand
challenge. While Inverse Reinforcement Learning (IRL) can infer reward
functions from behaviour, existing approaches either produce a single,
overconfident reward estimate or fail to address the fundamental ambiguity of
the task (non-identifiability). This paper introduces a principled auditing
framework that re-frames reward inference from a simple estimation task to a
comprehensive process for verification. Our framework leverages Bayesian IRL to
not only recover a distribution over objectives but to enable three critical
audit capabilities: (i) Quantifying and systematically reducing
non-identifiability by demonstrating posterior contraction over sequential
rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics
that expose spurious shortcuts and identify out-of-distribution prompts where
the inferred objective cannot be trusted; and (iii) Validating policy-level
utility by showing that the refined, low-uncertainty reward can be used
directly in RLHF to achieve training dynamics and toxicity reductions
comparable to the ground-truth alignment process. Empirically, our framework
successfully audits a detoxified LLM, yielding a well-calibrated and
interpretable objective that strengthens alignment guarantees. Overall, this
work provides a practical toolkit for auditors, safety teams, and regulators to
verify what LLMs are truly trying to achieve, moving us toward more trustworthy
and accountable AI.

---

### 22. Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance   Choices

**Authors:** Mallika Mainali, Harsha Sureshbabu, Anik Sen, Christopher B. Rauch, Noah D. Reifsnyder, John Meyer, J. T. Turner, Michael W. Floyd, Matthew Molineaux, Rosina O. Weber

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06093v1](http://arxiv.org/pdf/2510.06093v1)

**Abstract:**

As algorithmic decision-makers are increasingly applied to high-stakes
domains, AI alignment research has evolved from a focus on universal value
alignment to context-specific approaches that account for decision-maker
attributes. Prior work on Decision-Maker Alignment (DMA) has explored two
primary strategies: (1) classical AI methods integrating case-based reasoning,
Bayesian reasoning, and naturalistic decision-making, and (2) large language
model (LLM)-based methods leveraging prompt engineering. While both approaches
have shown promise in limited domains such as medical triage, their
generalizability to novel contexts remains underexplored. In this work, we
implement a prior classical AI model and develop an LLM-based algorithmic
decision-maker evaluated using a large reasoning model (GPT-5) and a
non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot
prompting framework, as proposed in recent literature. We evaluate both
approaches on a health insurance decision-making dataset annotated for three
target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).
In the experiments reported herein, classical AI and LLM-based models achieved
comparable alignment with attribute-based targets, with classical AI exhibiting
slightly better alignment for a moderate risk profile. The dataset and
open-source implementation are publicly available at:
https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and
https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.

---

### 23. Learning from Failures: Understanding LLM Alignment through   Failure-Aware Inverse RL

**Authors:** Nyal Patel, Matthieu Bou, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06092v1](http://arxiv.org/pdf/2510.06092v1)

**Abstract:**

Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with human preferences, yet the underlying reward signals they
internalize remain hidden, posing a critical challenge for interpretability and
safety. Existing approaches attempt to extract these latent incentives using
Inverse Reinforcement Learning (IRL), but treat all preference pairs equally,
often overlooking the most informative signals: those examples the extracted
reward model misclassifies or assigns nearly equal scores, which we term
\emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that
focuses on misclassified or difficult examples to recover the latent rewards
defining model behaviors. By learning from these failures, our failure-aware
IRL extracts reward functions that better reflect the true objectives behind
RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines
across multiple metrics when applied to LLM detoxification, without requiring
external classifiers or supervision. Crucially, failure-aware IRL yields
rewards that better capture the true incentives learned during RLHF, enabling
more effective re-RLHF training than standard IRL. This establishes
failure-aware IRL as a robust, scalable method for auditing model alignment and
reducing ambiguity in the IRL process.

---

### 24. Constraint-Aware Route Recommendation from Natural Language via   Hierarchical LLM Agents

**Authors:** Tao Zhe, Rui Liu, Fateme Memar, Xiao Luo, Wei Fan, Xinyue Ye, Zhongren Peng, Dongjie Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06078v1](http://arxiv.org/pdf/2510.06078v1)

**Abstract:**

Route recommendation aims to provide users with optimal travel plans that
satisfy diverse and complex requirements. Classical routing algorithms (e.g.,
shortest-path and constraint-aware search) are efficient but assume structured
inputs and fixed objectives, limiting adaptability to natural-language queries.
Recent LLM-based approaches enhance flexibility but struggle with spatial
reasoning and the joint modeling of route-level and POI-level preferences. To
address these limitations, we propose RouteLLM, a hierarchical multi-agent
framework that grounds natural-language intents into constraint-aware routes.
It first parses user queries into structured intents including POIs, paths, and
constraints. A manager agent then coordinates specialized sub-agents: a
constraint agent that resolves and formally check constraints, a POI agent that
retrieves and ranks candidate POIs, and a path refinement agent that refines
routes via a routing engine with preference-conditioned costs. A final verifier
agent ensures constraint satisfaction and produces the final route with an
interpretable rationale. This design bridges linguistic flexibility and spatial
structure, enabling reasoning over route feasibility and user preferences.
Experiments show that our method reliably grounds textual preferences into
constraint-aware routes, improving route quality and preference satisfaction
over classical methods.

---

### 25. ASPO: Asymmetric Importance Sampling Policy Optimization

**Authors:** Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06062v1](http://arxiv.org/pdf/2510.06062v1)

**Abstract:**

Recent Large Language Model (LLM) post-training methods rely on token-level
clipping mechanisms during Reinforcement Learning (RL). However, we identify a
fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance
Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to
unbalanced token weighting for positive and negative tokens. This mismatch
suppresses the update of low-probability tokens while over-amplifying already
high-probability ones. To address this, we propose Asymmetric Importance
Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy
that flips the IS ratios of positive-advantage tokens, aligning their update
direction with the learning dynamics of negative ones. AIS further incorporates
a soft dual-clipping mechanism to stabilize extreme updates while maintaining
gradient flow. Comprehensive experiments on coding and mathematical reasoning
benchmarks demonstrate that ASPO significantly mitigates premature convergence,
improves training stability, and enhances final performance over strong
GRPO-based baselines. Our analysis provides new insights into the role of
token-level weighting in OSRL and highlights the critical importance of
correcting IS in LLM RL. The code and models of ASPO are available at
https://github.com/wizard-III/Archer2.0.

---

### 26. Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep   Research

**Authors:** Gang Liu, Yihan Zhu, Jie Chen, Meng Jiang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06056v1](http://arxiv.org/pdf/2510.06056v1)

**Abstract:**

Large language models hold promise as scientific assistants, yet existing
agents either rely solely on algorithm evolution or on deep research in
isolation, both of which face critical limitations. Pure algorithm evolution,
as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly
plateaus in complex domains, while pure deep research proposes ideas without
validation, resulting in unrealistic or unimplementable solutions. We present
DeepEvolve, an agent that integrates deep research with algorithm evolution,
uniting external knowledge retrieval, cross-file code editing, and systematic
debugging under a feedback-driven iterative loop. Each iteration not only
proposes new hypotheses but also refines, implements, and tests them, avoiding
both shallow improvements and unproductive over-refinements. Across nine
benchmarks in chemistry, mathematics, biology, materials, and patents,
DeepEvolve consistently improves the initial algorithm, producing executable
new algorithms with sustained gains. By bridging the gap between unguided
evolution and research without grounding, DeepEvolve provides a reliable
framework for advancing scientific algorithm discovery. Our code is available
at https://github.com/liugangcode/deepevolve.

---

### 27. BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection   in Language Model Pretraining

**Authors:** Jie Hao, Rui Yu, Wei Zhang, Huixia Wang, Jie Xu, Mingrui Liu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06048v1](http://arxiv.org/pdf/2510.06048v1)

**Abstract:**

Effective data selection is essential for pretraining large language models
(LLMs), enhancing efficiency and improving generalization to downstream tasks.
However, existing approaches often require leveraging external pretrained
models, making it difficult to disentangle the effects of data selection from
those of the external pretrained models. In addition, they often overlook the
long-term impact of selected data if the model is trained to convergence,
primarily due to the prohibitive cost of full-scale LLM pretraining. In this
paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence
\textbf{S}coring method for data \textbf{S}election): a lightweight data
selection method that operates entirely \emph{from scratch}, without relying on
any external pretrained oracle models, while explicitly accounting for the
long-term impact of selected data. BLISS leverages a small proxy model as a
surrogate for the LLM and employs a score model to estimate the long-term
influence of training samples if the proxy model is trained to convergence. We
formulate data selection as a bilevel optimization problem, where the
upper-level objective optimizes the score model to assign importance weights to
training samples, ensuring that minimizing the lower-level objective (i.e.,
training the proxy model over the weighted training loss until convergence)
leads to best validation performance. Once optimized, the trained score model
predicts influence scores for the dataset, enabling efficient selection of
high-quality samples for LLM pretraining. We validate BLISS by pretraining
410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4
dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$
speedup in reaching the same performance as the state-of-the-art method,
demonstrating superior performance across multiple downstream tasks.

---

### 28. VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via   Tree-based Group Relative Policy Optimization

**Authors:** Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06040v1](http://arxiv.org/pdf/2510.06040v1)

**Abstract:**

Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.

---

### 29. CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive   Evaluation of Chinese LLMs

**Authors:** Chengwei Wu, Jiapu Wang, Mingyang Gao, Xingrui Zhuo, Jipeng Guo, Runlin Lei, Haoran Luo, Tianyu Chen, Haoyi Zhou, Shirui Pan, Zechao Li

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06039v1](http://arxiv.org/pdf/2510.06039v1)

**Abstract:**

Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language processing tasks. However, Chinese LLMs face unique
challenges, primarily due to the dominance of unstructured free text and the
lack of structured representations in Chinese corpora. While existing
benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly
English-centric and fail to address the unique linguistic characteristics of
Chinese, lacking structured datasets essential for robust evaluation. To
address these challenges, we present a Comprehensive Benchmark for Evaluating
Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese
Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million
aligned text pairs, each consisting of unstructured text coupled with one or
more corresponding triples, alongside a total of 15 million triples spanning
four critical domains. The core contributions of CDTP are threefold: (i)
enriching Chinese corpora with high-quality structured information; (ii)
enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)
supporting multi-task fine-tuning to assess generalization and robustness
across scenarios, including Knowledge Graph Completion, Triple-to-Text
generation, and Question Answering. Furthermore, we conduct rigorous
evaluations through extensive experiments and ablation studies to assess the
effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.
To support reproducible research, we offer an open-source codebase and outline
potential directions for future investigations based on our insights.

---

### 30. Evaluating The Impact of Stimulus Quality in Investigations of LLM   Language Performance

**Authors:** Timothy Pistotti, Jason Brown, Michael Witbrock

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06018v1](http://arxiv.org/pdf/2510.06018v1)

**Abstract:**

Recent studies employing Large Language Models (LLMs) to test the Argument
from the Poverty of the Stimulus (APS) have yielded contrasting results across
syntactic phenomena. This paper investigates the hypothesis that
characteristics of the stimuli used in recent studies, including lexical
ambiguities and structural complexities, may confound model performance. A
methodology is proposed for re-evaluating LLM competence on syntactic
prediction, focusing on GPT-2. This involves: 1) establishing a baseline on
previously used (both filtered and unfiltered) stimuli, and 2) generating a
new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5
Pro Preview) guided by linguistically-informed templates designed to mitigate
identified confounds. Our preliminary findings indicate that GPT-2 demonstrates
notably improved performance on these refined PG stimuli compared to baselines,
suggesting that stimulus quality significantly influences outcomes in
surprisal-based evaluations of LLM syntactic competency.

---

### 31. Detection and Measurement of Hailstones with Multimodal Large Language   Models

**Authors:** Moritz Alker, David C. Schedl, Andreas StÃ¶ckl

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06008v1](http://arxiv.org/pdf/2510.06008v1)

**Abstract:**

This study examines the use of social media and news images to detect and
measure hailstones, utilizing pre-trained multimodal large language models. The
dataset for this study comprises 474 crowdsourced images of hailstones from
documented hail events in Austria, which occurred between January 2022 and
September 2024. These hailstones have maximum diameters ranging from 2 to 11cm.
We estimate the hail diameters and compare four different models utilizing
one-stage and two-stage prompting strategies. The latter utilizes additional
size cues from reference objects, such as human hands, within the image. Our
results show that pretrained models already have the potential to measure
hailstone diameters from images with an average mean absolute error of 1.12cm
for the best model. In comparison to a single-stage prompt, two-stage prompting
improves the reliability of most models. Our study suggests that these
off-the-shelf models, even without fine-tuning, can complement traditional hail
sensors by extracting meaningful and spatially dense information from social
media imagery, enabling faster and more detailed assessments of severe weather
events. The automated real-time image harvesting from social media and other
sources remains an open task, but it will make our approach directly applicable
to future hail events.

---

### 32. MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A   Shared Adaptation

**Authors:** Qin Dong, Yuntian Tang, Heming Jia, Yunhang Shen, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Shaohui Lin

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06005v1](http://arxiv.org/pdf/2510.06005v1)

**Abstract:**

Low-Rank Adaptation (LoRA) has emerged as a dominant method in
Parameter-Efficient Fine-Tuning (PEFT) for large language models, which
augments the transformer layer with one down-projection $A$ and one
up-projection $B$. However, LoRA's reliance on a single down-projection matrix
($A$) creates a representational bottleneck, as this solitary feature extractor
is inherently insufficient for capturing the diverse signals required by
complex tasks. This motivates our architectural shift to focus on enriching the
feature adaptation to improve the downstream task adaptation ability. We
propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a
multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is
asymmetrically shared across layers to ensure parameter efficiency. In MASA,
these specialized experts capture diverse features, which are then integrated
by a single, layer-specific $B$-matrix. The effectiveness and versatility of
our method are validated through a comprehensive suite of experiments spanning
multi-domain generalization, single-domain specialization, and multi-task
reasoning. For example, on the MMLU benchmark, MASA achieves an average
accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative
improvement of 1.84%) with comparable learnable parameters of 0.52%.

---

### 33. Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic   Assessments

**Authors:** Timothy Pistotti, Jason Brown, Michael Witbrock

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06001v1](http://arxiv.org/pdf/2510.06001v1)

**Abstract:**

Recent studies probing the Argument from the Poverty of the Stimulus (APS)
have applied Large Language Models (LLMs) to test the learnability of complex
syntax through surprisal-based metrics. However, divergent conclusions raise
questions concerning the insights these metrics offer. While Wilcox et al.
(2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate
that models successfully generalise knowledge of filler-gap dependencies, Lan
et al. (2024) used a Difference-in-Differences (DiD) metric and found that
models largely fail on parasitic gaps (PGs). This paper argues that the direct
minimal pair approach offers greater diagnostic transparency. We demonstrate
this by generating a full 8-permutation paradigm of refined PG stimuli and
evaluating the GPT-2 model used in previous studies with a systematic
Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across
all four tested conditions, indicating robust knowledge of filler-gap licensing
principles even in complex PG environments. This finding, which contrasts with
the more ambiguous results from DiD-style metrics, suggests that the choice of
evaluation metric is critical for assessing an LLM's syntactic competence.

---

### 34. Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning   in LLMs

**Authors:** Xueyan Li, Guinan Su, Mrinmaya Sachan, Jonas Geiping

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05987v1](http://arxiv.org/pdf/2510.05987v1)

**Abstract:**

Large Language Models (LLMs) are increasingly applied to complex tasks that
require extended reasoning. In such settings, models often benefit from diverse
chains-of-thought to arrive at multiple candidate solutions. This requires two
competing objectives: to inject enough stochasticity to explore multiple
reasoning chains, and to ensure sufficient accuracy and quality in each path.
Existing works pursue the first objective by increasing exploration at highly
uncertain steps with higher temperature or larger candidate token sets, while
others improve reliability by rejecting samples with low confidence
post-generation, implying that low confidence correlates with low answer
quality. These two lines of thought are in conflict, as they conflate different
sources of uncertainty. To resolve this, we argue that the decoding rule should
be calibrated by correctness, not confidence alone. We should sample from
tokens with higher estimated correctness, and reduce sampling where expected
correctness is low. We propose simple strategies that achieve this goal:
Greedy-Threshold makes sampling greedy at very low confidence steps.
Calibrated-TopK and Calibrated-epsilon set truncation threshold based on
estimated rank-wise correctness. Together, our findings challenge prevailing
heuristics about decoding under uncertainty and show gains across math and
general reasoning benchmarks.

---

### 35. Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective   Taxonomy and Performance Analysis

**Authors:** Eashan Adhikarla, Yixin Liu, Brian D. Davison

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05976v1](http://arxiv.org/pdf/2510.05976v1)

**Abstract:**

Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.

---

### 36. LexiCon: a Benchmark for Planning under Temporal Constraints in Natural   Language

**Authors:** Periklis Mantenoglou, Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05972v1](http://arxiv.org/pdf/2510.05972v1)

**Abstract:**

Owing to their reasoning capabilities, large language models (LLMs) have been
evaluated on planning tasks described in natural language. However, LLMs have
largely been tested on planning domains without constraints. In order to deploy
them in real-world settings where adherence to constraints, in particular
safety constraints, is critical, we need to evaluate their performance on
constrained planning tasks. We introduce LexiCon -- a natural language-based
(Lexi) constrained (Con) planning benchmark, consisting of a suite of
environments, that can be used to evaluate the planning capabilities of LLMs in
a principled fashion. The core idea behind LexiCon is to take existing planning
environments and impose temporal constraints on the states. These constrained
problems are then translated into natural language and given to an LLM to
solve. A key feature of LexiCon is its extensibility. That is, the set of
supported environments can be extended with new (unconstrained) environment
generators, for which temporal constraints are constructed automatically. This
renders LexiCon future-proof: the hardness of the generated planning problems
can be increased as the planning capabilities of LLMs improve. Our experiments
reveal that the performance of state-of-the-art LLMs, including reasoning
models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of
the planning tasks increases.

---

### 37. Probing the Difficulty Perception Mechanism of Large Language Models

**Authors:** Sunbowen Lee, Qingyu Yin, Chak Tou Leong, Jialiang Zhang, Yicheng Gong, Xiaoyu Shen

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05969v1](http://arxiv.org/pdf/2510.05969v1)

**Abstract:**

Large language models (LLMs) are increasingly deployed on complex reasoning
tasks, yet little is known about their ability to internally evaluate problem
difficulty, which is an essential capability for adaptive reasoning and
efficient resource allocation. In this work, we investigate whether LLMs
implicitly encode problem difficulty in their internal representations. Using a
linear probe on the final-token representations of LLMs, we demonstrate that
the difficulty level of math problems can be linearly modeled. We further
locate the specific attention heads of the final Transformer layer: these
attention heads have opposite activation patterns for simple and difficult
problems, thus achieving perception of difficulty. Our ablation experiments
prove the accuracy of the location. Crucially, our experiments provide
practical support for using LLMs as automatic difficulty annotators,
potentially substantially reducing reliance on costly human labeling in
benchmark construction and curriculum learning. We also uncover that there is a
significant difference in entropy and difficulty perception at the token level.
Our study reveals that difficulty perception in LLMs is not only present but
also structurally organized, offering new theoretical insights and practical
directions for future research.

---

### 38. Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents

**Authors:** Songyuan Sui, Zihang Xu, Yu-Neng Chuang, Kwei-Herng Lai, Xia Hu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05950v1](http://arxiv.org/pdf/2510.05950v1)

**Abstract:**

Time series classification (TSC) spans diverse application scenarios, yet
labeled data are often scarce, making task-specific training costly and
inflexible. Recent reasoning-oriented large language models (LLMs) show promise
in understanding temporal patterns, but purely zero-shot usage remains
suboptimal. We propose FETA, a multi-agent framework for training-free TSC via
exemplar-based in-context reasoning. FETA decomposes a multivariate series into
channel-wise subproblems, retrieves a few structurally similar labeled examples
for each channel, and leverages a reasoning LLM to compare the query against
these exemplars, producing channel-level labels with self-assessed confidences;
a confidence-weighted aggregator then fuses all channel decisions. This design
eliminates the need for pretraining or fine-tuning, improves efficiency by
pruning irrelevant channels and controlling input length, and enhances
interpretability through exemplar grounding and confidence estimation. On nine
challenging UEA datasets, FETA achieves strong accuracy under a fully
training-free setting, surpassing multiple trained baselines. These results
demonstrate that a multi-agent in-context reasoning framework can transform
LLMs into competitive, plug-and-play TSC solvers without any parameter
training. The code is available at https://github.com/SongyuanSui/FETATSC.

---

### 39. EARL: Efficient Agentic Reinforcement Learning Systems for Large   Language Models

**Authors:** Zheyue Tan, Mustapha Abdullahi, Tuo Shi, Huining Yuan, Zelai Xu, Chao Yu, Boxun Li, Bo Zhao

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05943v1](http://arxiv.org/pdf/2510.05943v1)

**Abstract:**

Reinforcement learning (RL) has become a pivotal component of large language
model (LLM) post-training, and agentic RL extends this paradigm to operate as
agents through multi-turn interaction and tool use. Scaling such systems
exposes two practical bottlenecks: (1) context length grows rapidly during
training, inflating memory usage and latency, and triggering out-of-memory
(OOM) failures; and (2) intermediate tensors accumulate with context length,
making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a
parallelism selector that dynamically adapts model and training parallelism
across RL stages based on sequence length and system load, and a data
dispatcher that performs layout-aware, decentralized exchange of intermediate
data batches. Together, these components increase throughput, reduce
long-context failures, and enable stable large-scale training of agentic LLMs
without relying on hard limits or penalties of context length.

---

### 40. EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation   for Moral Alignment in Large Language Models

**Authors:** Hadi Mohammadi, Anastasia Giachanou, Ayoub Bagheri

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05942v1](http://arxiv.org/pdf/2510.05942v1)

**Abstract:**

We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that
uses two scoring methods (log-probabilities and direct ratings) plus a
model-as-judge peer review to evaluate moral alignment in 20 large language
models. We assess models on the World Values Survey (55 countries, 19 topics)
and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,
top models align closely with survey responses (Pearson's r approximately 0.90
on WVS). Yet we find a clear regional difference: Western regions average
r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),
indicating consistent regional bias. Our framework adds three parts: (1) two
scoring methods for all models to enable fair comparison, (2) a structured
chain-of-thought protocol with self-consistency checks, and (3) a
model-as-judge peer review that flags 348 conflicts using a data-driven
threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,
both p<.001), supporting automated quality checks. These results show real
progress toward culture-aware AI while highlighting open challenges for use
across regions.

---

### 41. LLM-FS-Agent: A Deliberative Role-based Large Language Model   Architecture for Transparent Feature Selection

**Authors:** Mohamed Bal-Ghaoui, Fayssal Sabri

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05935v1](http://arxiv.org/pdf/2510.05935v1)

**Abstract:**

High-dimensional data remains a pervasive challenge in machine learning,
often undermining model interpretability and computational efficiency. While
Large Language Models (LLMs) have shown promise for dimensionality reduction
through feature selection, existing LLM-based approaches frequently lack
structured reasoning and transparent justification for their decisions. This
paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for
interpretable and robust feature selection. The system orchestrates a
deliberative "debate" among multiple LLM agents, each assigned a specific role,
enabling collective evaluation of feature relevance and generation of detailed
justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the
CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance
against strong baselines, including LLM-Select and traditional methods such as
PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves
superior or comparable classification performance while reducing downstream
training time by an average of 46% (statistically significant improvement, p =
0.028 for XGBoost). These findings highlight that the proposed deliberative
architecture enhances both decision transparency and computational efficiency,
establishing LLM-FS-Agent as a practical and reliable solution for real-world
applications.

---

### 42. Hire Your Anthropologist! Rethinking Culture Benchmarks Through an   Anthropological Lens

**Authors:** Mai AlKhamissi, Yunze Xiao, Badr AlKhamissi, Mona Diab

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05931v1](http://arxiv.org/pdf/2510.05931v1)

**Abstract:**

Cultural evaluation of large language models has become increasingly
important, yet current benchmarks often reduce culture to static facts or
homogeneous values. This view conflicts with anthropological accounts that
emphasize culture as dynamic, historically situated, and enacted in practice.
To analyze this gap, we introduce a four-part framework that categorizes how
benchmarks frame culture, such as knowledge, preference, performance, or bias.
Using this lens, we qualitatively examine 20 cultural benchmarks and identify
six recurring methodological issues, including treating countries as cultures,
overlooking within-culture diversity, and relying on oversimplified survey
formats. Drawing on established anthropological methods, we propose concrete
improvements: incorporating real-world narratives and scenarios, involving
cultural communities in design and validation, and evaluating models in context
rather than isolation. Our aim is to guide the development of cultural
benchmarks that go beyond static recall tasks and more accurately capture the
responses of the models to complex cultural situations.

---

### 43. Prompt reinforcing for long-term planning of large language models

**Authors:** Hsien-Chin Lin, Benjamin Matthias Ruppik, Carel van Niekerk, Chia-Hao Shen, Michael Heck, Nurul Lubis, Renato Vukovic, Shutong Feng, Milica GaÅ¡iÄ

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05921v1](http://arxiv.org/pdf/2510.05921v1)

**Abstract:**

Large language models (LLMs) have achieved remarkable success in a wide range
of natural language processing tasks and can be adapted through prompting.
However, they remain suboptimal in multi-turn interactions, often relying on
incorrect early assumptions and failing to track user goals over time, which
makes such tasks particularly challenging. Prior works in dialogue systems have
shown that long-term planning is essential for handling interactive tasks. In
this work, we propose a prompt optimisation framework inspired by reinforcement
learning, which enables such planning to take place by only modifying the task
instruction prompt of the LLM-based agent. By generating turn-by-turn feedback
and leveraging experience replay for prompt rewriting, our proposed method
shows significant improvement in multi-turn tasks such as text-to-SQL and
task-oriented dialogue. Moreover, it generalises across different LLM-based
agents and can leverage diverse LLMs as meta-prompting agents. This warrants
future research in reinforcement learning-inspired parameter-free optimisation
methods.

---

### 44. Optimizing for Persuasion Improves LLM Generalization: Evidence from   Quality-Diversity Evolution of Debate Strategies

**Authors:** Aksel Joonas Reedi, Corentin LÃ©ger, Julien Pourcel, Loris Gaven, Perrine Charriau, Guillaume Pourcel

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05909v1](http://arxiv.org/pdf/2510.05909v1)

**Abstract:**

Large Language Models (LLMs) optimized to output truthful answers often
overfit, producing brittle reasoning that fails to generalize. While
persuasion-based optimization has shown promise in debate settings, it has not
been systematically compared against mainstream truth-based approaches. We
introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm
that evolves diverse debate strategies across different categories
(rationality, authority, emotional appeal, etc.) through tournament-style
competitions where two LLMs debate while a third judges. Unlike previously
proposed methods that require a population of LLMs, our approach maintains
diversity of opponents through prompt-based strategies within a single LLM
architecture, making it more accessible for experiments while preserving the
key benefits of population-based optimization. In contrast to prior work, we
explicitly isolate the role of the optimization objective by fixing the debate
protocol and swapping only the fitness function: persuasion rewards strategies
that convince the judge irrespective of truth, whereas truth rewards
collaborative correctness. Across three model scales (7B, 32B, 72B parameters)
and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized
strategies achieve up to 13.94% smaller train-test generalization gaps, while
matching or exceeding truth optimization's test performance. These results
provide the first controlled evidence that competitive pressure to persuade,
rather than seek the truth collaboratively, fosters more transferable reasoning
skills, offering a promising path for improving LLM generalization.

---

### 45. The fragility of "cultural tendencies" in LLMs

**Authors:** Kun Sun, Rong Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05869v1](http://arxiv.org/pdf/2510.05869v1)

**Abstract:**

In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large
language models (LLMs), when prompted in different languages, display
culturally specific tendencies. They report that the two models (i.e., GPT and
ERNIE) respond in more interdependent and holistic ways when prompted in
Chinese, and more independent and analytic ways when prompted in English. LSZ
attribute these differences to deep-seated cultural patterns in the models,
claiming that prompt language alone can induce substantial cultural shifts.
While we acknowledge the empirical patterns they observed, we find their
experiments, methods, and interpretations problematic. In this paper, we
critically re-evaluate the methodology, theoretical framing, and conclusions of
LSZ. We argue that the reported "cultural tendencies" are not stable traits but
fragile artifacts of specific models and task design. To test this, we
conducted targeted replications using a broader set of LLMs and a larger number
of test items. Our results show that prompt language has minimal effect on
outputs, challenging LSZ's claim that these models encode grounded cultural
beliefs.

---

### 46. Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input

**Authors:** Faeze Ghorbanpour, Alexander Fraser

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05864v1](http://arxiv.org/pdf/2510.05864v1)

**Abstract:**

Large language models (LLMs) increasingly support applications that rely on
extended context, from document processing to retrieval-augmented generation.
While their long-context capabilities are well studied for reasoning and
retrieval, little is known about their behavior in safety-critical scenarios.
We evaluate LLMs' sensitivity to harmful content under extended context,
varying type (explicit vs. implicit), position (beginning, middle, end),
prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).
Across harmful content categories such as toxic, offensive, and hate speech,
with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance
peaks at moderate harmful prevalence (0.25) but declines when content is very
sparse or dominant; recall decreases with increasing context length; harmful
sentences at the beginning are generally detected more reliably; and explicit
content is more consistently recognized than implicit. These findings provide
the first systematic view of how LLMs prioritize and calibrate harmful content
in long contexts, highlighting both their emerging strengths and the challenges
that remain for safety-critical use.

---

### 47. Automated Boilerplate: Prevalence and Quality of Contract Generators in   the Context of Swiss Privacy Policies

**Authors:** Luka Nenadic, David Rodriguez

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05860v1](http://arxiv.org/pdf/2510.05860v1)

**Abstract:**

It has become increasingly challenging for firms to comply with a plethora of
novel digital regulations. This is especially true for smaller businesses that
often lack both the resources and know-how to draft complex legal documents.
Instead of seeking costly legal advice from attorneys, firms may turn to
cheaper alternative legal service providers such as automated contract
generators. While these services have a long-standing presence, there is little
empirical evidence on their prevalence and output quality.
  We address this gap in the context of a 2023 Swiss privacy law revision. To
enable a systematic evaluation, we create and annotate a multilingual benchmark
dataset that captures key compliance obligations under Swiss and EU privacy
law. Using this dataset, we validate a novel GPT-5-based method for large-scale
compliance assessment of privacy policies, allowing us to measure the impact of
the revision. We observe compliance increases indicating an effect of the
revision. Generators, explicitly referenced by 18% of local websites, are
associated with substantially higher levels of compliance, with increases of up
to 15 percentage points compared to privacy policies without generator use.
These findings contribute to three debates: the potential of LLMs for
cross-lingual legal analysis, the Brussels Effect of EU regulations, and,
crucially, the role of automated tools in improving compliance and contractual
quality.

---

### 48. DACP: Domain-Adaptive Continual Pre-Training of Large Language Models   for Phone Conversation Summarization

**Authors:** Xue-Yong Fu, Elena Khasanova, Md Tahmid Rahman Laskar, Harsh Saini, Shashi Bhushan TN

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05858v1](http://arxiv.org/pdf/2510.05858v1)

**Abstract:**

Large language models (LLMs) have achieved impressive performance in text
summarization, yet their performance often falls short when applied to
specialized domains %or conversational data that differ from their original
pre-training distribution. While fine-tuning can improve summarization quality,
it typically relies on costly and scarce high-quality labeled data. In this
work, we explore continual pre-training as a scalable, self-supervised approach
to adapt LLMs for downstream summarization tasks, particularly in the context
of noisy real-world conversation transcripts. We conduct extensive experiments
using large-scale, unlabeled business conversation data to investigate whether
continual pre-training enhances model capabilities in conversational
summarization. Our results demonstrate that continual pre-training yields
substantial gains in both in-domain and out-of-domain summarization benchmarks,
while maintaining strong generalization and robustness. We also analyze the
effects of data selection strategies, providing practical guidelines for
applying continual pre-training in summarization-focused industrial
applications.

---

### 49. Luth: Efficient French Specialization for Small Language Models and   Cross-Lingual Transfer

**Authors:** Maxence Lasbordes, SinouÃ© Gad

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05846v1](http://arxiv.org/pdf/2510.05846v1)

**Abstract:**

The landscape of Large Language Models (LLMs) remains predominantly
English-centric, resulting in a significant performance gap for other major
languages, such as French, especially in the context of Small Language Models
(SLMs). Existing multilingual models demonstrate considerably lower performance
in French compared to English, and research on efficient adaptation methods for
French remains limited. To address this, we introduce \textbf{Luth}, a family
of French-specialized SLMs: through targeted post-training on curated,
high-quality French data, our models outperform all open-source counterparts of
comparable size on multiple French benchmarks while retaining their original
English capabilities. We further show that strategic model merging enhances
performance in both languages, establishing Luth as a new state of the art for
French SLMs and a robust baseline for future French-language research.

---

### 50. EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget

**Authors:** Liang Chen, Xueting Han, Qizhou Wang, Bo Han, Jing Bai, Hinrich Schutze, Kam-Fai Wong

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05837v1](http://arxiv.org/pdf/2510.05837v1)

**Abstract:**

Balancing exploration and exploitation remains a central challenge in
reinforcement learning with verifiable rewards (RLVR) for large language models
(LLMs). Current RLVR methods often overemphasize exploitation, leading to
entropy collapse, diminished exploratory capacity, and ultimately limited
performance gains. Although techniques that increase policy stochasticity can
promote exploration, they frequently fail to escape dominant behavioral modes.
This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant
modes-that further erodes exploration. We introduce Exploration-Enhanced Policy
Optimization (EEPO), a framework that promotes exploration via two-stage
rollouts with adaptive unlearning. In the first stage, the model generates half
of the trajectories; it then undergoes a lightweight unlearning step to
temporarily suppress these sampled responses, forcing the second stage to
explore different regions of the output space. This sample-then-forget
mechanism disrupts the self-reinforcing loop and promotes wider exploration
during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,
achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on
Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.

---

### 51. StereoSync: Spatially-Aware Stereo Audio Generation from Video

**Authors:** Christian Marinoni, Riccardo Fosco Gramaccioni, Kazuki Shimada, Takashi Shibuya, Yuki Mitsufuji, Danilo Comminiello

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05828v1](http://arxiv.org/pdf/2510.05828v1)

**Abstract:**

Although audio generation has been widely studied over recent years,
video-aligned audio generation still remains a relatively unexplored frontier.
To address this gap, we introduce StereoSync, a novel and efficient model
designed to generate audio that is both temporally synchronized with a
reference video and spatially aligned with its visual context. Moreover,
StereoSync also achieves efficiency by leveraging pretrained foundation models,
reducing the need for extensive training while maintaining high-quality
synthesis. Unlike existing methods that primarily focus on temporal
synchronization, StereoSync introduces a significant advancement by
incorporating spatial awareness into video-aligned audio generation. Indeed,
given an input video, our approach extracts spatial cues from depth maps and
bounding boxes, using them as cross-attention conditioning in a diffusion-based
audio generation model. Such an approach allows StereoSync to go beyond simple
synchronization, producing stereo audio that dynamically adapts to the spatial
structure and movement of a video scene. We evaluate StereoSync on Walking The
Maps, a curated dataset comprising videos from video games that feature
animated characters walking through diverse environments. Experimental results
demonstrate the ability of StereoSync to achieve both temporal and spatial
alignment, advancing the state of the art in video-to-audio generation and
resulting in a significantly more immersive and realistic audio experience.

---

### 52. VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought   Reasoning for Language-driven Grasp Generation

**Authors:** Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Yuedi Zhang, Qi Zhang, Pengxiang Ding, Cheng Chi, Donglin Wang, Badong Chen

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05827v1](http://arxiv.org/pdf/2510.05827v1)

**Abstract:**

Robotic grasping is one of the most fundamental tasks in robotic
manipulation, and grasp detection/generation has long been the subject of
extensive research. Recently, language-driven grasp generation has emerged as a
promising direction due to its practical interaction capabilities. However,
most existing approaches either lack sufficient reasoning and generalization
capabilities or depend on complex modular pipelines. Moreover, current grasp
foundation models tend to overemphasize dialog and object semantics, resulting
in inferior performance and restriction to single-object grasping. To maintain
strong reasoning ability and generalization in cluttered environments, we
propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates
visual chain-of-thought reasoning to enhance visual understanding for grasp
generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically
focuses on visual inputs while providing interpretable reasoning traces. For
training, we refine and introduce a large-scale dataset, VCoT-GraspSet,
comprising 167K synthetic images with over 1.36M grasps, as well as 400+
real-world images with more than 1.2K grasps, annotated with intermediate
bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot
demonstrate that our method significantly improves grasp success rates and
generalizes effectively to unseen objects, backgrounds, and distractors. More
details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.

---

### 53. Data-efficient Targeted Token-level Preference Optimization for   LLM-based Text-to-Speech

**Authors:** Rikuto Kotoge, Yuichi Sasaki

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05799v1](http://arxiv.org/pdf/2510.05799v1)

**Abstract:**

Aligning text-to-speech (TTS) system outputs with human feedback through
preference optimization has been shown to effectively improve the robustness
and naturalness of language model-based TTS models. Current approaches
primarily require paired desirable and undesirable samples at the utterance
level. However, such pairs are often limited in TTS output data, and
utterance-level formulation prevents fine-grained token-level optimization
needed for accurate pronunciation alignment. In this study, we propose TKTO
that eliminates the need for paired data, enabling a more data-efficient
training paradigm, and directly targets token-level units, automatically
providing fine-grained alignment signals without token-level annotations. TKTO
improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,
automatically assigning 12.8 times stronger reward to targeted tokens.

---

### 54. ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level   Constraint Programming

**Authors:** Weichun Shi, Minghao Liu, Wanting Zhang, Langchen Shi, Fuqi Jia, Feifei Ma, Jian Zhang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05774v1](http://arxiv.org/pdf/2510.05774v1)

**Abstract:**

Constraint programming (CP) is a crucial technology for solving real-world
constraint optimization problems (COPs), with the advantages of rich modeling
semantics and high solving efficiency. Using large language models (LLMs) to
generate formal modeling automatically for COPs is becoming a promising
approach, which aims to build trustworthy neuro-symbolic AI with the help of
symbolic solvers. However, CP has received less attention compared to works
based on operations research (OR) models. We introduce ConstraintLLM, the first
LLM specifically designed for CP modeling, which is trained on an open-source
LLM with multi-instruction supervised fine-tuning. We propose the
Constraint-Aware Retrieval Module (CARM) to increase the in-context learning
capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with
guided self-correction mechanism. Moreover, we construct and release IndusCP,
the first industrial-level benchmark for CP modeling, which contains 140
challenging tasks from various domains. Our experiments demonstrate that
ConstraintLLM achieves state-of-the-art solving accuracy across multiple
benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.
Code and data are available at: https://github.com/william4s/ConstraintLLM.

---

### 55. Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music

**Authors:** Aleksandr Lukoianov, Anssi Klapuri

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05756v1](http://arxiv.org/pdf/2510.05756v1)

**Abstract:**

Whereas chord transcription has received considerable attention during the
past couple of decades, far less work has been devoted to transcribing and
encoding the rhythmic patterns that occur in a song. The topic is especially
relevant for instruments such as the rhythm guitar, which is typically played
by strumming rhythmic patterns that repeat and vary over time. However, in many
cases one cannot objectively define a single "right" rhythmic pattern for a
given song section. To create a dataset with well-defined ground-truth labels,
we asked expert musicians to transcribe the rhythmic patterns in 410 popular
songs and record cover versions where the guitar tracks followed those
transcriptions. To transcribe the strums and their corresponding rhythmic
patterns, we propose a three-step framework. Firstly, we perform approximate
stem separation to extract the guitar part from the polyphonic mixture.
Secondly, we detect individual strums within the separated guitar audio, using
a pre-trained foundation model (MERT) as a backbone. Finally, we carry out a
pattern-decoding process in which the transcribed sequence of guitar strums is
represented by patterns drawn from an expert-curated vocabulary. We show that
it is possible to transcribe the rhythmic patterns of the guitar track in
polyphonic music with quite high accuracy, producing a representation that is
human-readable and includes automatically detected bar lines and time signature
markers. We perform ablation studies and error analysis and propose a set of
evaluation metrics to assess the accuracy and readability of the predicted
rhythmic pattern sequence.

---

### 56. Empirical Comparison of Membership Inference Attacks in Deep Transfer   Learning

**Authors:** Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, Antti Honkela

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05753v1](http://arxiv.org/pdf/2510.05753v1)

**Abstract:**

With the emergence of powerful large-scale foundation models, the training
paradigm is increasingly shifting from from-scratch training to transfer
learning. This enables high utility training with small, domain-specific
datasets typical in sensitive applications.Membership inference attacks (MIAs)
provide an empirical estimate of the privacy leakage by machine learning
models. Yet, prior assessments of MIAs against models fine-tuned with transfer
learning rely on a small subset of possible attacks. We address this by
comparing performance of diverse MIAs in transfer learning settings to help
practitioners identify the most efficient attacks for privacy risk evaluation.
We find that attack efficacy decreases with the increase in training data for
score-based MIAs. We find that there is no one MIA which captures all privacy
risks in models trained with transfer learning. While the Likelihood Ratio
Attack (LiRA) demonstrates superior performance across most experimental
scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against
models fine-tuned on PatchCamelyon dataset in high data regime.

---

### 57. Communication Enables Cooperation in LLM Agents: A Comparison with   Curriculum-Based Approaches

**Authors:** Hachem Madmoun, Salem Lahlou

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05748v1](http://arxiv.org/pdf/2510.05748v1)

**Abstract:**

Eliciting cooperation in multi-agent LLM systems is critical for AI
alignment. We investigate two approaches: direct communication and curriculum
learning. In a 4-player Stag Hunt, a one-word "cheap talk" channel increases
cooperation from 0% to 48.3%, demonstrating communication as a robust
coordination mechanism. In contrast, we find that curriculum learning is highly
sensitive to design choices: our pedagogical curriculum through progressively
complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game
with Punishment. Qualitative analysis reveals that curricula emphasizing
defection-equilibrium games can induce "learned pessimism" in agents. These
findings suggest that for coordination problems, simple communication protocols
may be more reliable than experience-based training, and that curriculum design
for social dilemmas requires careful attention to the strategic lessons
embedded in game sequences.

---

### 58. ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent   Systems

**Authors:** Bohan Yao, Shiva Krishna Reddy Malay, Vikas Yadav

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05746v1](http://arxiv.org/pdf/2510.05746v1)

**Abstract:**

Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved
state-of-the-art results on various complex reasoning tasks. Recent works have
proposed techniques to automate the design of MASes, eliminating the need for
manual engineering. However, these techniques perform poorly, often achieving
similar or inferior performance to simple baselines. Furthermore, they require
computationally expensive re-discovery of architectures for each new task
domain and expensive data annotation on domains without existing labeled
validation sets. A critical insight is that simple Chain of Thought (CoT)
reasoning often performs competitively with these complex systems, suggesting
that the fundamental reasoning unit of MASes, CoT, warrants further
investigation. To this end, we present a new paradigm for automatic MAS design
that pivots the focus to optimizing CoT reasoning. We introduce the Agentic
Reasoning Module (ARM), an agentic generalization of CoT where each granular
reasoning step is executed by a specialized reasoning module. This module is
discovered through a tree search over the code space, starting from a simple
CoT module and evolved using mutations informed by reflection on execution
traces. The resulting ARM acts as a versatile reasoning building block which
can be utilized as a direct recursive loop or as a subroutine in a learned
meta-orchestrator. Our approach significantly outperforms both manually
designed MASes and state-of-the-art automatic MAS design methods. Crucially,
MASes built with ARM exhibit superb generalization, maintaining high
performance across different foundation models and task domains without further
optimization.

---

### 59. Adaptive and Multi-Source Entity Matching for Name Standardization of   Astronomical Observation Facilities

**Authors:** Liza Fretel, Baptiste Cecconi, Laura Debisschop

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05744v1](http://arxiv.org/pdf/2510.05744v1)

**Abstract:**

This ongoing work focuses on the development of a methodology for generating
a multi-source mapping of astronomical observation facilities. To compare two
entities, we compute scores with adaptable criteria and Natural Language
Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches,
and surface approaches) to map entities extracted from eight semantic
artifacts, including Wikidata and astronomy-oriented resources. We utilize
every property available, such as labels, definitions, descriptions, external
identifiers, and more domain-specific properties, such as the observation
wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a
Large Language Model (LLM) to accept or reject a mapping suggestion and provide
a justification, ensuring the plausibility and FAIRness of the validated
synonym pairs. The resulting mapping is composed of multi-source synonym sets
providing only one standardized label per entity. Those mappings will be used
to feed our Name Resolver API and will be integrated into the International
Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro
platform.

---

### 60. Artificially intelligent agents in the social and behavioral sciences: A   history and outlook

**Authors:** Petter Holme, Milena Tsvetkova

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05743v1](http://arxiv.org/pdf/2510.05743v1)

**Abstract:**

We review the historical development and current trends of artificially
intelligent agents (agentic AI) in the social and behavioral sciences: from the
first programmable computers, and social simulations soon thereafter, to
today's experiments with large language models. This overview emphasizes the
role of AI in the scientific process and the changes brought about, both
through technological advancements and the broader evolution of science from
around 1950 to the present. Some of the specific points we cover include: the
challenges of presenting the first social simulation studies to a world unaware
of computers, the rise of social systems science, intelligent game theoretic
agents, the age of big data and the epistemic upheaval in its wake, and the
current enthusiasm around applications of generative AI, and many other topics.
A pervasive theme is how deeply entwined we are with the technologies we use to
understand ourselves.

---

### 61. Redefining Generalization in Visual Domains: A Two-Axis Framework for   Fake Image Detection with FusionDetect

**Authors:** Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05740v1](http://arxiv.org/pdf/2510.05740v1)

**Abstract:**

The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect

---

### 62. Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot   Fault Diagnosis on the Edge

**Authors:** Zijun Jia, Shuang Liang, Jinsong Yu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05733v1](http://arxiv.org/pdf/2510.05733v1)

**Abstract:**

Industrial fault diagnosis faces the dual challenges of data scarcity and the
difficulty of deploying large AI models in resource-constrained environments.
This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that
leverages Large Language Models to overcome these limitations in few-shot fault
diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic
Synergy, which aligns signal features with the LLM's semantic space through
cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically
constructs contextual prompts to enhance diagnostic accuracy with limited
samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create
a lightweight, efficient edge model capable of online updates via a shared
decision space. Extensive experiments on six datasets covering different CWRU
and SEU working conditions show that Syn-Diag significantly outperforms
existing methods, especially in 1-shot and cross-condition scenarios. The edge
model achieves performance comparable to the cloud version while reducing model
size by 83% and latency by 50%, offering a practical, robust, and deployable
paradigm for modern intelligent diagnostics.

---

### 63. FinReflectKG - EvalBench: Benchmarking Financial KG with   Multi-Dimensional Evaluation

**Authors:** Fabrizio Dimino, Abhinav Arun, Bhaskarjit Sarmah, Stefano Pasquali

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05710v1](http://arxiv.org/pdf/2510.05710v1)

**Abstract:**

Large language models (LLMs) are increasingly being used to extract
structured knowledge from unstructured financial text. Although prior studies
have explored various extraction methods, there is no universal benchmark or
unified evaluation framework for the construction of financial knowledge graphs
(KG). We introduce FinReflectKG - EvalBench, a benchmark and evaluation
framework for KG extraction from SEC 10-K filings. Building on the agentic and
holistic evaluation principles of FinReflectKG - a financial KG linking audited
triples to source chunks from S&P 100 filings and supporting single-pass,
multi-pass, and reflection-agent-based extraction modes - EvalBench implements
a deterministic commit-then-justify judging protocol with explicit bias
controls, mitigating position effects, leniency, verbosity and world-knowledge
reliance. Each candidate triple is evaluated with binary judgments of
faithfulness, precision, and relevance, while comprehensiveness is assessed on
a three-level ordinal scale (good, partial, bad) at the chunk level. Our
findings suggest that, when equipped with explicit bias controls, LLM-as-Judge
protocols provide a reliable and cost-efficient alternative to human
annotation, while also enabling structured error analysis. Reflection-based
extraction emerges as the superior approach, achieving best performance in
comprehensiveness, precision, and relevance, while single-pass extraction
maintains the highest faithfulness. By aggregating these complementary
dimensions, FinReflectKG - EvalBench enables fine-grained benchmarking and
bias-aware evaluation, advancing transparency and governance in financial AI
applications.

---

### 64. Towards Reliable and Practical LLM Security Evaluations via Bayesian   Modelling

**Authors:** Mary Llewellyn, Annie Gray, Josh Collyer, Michael Harries

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05709v1](http://arxiv.org/pdf/2510.05709v1)

**Abstract:**

Before adopting a new large language model (LLM) architecture, it is critical
to understand vulnerabilities accurately. Existing evaluations can be difficult
to trust, often drawing conclusions from LLMs that are not meaningfully
comparable, relying on heuristic inputs or employing metrics that fail to
capture the inherent uncertainty. In this paper, we propose a principled and
practical end-to-end framework for evaluating LLM vulnerabilities to prompt
injection attacks. First, we propose practical approaches to experimental
design, tackling unfair LLM comparisons by considering two practitioner
scenarios: when training an LLM and when deploying a pre-trained LLM. Second,
we address the analysis of experiments and propose a Bayesian hierarchical
model with embedding-space clustering. This model is designed to improve
uncertainty quantification in the common scenario that LLM outputs are not
deterministic, test prompts are designed imperfectly, and practitioners only
have a limited amount of compute to evaluate vulnerabilities. We show the
improved inferential capabilities of the model in several prompt injection
attack settings. Finally, we demonstrate the pipeline to evaluate the security
of Transformer versus Mamba architectures. Our findings show that consideration
of output variability can suggest less definitive findings. However, for some
attacks, we find notably increased Transformer and Mamba-variant
vulnerabilities across LLMs with the same training data or mathematical
ability.

---

### 65. Primal-Dual Direct Preference Optimization for Constrained LLM Alignment

**Authors:** Yihan Du, Seo Taek Kong, R. Srikant

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05703v1](http://arxiv.org/pdf/2510.05703v1)

**Abstract:**

The widespread application of Large Language Models (LLMs) imposes increasing
demands on safety, such as reducing harmful content and fake information, and
avoiding certain forbidden tokens due to rules and laws. While there have been
several recent works studying safe alignment of LLMs, these works either
require the training of reward and cost models and incur high memory and
computational costs, or need prior knowledge about the optimal solution.
Motivated by this fact, we study the problem of constrained alignment in LLMs,
i.e., maximizing the output reward while restricting the cost due to
potentially unsafe content to stay below a threshold. For this problem, we
propose a novel primal-dual DPO approach, which first trains a model using
standard DPO on reward preference data to provide reward information, and then
adopts a rearranged Lagrangian DPO objective utilizing the provided reward
information to fine-tune LLMs on cost preference data. Our approach
significantly reduces memory and computational costs, and does not require
extra prior knowledge. Moreover, we establish rigorous theoretical guarantees
on the suboptimality and constraint violation of the output policy. We also
extend our approach to an online data setting by incorporating exploration
bonuses, which enables our approach to explore uncovered prompt-response space,
and then provide theoretical results that get rid of the dependence on
preference data coverage. Experimental results on the widely-used preference
dataset PKU-SafeRLHF demonstrate the effectiveness of our approach.

---

### 66. Uncovering Representation Bias for Investment Decisions in Open-Source   Large Language Models

**Authors:** Fabrizio Dimino, Krati Saxena, Bhaskarjit Sarmah, Stefano Pasquali

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05702v1](http://arxiv.org/pdf/2510.05702v1)

**Abstract:**

Large Language Models are increasingly adopted in financial applications to
support investment workflows. However, prior studies have seldom examined how
these models reflect biases related to firm size, sector, or financial
characteristics, which can significantly impact decision-making. This paper
addresses this gap by focusing on representation bias in open-source Qwen
models. We propose a balanced round-robin prompting method over approximately
150 U.S. equities, applying constrained decoding and token-logit aggregation to
derive firm-level confidence scores across financial contexts. Using
statistical tests and variance analysis, we find that firm size and valuation
consistently increase model confidence, while risk factors tend to decrease it.
Confidence varies significantly across sectors, with the Technology sector
showing the greatest variability. When models are prompted for specific
financial categories, their confidence rankings best align with fundamental
data, moderately with technical signals, and least with growth indicators.
These results highlight representation bias in Qwen models and motivate
sector-aware calibration and category-conditioned evaluation protocols for safe
and fair financial LLM deployment.

---

### 67. Membership Inference Attacks on Tokenizers of Large Language Models

**Authors:** Meng Tong, Yuntao Du, Kejiang Chen, Weiming Zhang, Ninghui Li

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05699v1](http://arxiv.org/pdf/2510.05699v1)

**Abstract:**

Membership inference attacks (MIAs) are widely used to assess the privacy
risks associated with machine learning models. However, when these attacks are
applied to pre-trained large language models (LLMs), they encounter significant
challenges, including mislabeled samples, distribution shifts, and
discrepancies in model size between experimental and real-world settings. To
address these limitations, we introduce tokenizers as a new attack vector for
membership inference. Specifically, a tokenizer converts raw text into tokens
for LLMs. Unlike full models, tokenizers can be efficiently trained from
scratch, thereby avoiding the aforementioned challenges. In addition, the
tokenizer's training data is typically representative of the data used to
pre-train LLMs. Despite these advantages, the potential of tokenizers as an
attack vector remains unexplored. To this end, we present the first study on
membership leakage through tokenizers and explore five attack methods to infer
dataset membership. Extensive experiments on millions of Internet samples
reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To
mitigate this emerging risk, we further propose an adaptive defense. Our
findings highlight tokenizers as an overlooked yet critical privacy threat,
underscoring the urgent need for privacy-preserving mechanisms specifically
designed for them.

---

### 68. Joint Communication Scheduling and Velocity Control for   Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context   Learning Approach

**Authors:** Yousef Emami, Seyedsina Nabavirazavi, Jingjing Zheng, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05698v1](http://arxiv.org/pdf/2510.05698v1)

**Abstract:**

Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated
to collect sensory data in post-disaster monitoring scenarios, such as
tsunamis, where early actions are critical to limit coastal damage. A major
challenge is to design the data collection schedules and flight velocities, as
unfavorable schedules and velocities can lead to transmission errors and buffer
overflows of the ground sensors, ultimately resulting in significant packet
loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a
complex training process and a mismatch between simulation and reality that
does not meet the urgent requirements of tsunami monitoring. Recent advances in
Large Language Models (LLMs) offer a compelling alternative. With their strong
reasoning and generalization capabilities, LLMs can adapt to new tasks through
In-Context Learning (ICL), which enables task adaptation through natural
language prompts and example-based guidance without retraining. However, LLM
models have input data limitations and thus require customized approaches. In
this paper, a joint optimization of data collection schedules and velocities
control for multiple UAVs is proposed to minimize data loss. The battery level
of the ground sensors, the length of the queues, and the channel conditions, as
well as the trajectories of the UAVs, are taken into account. Attention-Based
In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS)
is proposed as an alternative to DRL in emergencies. The simulation results
show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and
maximum channel gain baselines.

---

### 69. DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision   and Execution Optimization via Process Supervision

**Authors:** Yongqi Leng, Yikun Lei, Xikai Liu, Meizhi Zhong, Bojian Xiong, Yurong Zhang, Yan Gao, Yi Wu, Yao Hu, Deyi Xiong

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05691v1](http://arxiv.org/pdf/2510.05691v1)

**Abstract:**

Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.

---

### 70. D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to   Embodied AI

**Authors:** Suwhan Choi, Jaeyoon Jung, Haebin Seong, Minchan Kim, Minyeong Kim, Yongjun Cho, Yoonshik Kim, Yubeen Park, Youngjae Yu, Yunsung Lee

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05684v1](http://arxiv.org/pdf/2510.05684v1)

**Abstract:**

Large language models leverage internet-scale text data, yet embodied AI
remains constrained by the prohibitive costs of physical trajectory collection.
Desktop environments -- particularly gaming -- offer a compelling alternative:
they provide rich sensorimotor interactions at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to Embodied AI), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the OWA Toolkit
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the Generalist-IDM that achieves strong zero-shot
generalization across unseen games through timestamp-based event prediction,
enabling internet-scale pseudo-labeling, and (3) VAPT that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on CANVAS navigation benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the OWA toolkit, datasets of human-collected and
pseudo-labeled, and VAPT-trained models available at
https://worv-ai.github.io/d2e/

---

### 71. Code-Switching In-Context Learning for Cross-Lingual Transfer of Large   Language Models

**Authors:** Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05678v1](http://arxiv.org/pdf/2510.05678v1)

**Abstract:**

While large language models (LLMs) exhibit strong multilingual abilities,
their reliance on English as latent representations creates a translation
barrier, where reasoning implicitly depends on internal translation into
English. When this process fails, performance in non-English languages
deteriorates sharply, limiting the inclusiveness of LLM-based applications.
Existing cross-lingual in-context learning (X-ICL) methods primarily leverage
monolingual demonstrations, often failing to mitigate this barrier and instead
reinforcing it. In this work, we introduce code-switching in-context learning
(CSICL), a simple yet effective prompting strategy that progressively
transitions from a target language to English within demonstrations and
instruction to facilitate their latent reasoning in English. By explicitly
scaffolding the reasoning process through controlled code-switching, CSICL acts
as an implicit linguistic bridge that enhances cross-lingual alignment and
reduces reliance on the translation barrier. We conduct extensive experiments
across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive
and reasoning-oriented domains. Our results demonstrate that CSICL consistently
outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target
and unseen languages, respectively. The improvement is even more pronounced in
low-resource settings, with gains of 14.7% in target and 5.3% in unseen
languages. These findings establish code-switching as a principled and robust
approach for overcoming the translation barrier during inference, moving LLMs
toward more equitable and effective multilingual systems.

---

### 72. Large Language Model-Based Uncertainty-Adjusted Label Extraction for   Artificial Intelligence Model Development in Upper Extremity Radiography

**Authors:** Hanna Kreutzer, Anne-Sophie Caselitz, Thomas Dratsch, Daniel Pinto dos Santos, Christiane Kuhl, Daniel Truhn, Sven Nebelung

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05664v1](http://arxiv.org/pdf/2510.05664v1)

**Abstract:**

Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with
uncertainty) from free-text radiology reports and to test how these labels
affect multi-label image classification of musculoskeletal radiographs.
Methods: This retrospective study included radiography series of the clavicle
(n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o
filled out structured templates by indicating imaging findings as present
("true"), absent ("false"), or "uncertain." To assess the impact of label
uncertainty, "uncertain" labels of the training and validation sets were
automatically reassigned to "true" (inclusive) or "false" (exclusive).
Label-image-pairs were used for multi-label classification using ResNet50.
Label extraction accuracy was manually verified on internal (clavicle: n=233,
elbow: n=745, thumb: n=393) and external test sets (n=300 for each).
Performance was assessed using macro-averaged receiver operating characteristic
(ROC) area under the curve (AUC), precision recall curves, sensitivity,
specificity, and accuracy. AUCs were compared with the DeLong test. Results:
Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the
test sets. Across anatomic regions, label-based model training yielded
competitive performance measured by macro-averaged AUC values for inclusive
(e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow:
AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets
(elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79
[range, 0.63-0.89]). No significant differences were observed across labeling
strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from
radiologic reports to train competitive multi-label classification models with
high accuracy. Detected uncertainty in the radiologic reports did not influence
the performance of these models.

---

### 73. From Principles to Practice: A Systematic Study of LLM Serving on   Multi-core NPUs

**Authors:** Tianhao Zhu, Dahu Feng, Erhu Feng, Yubin Xia

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05632v1](http://arxiv.org/pdf/2510.05632v1)

**Abstract:**

With the widespread adoption of Large Language Models (LLMs), the demand for
high-performance LLM inference services continues to grow. To meet this demand,
a growing number of AI accelerators have been proposed, such as Google TPU,
Huawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators
adopt multi-core architectures to achieve enhanced scalability, but lack the
flexibility of SIMT architectures. Therefore, without careful configuration of
the hardware architecture, as well as deliberate design of tensor parallelism
and core placement strategies, computational resources may be underutilized,
resulting in suboptimal inference performance.
  To address these challenges, we first present a multi-level simulation
framework with both transaction-level and performance-model-based simulation
for multi-core NPUs. Using this simulator, we conduct a systematic analysis and
further propose the optimal solutions for tensor parallelism strategies, core
placement policies, memory management methods, as well as the selection between
PD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive
experiments on representative LLMs and various NPU configurations. The
evaluation results demonstrate that, our solution can achieve 1.32x-6.03x
speedup compared to SOTA designs for multi-core NPUs across different hardware
configurations. As for LLM serving, our work offers guidance on designing
optimal hardware architectures and serving strategies for multi-core NPUs
across various LLM workloads.

---

### 74. InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to   Deployment

**Authors:** Ibrahim Salihu Yusuf, Iffanice Houndayi, Rym Oualha, Mohamed Aziz Cherif, Kobby Panford-Quainoo, Arnu Pretorius

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05617v1](http://arxiv.org/pdf/2510.05617v1)

**Abstract:**

Open-access multispectral imagery from missions like Landsat 8-9 and
Sentinel-2 has fueled the development of geospatial foundation models (GFMs)
for humanitarian and environmental applications. Yet, their deployment remains
limited by (i) the absence of automated geospatial data pipelines and (ii) the
large size of fine-tuned models. Existing GFMs lack workflows for processing
raw satellite imagery, and downstream adaptations often retain the full
complexity of the original encoder.
  We present InstaGeo, an open-source, end-to-end framework that addresses
these challenges by integrating: (1) automated data curation to transform raw
imagery into model-ready datasets; (2) task-specific model distillation to
derive compact, compute-efficient models; and (3) seamless deployment as
interactive web-map applications. Using InstaGeo, we reproduced datasets from
three published studies and trained models with marginal mIoU differences of
-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for
desert locust prediction. The distilled models are up to 8x smaller than
standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal
accuracy loss.
  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger
crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp
improvement over prior baselines. Moreover, InstaGeo enables users to progress
from raw data to model deployment within a single working day.
  By unifying data preparation, model compression, and deployment, InstaGeo
transforms research-grade GFMs into practical, low-carbon tools for real-time,
large-scale Earth observation. This approach shifts geospatial AI toward data
quality and application-driven innovation. Source code, datasets, and model
checkpoints are available at:
https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git

---

### 75. MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction

**Authors:** Wei-Chieh Huang, Cornelia Caragea

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05611v1](http://arxiv.org/pdf/2510.05611v1)

**Abstract:**

Implicit Attribute Value Extraction (AVE) is essential for accurately
representing products in e-commerce, as it infers lantent attributes from
multimodal data. Despite advances in multimodal large language models (MLLMs),
implicit AVE remains challenging due to the complexity of multidimensional data
and gaps in vision-text understanding. In this work, we introduce
\textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM
agents to iteratively refine inferences. Through a series of debate rounds,
agents verify and update each other's responses, thereby improving inference
performance and robustness. Experiments on the ImplicitAVE dataset demonstrate
that even a few rounds of debate significantly boost accuracy, especially for
attributes with initially low performance. We systematically evaluate various
debate configurations, including identical or different MLLM agents, and
analyze how debate rounds affect convergence dynamics. Our findings highlight
the potential of multi-agent debate strategies to address the limitations of
single-agent approaches and offer a scalable solution for implicit AVE in
multimodal e-commerce.

---

### 76. HOI-R1: Exploring the Potential of Multimodal Large Language Models for   Human-Object Interaction Detection

**Authors:** Junwen Chen, Peilin Xiong, Keiji Yanai

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05609v1](http://arxiv.org/pdf/2510.05609v1)

**Abstract:**

Recent Human-object interaction detection (HOID) methods highly require prior
knowledge from VLMs to enhance the interaction recognition capabilities. The
training strategies and model architectures for connecting the knowledge from
VLMs to the HOI instance representations from the object detector are
challenging, and the whole framework is complex for further development or
application. On the other hand, the inherent reasoning abilities of MLLMs on
human-object interaction detection are under-explored. Inspired by the recent
success of training MLLMs with reinforcement learning (RL) methods, we propose
HOI-R1 and first explore the potential of the language model on the HOID task
without any additional detection modules. We introduce an HOI reasoning process
and HOID reward functions to solve the HOID task by pure text. The results on
the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline
with great generalization ability. The source code is available at
https://github.com/cjw2021/HOI-R1.

---

### 77. A Goal Without a Plan Is Just a Wish: Efficient and Effective Global   Planner Training for Long-Horizon Agent Tasks

**Authors:** Shuzheng Si, Haozhe Zhao, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05608v1](http://arxiv.org/pdf/2510.05608v1)

**Abstract:**

Agents based on large language models (LLMs) struggle with brainless
trial-and-error and generating hallucinatory actions due to a lack of global
planning in long-horizon tasks. In this paper, we introduce a plan-and-execute
framework and propose EAGLET, an efficient and effective planner training
method to enhance the executor agent's planning abilities without human effort.
Specifically, we train a plug-and-play global planner through a two-step
process: we first synthesize high-quality plans from an advanced LLM using our
proposed homologous consensus filtering strategy, and apply fine-tuning as a
cold start. Moreover, we further improve the planner with a rule-based
reinforcement learning stage using a novel executor capability gain reward,
ensuring it can handle task instructions of varying difficulty. Experiments on
three long-horizon agent tasks show that executor agents equipped with our
planner outperform existing methods, achieving new state-of-the-art
performance. Meanwhile, EAGLET reduces training costs by 8x compared to
RL-based baselines, and it does not require manual effort or extra training
data, offering an efficient and effective solution.

---

### 78. AutoPentester: An LLM Agent-based Framework for Automated Pentesting

**Authors:** Yasod Ginige, Akila Niroshan, Sajal Jain, Suranga Seneviratne

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05605v1](http://arxiv.org/pdf/2510.05605v1)

**Abstract:**

Penetration testing and vulnerability assessment are essential industry
practices for safeguarding computer systems. As cyber threats grow in scale and
complexity, the demand for pentesting has surged, surpassing the capacity of
human professionals to meet it effectively. With advances in AI, particularly
Large Language Models (LLMs), there have been attempts to automate the
pentesting process. However, existing tools such as PentestGPT are still
semi-manual, requiring significant professional human interaction to conduct
pentests. To this end, we propose a novel LLM agent-based framework,
AutoPentester, which automates the pentesting process. Given a target IP,
AutoPentester automatically conducts pentesting steps using common security
tools in an iterative process. It can dynamically generate attack strategies
based on the tool outputs from the previous iteration, mimicking the human
pentester approach. We evaluate AutoPentester using Hack The Box and
custom-made VMs, comparing the results with the state-of-the-art PentestGPT.
Results show that AutoPentester achieves a 27.0% better subtask completion rate
and 39.5% more vulnerability coverage with fewer steps. Most importantly, it
requires significantly fewer human interactions and interventions compared to
PentestGPT. Furthermore, we recruit a group of security industry professional
volunteers for a user survey and perform a qualitative analysis to evaluate
AutoPentester against industry practices and compare it with PentestGPT. On
average, AutoPentester received a score of 3.93 out of 5 based on user reviews,
which was 19.8% higher than PentestGPT.

---

### 79. AgentDR Dynamic Recommendation with Implicit Item-Item Relations via   LLM-based Agents

**Authors:** Mingdai Yang, Nurendra Choudhary, Jiangshu Du, Edward W. Huang, Philip S. Yu, Karthik Subbian, Danai Kourta

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05598v1](http://arxiv.org/pdf/2510.05598v1)

**Abstract:**

Recent agent-based recommendation frameworks aim to simulate user behaviors
by incorporating memory mechanisms and prompting strategies, but they struggle
with hallucinating non-existent items and full-catalog ranking. Besides, a
largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning
to capture user intent through substitute and complement relationships between
items, which are usually implicit in datasets and difficult for traditional
ID-based recommenders to capture. In this work, we propose a novel LLM-agent
framework, AgenDR, which bridges LLM reasoning with scalable recommendation
tools. Our approach delegates full-ranking tasks to traditional models while
utilizing LLMs to (i) integrate multiple recommendation outputs based on
personalized tool suitability and (ii) reason over substitute and complement
relationships grounded in user history. This design mitigates hallucination,
scales to large catalogs, and enhances recommendation relevance through
relational reasoning. Through extensive experiments on three public grocery
datasets, we show that our framework achieves superior full-ranking
performance, yielding on average a twofold improvement over its underlying
tools. We also introduce a new LLM-based evaluation metric that jointly
measures semantic alignment and ranking correctness.

---

### 80. From Agentification to Self-Evolving Agentic AI for Wireless Networks:   Concepts, Approaches, and Future Research Directions

**Authors:** Changyuan Zhao, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Geng Sun, Xianbin Wang, Shiwen Mao, Abbas Jamalipour

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05596v1](http://arxiv.org/pdf/2510.05596v1)

**Abstract:**

Self-evolving agentic artificial intelligence (AI) offers a new paradigm for
future wireless systems by enabling autonomous agents to continually adapt and
improve without human intervention. Unlike static AI models, self-evolving
agents embed an autonomous evolution cycle that updates models, tools, and
workflows in response to environmental dynamics. This paper presents a
comprehensive overview of self-evolving agentic AI, highlighting its layered
architecture, life cycle, and key techniques, including tool intelligence,
workflow optimization, self-reflection, and evolutionary learning. We further
propose a multi-agent cooperative self-evolving agentic AI framework, where
multiple large language models (LLMs) are assigned role-specialized prompts
under the coordination of a supervisor agent. Through structured dialogue,
iterative feedback, and systematic validation, the system autonomously executes
the entire life cycle without human intervention. A case study on antenna
evolution in low-altitude wireless networks (LAWNs) demonstrates how the
framework autonomously upgrades fixed antenna optimization into movable antenna
optimization. Experimental results show that the proposed self-evolving agentic
AI autonomously improves beam gain and restores degraded performance by up to
52.02%, consistently surpassing the fixed baseline with little to no human
intervention and validating its adaptability and robustness for next-generation
wireless intelligence.

---

### 81. Improving Chain-of-Thought Efficiency for Autoregressive Image   Generation

**Authors:** Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, Jiawei Zhou, Abe Davis, Jialiang Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05593v1](http://arxiv.org/pdf/2510.05593v1)

**Abstract:**

Autoregressive multimodal large language models have recently gained
popularity for image generation, driven by advances in foundation models. To
enhance alignment and detail, newer approaches employ chain-of-thought (CoT)
reasoning, expanding user inputs into elaborated prompts prior to image
synthesis. However, this strategy can introduce unnecessary redundancy -- a
phenomenon we call visual overthinking -- which increases computational costs
and can introduce details that contradict the original prompt. In this work, we
explore how to generate more concise CoT sequences for more efficient image
generation. We introduce ShortCoTI, a lightweight optimization framework that
encourages more concise CoT while preserving output image quality. ShortCoTI
rewards more concise prompts with an adaptive function that scales according to
an estimated difficulty for each task. Incorporating this reward into a
reinforcement learning paradigm reduces prompt reasoning length by 54% while
maintaining or slightly improving quality metrics across multiple benchmarks
(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates
verbose explanations and repetitive refinements, producing reasoning prompts
that are both concise and semantically rich. As a result, ShortCoTI improves
computational efficiency without compromising the fidelity or visual appeal of
generated images.

---

### 82. In-the-Flow Agentic System Optimization for Effective Planning and Tool   Use

**Authors:** Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, Pan Lu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05592v1](http://arxiv.org/pdf/2510.05592v1)

**Abstract:**

Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

---

### 83. Deciphering Invariant Feature Decoupling in Source-free Time Series   Forecasting with Proxy Denoising

**Authors:** Kangjia Yan, Chenxi Liu, Hao Miao, Xinle Wu, Yan Zhao, Chenjuan Guo, Bin Yang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05589v1](http://arxiv.org/pdf/2510.05589v1)

**Abstract:**

The proliferation of mobile devices generates a massive volume of time series
across various domains, where effective time series forecasting enables a
variety of real-world applications. This study focuses on a new problem of
source-free domain adaptation for time series forecasting. It aims to adapt a
pretrained model from sufficient source time series to the sparse target time
series domain without access to the source data, embracing data protection
regulations. To achieve this, we propose TimePD, the first source-free time
series forecasting framework with proxy denoising, where large language models
(LLMs) are employed to benefit from their generalization capabilities.
Specifically, TimePD consists of three key components: (1) dual-branch
invariant disentangled feature learning that enforces representation- and
gradient-wise invariance by means of season-trend decomposition; (2)
lightweight, parameter-free proxy denoising that dynamically calibrates
systematic biases of LLMs; and (3) knowledge distillation that bidirectionally
aligns the denoised prediction and the original target prediction. Extensive
experiments on real-world datasets offer insight into the effectiveness of the
proposed TimePD, outperforming SOTA baselines by 9.3% on average.

---

### 84. (Token-Level) \textbf{InfoRMIA}: Stronger Membership Inference and   Memorization Assessment for LLMs

**Authors:** Jiashu Tao, Reza Shokri

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05582v1](http://arxiv.org/pdf/2510.05582v1)

**Abstract:**

Machine learning models are known to leak sensitive information, as they
inevitably memorize (parts of) their training data. More alarmingly, large
language models (LLMs) are now trained on nearly all available data, which
amplifies the magnitude of information leakage and raises serious privacy
risks. Hence, it is more crucial than ever to quantify privacy risk before the
release of LLMs. The standard method to quantify privacy is via membership
inference attacks, where the state-of-the-art approach is the Robust Membership
Inference Attack (RMIA). In this paper, we present InfoRMIA, a principled
information-theoretic formulation of membership inference. Our method
consistently outperforms RMIA across benchmarks while also offering improved
computational efficiency.
  In the second part of the paper, we identify the limitations of treating
sequence-level membership inference as the gold standard for measuring leakage.
We propose a new perspective for studying membership and memorization in LLMs:
token-level signals and analyses. We show that a simple token-based InfoRMIA
can pinpoint which tokens are memorized within generated outputs, thereby
localizing leakage from the sequence level down to individual tokens, while
achieving stronger sequence-level inference power on LLMs. This new scope
rethinks privacy in LLMs and can lead to more targeted mitigation, such as
exact unlearning.

---

### 85. Mission Impossible: Feedback-Guided Dynamic Interactive Planning for   Improving Reasoning on LLMs

**Authors:** Dong Yan, Gaochen Wu, Bowen Zhou

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05577v1](http://arxiv.org/pdf/2510.05577v1)

**Abstract:**

Recent advancements in language agents have led to significant improvements
in multi-hop reasoning tasks. However, existing approaches often struggle with
handling open-domain problems, which require massive information retrieval due
to their reliance on a fixed sequence of actions. To address this, we propose
Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework
tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive
strategies for information exploration in open-domain multi-hop reasoning
tasks. Our approach begins by identifying key entities relevant to the problem,
which serve as the initial nodes in the reasoning process. From these initial
nodes, we then generate reasoning child nodes with the process being refined
through a combination of historical error analysis and real-time feedback,
which allows the framework to dynamically adjust and optimize its reasoning
strategies. By integrating depth-first search with an innovative node
generation technique, our framework adapts based on both prior error paths and
concurrently generated nodes at the same hierarchical level. This dynamic
strategy effectively expands the search space while ensuring the reasoning
process systematically converges toward accurate solutions. Experimental
results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset
and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and
7.25% respectively, highlighting its versatility and potential to enhance
language agents in multi-hop reasoning tasks.

---

### 86. Domain-Shift-Aware Conformal Prediction for Large Language Models

**Authors:** Zhexiao Lin, Yuanyuan Li, Neeraj Sarna, Yuanyuan Gao, Michael von Gablenz

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05566v1](http://arxiv.org/pdf/2510.05566v1)

**Abstract:**

Large language models have achieved impressive performance across diverse
tasks. However, their tendency to produce overconfident and factually incorrect
outputs, known as hallucinations, poses risks in real world applications.
Conformal prediction provides finite-sample, distribution-free coverage
guarantees, but standard conformal prediction breaks down under domain shift,
often leading to under-coverage and unreliable prediction sets. We propose a
new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our
framework adapts conformal prediction to large language models under domain
shift, by systematically reweighting calibration samples based on their
proximity to the test prompt, thereby preserving validity while enhancing
adaptivity. Our theoretical analysis and experiments on the MMLU benchmark
demonstrate that the proposed method delivers more reliable coverage than
standard conformal prediction, especially under substantial distribution
shifts, while maintaining efficiency. This provides a practical step toward
trustworthy uncertainty quantification for large language models in real-world
deployment.

---

### 87. Critical attention scaling in long-context transformers

**Authors:** Shi Chen, Zhengjiang Lin, Yury Polyanskiy, Philippe Rigollet

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05554v1](http://arxiv.org/pdf/2510.05554v1)

**Abstract:**

As large language models scale to longer contexts, attention layers suffer
from a fundamental pathology: attention scores collapse toward uniformity as
context length $n$ increases, causing tokens to cluster excessively, a
phenomenon known as rank-collapse. While $\textit{attention scaling}$
effectively addresses this deficiency by rescaling attention scores with a
polylogarithmic factor $\beta_n$, theoretical justification for this approach
remains lacking.
  We analyze a simplified yet tractable model that magnifies the effect of
attention scaling. In this model, attention exhibits a phase transition
governed by the scaling factor $\beta_n$: insufficient scaling collapses all
tokens to a single direction, while excessive scaling reduces attention to
identity, thereby eliminating meaningful interactions between tokens. Our main
result identifies the critical scaling $\beta_n \asymp \log n$ and provides a
rigorous justification for attention scaling in YaRN and Qwen, clarifying why
logarithmic scaling maintains sparse, content-adaptive attention at large
context lengths.

---

### 88. Activation-Informed Pareto-Guided Low-Rank Compression for Efficient   LLM/VLM

**Authors:** Ryan Solgi, Parsa Madinei, Jiayi Tian, Rupak Swaminathan, Jing Liu, Nathan Susanj, Zheng Zhang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05544v1](http://arxiv.org/pdf/2510.05544v1)

**Abstract:**

Large language models (LLM) and vision-language models (VLM) have achieved
state-of-the-art performance, but they impose significant memory and computing
challenges in deployment. We present a novel low-rank compression framework to
address this challenge. First, we upper bound the change of network loss via
layer-wise activation-based compression errors, filling a theoretical gap in
the literature. We then formulate low-rank model compression as a bi-objective
optimization and prove that a single uniform tolerance yields surrogate
Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we
propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot
pipeline that improves activation-aware compression via Pareto-guided rank
selection and alternating least-squares implementation. We apply PGSVD to both
LLM and VLM, showing better accuracy at the same compression levels and
inference speedup.

---

### 89. Sci-Phi: A Large Language Model Spatial Audio Descriptor

**Authors:** Xilin Jiang, Hannes Gamper, Sebastian Braun

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05542v1](http://arxiv.org/pdf/2510.05542v1)

**Abstract:**

Acoustic scene perception involves describing the type of sounds, their
timing, their direction and distance, as well as their loudness and
reverberation. While audio language models excel in sound recognition,
single-channel input fundamentally limits spatial understanding. This work
presents Sci-Phi, a spatial audio large language model with dual spatial and
spectral encoders that estimates a complete parameter set for all sound sources
and the surrounding environment. Learning from over 4,000 hours of synthetic
first-order Ambisonics recordings including metadata, Sci-Phi enumerates and
describes up to four directional sound sources in one pass, alongside
non-directional background sounds and room characteristics. We evaluate the
model with a permutation-invariant protocol and 15 metrics covering content,
location, timing, loudness, and reverberation, and analyze its robustness
across source counts, signal-to-noise ratios, reverberation levels, and
challenging mixtures of acoustically, spatially, or temporally similar sources.
Notably, Sci-Phi generalizes to real room impulse responses with only minor
performance degradation. Overall, this work establishes the first audio LLM
capable of full spatial-scene description, with strong potential for real-world
deployment. Demo: https://sci-phi-audio.github.io/demo

---

### 90. Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret   and Grade Handwritten Student Work

**Authors:** Owen Henkel, Bill Roberts, Doug Jaffe, Laurence Holt

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05538v1](http://arxiv.org/pdf/2510.05538v1)

**Abstract:**

Recent advances in multimodal large language models (MLLMs) raise the
question of their potential for grading, analyzing, and offering feedback on
handwritten student classwork. This capability would be particularly beneficial
in elementary and middle-school mathematics education, where most work remains
handwritten, because seeing students' full working of a problem provides
valuable insights into their learning processes, but is extremely
time-consuming to grade. We present two experiments investigating MLLM
performance on handwritten student mathematics classwork. Experiment A examines
288 handwritten responses from Ghanaian middle school students solving
arithmetic problems with objective answers. In this context, models achieved
near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human
educators would be unlikely to make. Experiment B evaluates 150 mathematical
illustrations from American elementary students, where the drawings are the
answer to the question. These tasks lack single objective answers and require
sophisticated visual interpretation as well as pedagogical judgment in order to
analyze and evaluate them. We attempted to separate MLLMs' visual capabilities
from their pedagogical abilities by first asking them to grade the student
illustrations directly, and then by augmenting the image with a detailed human
description of the illustration. We found that when the models had to analyze
the student illustrations directly, they struggled, achieving only k = 0.20
with ground truth scores, but when given human descriptions, their agreement
levels improved dramatically to k = 0.47, which was in line with human-to-human
agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic
work relatively well, but still struggle to "see" student mathematical
illustrations.

---

### 91. On the Role of Difficult Prompts in Self-Play Preference Optimization

**Authors:** Yao Xiao, Jung-jae Kim, Roy Ka-wei Lee, Lidong Bing

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05534v1](http://arxiv.org/pdf/2510.05534v1)

**Abstract:**

Self-play preference optimization has emerged as a prominent paradigm for
aligning large language models (LLMs). It typically involves a language model
to generate on-policy responses for prompts and a reward model (RM) to guide
the selection of chosen and rejected responses, which can be further trained
with direct preference optimization (DPO). However, the role of prompts remains
underexplored, despite being a core component in this pipeline. In this work,
we investigate how prompts of varying difficulty influence self-play preference
optimization. We first use the mean reward of $N$ sampled responses of a prompt
as a proxy for its difficulty. We find that difficult prompts exhibit
substantially inferior self-play optimization performance in comparison to easy
prompts for language models. Moreover, incorporating difficult prompts into
training fails to enhance overall performance and, in fact, leads to slight
degradation compared to training on easy prompts alone. We also observe that
the performance gap between difficult and easy prompts closes as the model
capacity increases, suggesting that difficulty interacts with the model
capacity. Building on these findings, we explore strategies to mitigate the
negative effect of difficult prompts on final performance. We demonstrate that
selectively removing an appropriate portion of challenging prompts enhances
overall self-play performance, while also reporting failed attempts and lessons
learned.

---

### 92. H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model   Inference

**Authors:** Harshil Vejendla

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05529v1](http://arxiv.org/pdf/2510.05529v1)

**Abstract:**

Autoregressive decoding in large language models (LLMs) requires caching a
growing list of past key-value (KV) pairs, making long-context inference a
memory-bound problem. While recent methods have explored quantizing the cache,
evicting tokens, or using binary sketches for keys (e.g., Loki), these
approaches often provide an incomplete solution by leaving one component (like
values) uncompressed or by discarding context information. This paper
introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression
scheme that radically reduces memory usage without sacrificing context. H1B-KV
represents each key vector using a 1-bit binary sketch, enabling
hardware-friendly bitwise attention, and further compresses value vectors using
4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter
LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x
reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches
full-precision performance not only on perplexity benchmarks but also on
complex downstream tasks like mathematical reasoning (GSM8K), multi-task
understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV
significantly outperforms leading quantization (KIVI), token eviction
(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,
establishing it as a robust solution for deploying LLMs in memory-constrained
environments.

---

### 93. ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix   Factorization

**Authors:** Lawrence Liu, Alexander Liu, Mengdi Wang, Tuo Zhao, Lin F. Yang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05528v1](http://arxiv.org/pdf/2510.05528v1)

**Abstract:**

Large language models (LLMs) present significant deployment challenges due to
their immense computational and memory requirements. While semi-structured
pruning, particularly 2:4 sparsity, offers a path to practical hardware
acceleration, existing methods often incur substantial performance degradation.
To bridge this gap, we introduce ARMOR: (Adaptive Representation with
Matrix-factORization), a novel one-shot post-training pruning algorithm.
Instead of directly pruning weights, ARMOR factorizes each weight matrix into a
2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These
wrappers act as efficient pre and post-transformation error correctors,
offering greater flexibility to preserve model quality compared to conventional
2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen
through a block coordinate descent algorithm that minimizes a layer-wise proxy
loss. We theoretically prove this optimization is guaranteed to converge to a
solution with a proxy loss less than or equal to state-of-the-art pruning
algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and
Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and
significantly outperforms state-of-the-art 2:4 pruning methods across a wide
range of downstream tasks and perplexity evaluations. ARMOR achieves this
superior performance while retaining the inference speedups and substantial
memory usage reductions of 2:4 pruning, establishing a more effective trade-off
between model compression and task accuracy

---

### 94. Provably Mitigating Corruption, Overoptimization, and Verbosity   Simultaneously in Offline and Online RLHF/DPO Alignment

**Authors:** Ziyi Chen, Junyi Li, Peiran Yu, Heng Huang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05526v1](http://arxiv.org/pdf/2510.05526v1)

**Abstract:**

Reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) are important techniques to align large language models
(LLM) with human preference. However, the quality of RLHF and DPO training is
seriously compromised by \textit{\textbf{C}orrupted} preference, reward
\textit{\textbf{O}veroptimization}, and bias towards
\textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only
one of these important issues, and the few other works require much computation
to estimate multiple reward models and lack theoretical guarantee of
generalization ability. In this work, we propose RLHF-\textbf{COV} and
DPO-\textbf{COV} algorithms that can simultaneously mitigate these three
issues, in both offline and online settings. This ability is theoretically
demonstrated by obtaining length-regularized generalization error rates for our
DPO-COV algorithms trained on corrupted data, which match the best-known rates
for simpler cases with clean data and without length regularization. Moreover,
our DPO-COV algorithm is simple to implement without reward estimation, and is
proved to be equivalent to our RLHF-COV algorithm, which directly implies the
equivalence between the vanilla RLHF and DPO algorithms. Experiments
demonstrate the effectiveness of our DPO-COV algorithms under both offline and
online settings.

---

### 95. KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for   Safety-Critical Aviation Maintenance

**Authors:** Kuangshi Ai, Jonathan A. Karr Jr, Meng Jiang, Nitesh V. Chawla, Chaoli Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05524v1](http://arxiv.org/pdf/2510.05524v1)

**Abstract:**

We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge
extraction and reasoning framework with large language models (LLMs) in
safety-critical contexts. Using the Operations and Maintenance Intelligence
(OMIn) dataset, we construct a QA benchmark spanning global sensemaking and
actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and
integrates it into a retrieval-augmented generation (RAG) pipeline, enabling
more coherent, dataset-wide reasoning than traditional text-chunk RAG. We
evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ
stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO
markedly improves global sensemaking by revealing patterns and system-level
insights, while text-chunk RAG remains effective for fine-grained procedural
tasks requiring localized retrieval. These findings underscore the promise of
KG-augmented LLMs for secure, domain-specific QA and their potential in
high-stakes reasoning.

---

### 96. CAM: A Constructivist View of Agentic Memory for LLM-Based Reading   Comprehension

**Authors:** Rui Li, Zeyu Zhang, Xiaohe Bo, Zihang Tian, Xu Chen, Quanyu Dai, Zhenhua Dong, Ruiming Tang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05520v1](http://arxiv.org/pdf/2510.05520v1)

**Abstract:**

Current Large Language Models (LLMs) are confronted with overwhelming
information volume when comprehending long-form documents. This challenge
raises the imperative of a cohesive memory module, which can elevate vanilla
LLMs into autonomous reading agents. Despite the emergence of some heuristic
approaches, a systematic design principle remains absent. To fill this void, we
draw inspiration from Jean Piaget's Constructivist Theory, illuminating three
traits of the agentic memory -- structured schemata, flexible assimilation, and
dynamic accommodation. This blueprint forges a clear path toward a more robust
and efficient memory system for LLM-based reading comprehension. To this end,
we develop CAM, a prototype implementation of Constructivist Agentic Memory
that simultaneously embodies the structurality, flexibility, and dynamicity. At
its core, CAM is endowed with an incremental overlapping clustering algorithm
for structured memory development, supporting both coherent hierarchical
summarization and online batch integration. During inference, CAM adaptively
explores the memory structure to activate query-relevant information for
contextual response, akin to the human associative process. Compared to
existing approaches, our design demonstrates dual advantages in both
performance and efficiency across diverse long-text reading comprehension
tasks, including question answering, query-based summarization, and claim
verification.

---

### 97. Prototype-Based Dynamic Steering for Large Language Models

**Authors:** Ceyhun Efe Kayan, Li Zhang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05498v1](http://arxiv.org/pdf/2510.05498v1)

**Abstract:**

Despite impressive breadth, LLMs still rely on explicit reasoning
instructions or static, one-fits-all steering methods, leaving a gap for
adaptive, instruction-free reasoning amplification. We present Prototype-Based
Dynamic Steering (PDS), a test-time method that amplifies large language model
(LLM) reasoning without adding or altering instructions. We introduce
"reasoning prototypes" by clustering activation differences between
Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden
state is projected onto these prototypes to form an instance-specific steering
vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently
improves accuracy without fine-tuning or prompt engineering. Notably, the gains
persist even when CoT is explicitly suppressed to improve cost-efficiency,
indicating that the intervention strengthens latent reasoning processes rather
than inducing a superficial behavioral shift. These results position dynamic,
prototype-guided steering as a lightweight alternative to training-time
approaches for enhancing LLM reasoning.

---

### 98. Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data   Movement Forecasting

**Authors:** Zhongkai Yu, Yue Guan, Zihao Yu, Chenyang Zhou, Shuyi Pei, Yangwook Kang, Yufei Ding, Po-An Tsai

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05497v1](http://arxiv.org/pdf/2510.05497v1)

**Abstract:**

Large Language Models (LLMs) with Mixture of Experts (MoE) architectures
achieve remarkable performance improvements, but their random expert selection
mechanism introduces significant data movement overhead that becomes the
dominant bottleneck in multi-unit serving systems. To forecast the patterns
underlying this data movement, we conduct comprehensive data-movement-centric
profiling across three state-of-the-art large-scale MoE models (200B- 671B)
using over 24,000 requests spanning diverse workloads. With the resulting
150GB+ trace files, we perform systematic analysis from both temporal and
spatial perspectives and distill six key insights to guide the design of
diverse future serving systems. Taking wafer-scale GPUs as a case study, we
demonstrate that minor architectural modifications leveraging our insights
achieve substantial performance gains, delivering 6.3X and 4.0X average
speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first
comprehensive data-centric analysis of MoE models at scale. Our profiling
traces and analysis results are publicly available at
{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will
also release our simulation framework shortly to facilitate future research in
this area.

---

### 99. NorMuon: Making Muon more efficient and scalable

**Authors:** Zichong Li, Liming Liu, Chen Liang, Weizhu Chen, Tuo Zhao

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05491v1](http://arxiv.org/pdf/2510.05491v1)

**Abstract:**

The choice of optimizer significantly impacts the training efficiency and
computational costs of large language models (LLMs). Recently, the Muon
optimizer has demonstrated promising results by orthogonalizing parameter
updates, improving optimization geometry through better conditioning. Despite
Muon's emergence as a candidate successor to Adam, the potential for jointly
leveraging their strengths has not been systematically explored. In this work,
we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an
optimizer that synergistically combines orthogonalization with neuron-level
adaptive learning rates. Our analysis reveals that while Muon effectively
reduces condition numbers, the resulting updates exhibit highly non-uniform
neuron norms, causing certain neurons to dominate the optimization process.
NorMuon addresses this imbalance by maintaining second-order momentum
statistics for each neuron and applying row-wise normalization after
orthogonalization, ensuring balanced parameter utilization while preserving
Muon's conditioning benefits. To enable practical deployment at scale, we
develop an efficient distributed implementation under the FSDP2 framework that
strategically distributes orthogonalization computations across devices.
Experiments across multiple model scales demonstrate that NorMuon consistently
outperforms both Adam and Muon, achieving 21.74% better training efficiency
than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while
maintaining a comparable memory footprint to Muon. Our findings suggest that
orthogonalization and adaptive learning rates are complementary rather than
competing approaches, opening new avenues for optimizer design in large-scale
deep learning.

---

### 100. LANTERN: Scalable Distillation of Large Language Models for Job-Person   Fit and Explanation

**Authors:** Zhoutong Fu, Yihan Cao, Yi-Lin Chen, Aman Lunia, Liming Dong, Neha Saraf, Ruijie Jiang, Yun Dai, Qingquan Song, Tan Wang, Guoyao Li, Derek Koh, Haichao Wei, Zhipeng Wang, Aman Gupta, Chengming Jiang, Jianqiang Shen, Liangjie Hong, Wenjing Zhang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05490v1](http://arxiv.org/pdf/2510.05490v1)

**Abstract:**

Large language models (LLMs) have achieved strong performance across a wide
range of natural language processing tasks. However, deploying LLMs at scale
for domain specific applications, such as job-person fit and explanation in job
seeking platforms, introduces distinct challenges. At LinkedIn, the job person
fit task requires analyzing a candidate's public profile against job
requirements to produce both a fit assessment and a detailed explanation.
Directly applying open source or finetuned LLMs to this task often fails to
yield high quality, actionable feedback due to the complexity of the domain and
the need for structured outputs. Moreover, the large size of these models leads
to high inference latency and limits scalability, making them unsuitable for
online use. To address these challenges, we introduce LANTERN, a novel LLM
knowledge distillation framework tailored specifically for job person fit
tasks. LANTERN involves modeling over multiple objectives, an encoder model for
classification purpose, and a decoder model for explanation purpose. To better
distill the knowledge from a strong black box teacher model to multiple
downstream models, LANTERN incorporates multi level knowledge distillation that
integrates both data and logit level insights. In addition to introducing the
knowledge distillation framework, we share our insights on post training
techniques and prompt engineering, both of which are crucial for successfully
adapting LLMs to domain specific downstream tasks. Extensive experimental
results demonstrate that LANTERN significantly improves task specific metrics
for both job person fit and explanation. Online evaluations further confirm its
effectiveness, showing measurable gains in job seeker engagement, including a
0.24\% increase in apply rate and a 0.28\% increase in qualified applications.

---

### 101. Reference Grounded Skill Discovery

**Authors:** Seungeun Rho, Aaron Trinh, Danfei Xu, Sehoon Ha

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06203v1](http://arxiv.org/pdf/2510.06203v1)

**Abstract:**

Scaling unsupervised skill discovery algorithms to high-DoF agents remains
challenging. As dimensionality increases, the exploration space grows
exponentially, while the manifold of meaningful skills remains limited.
Therefore, semantic meaningfulness becomes essential to effectively guide
exploration in high-dimensional spaces. In this work, we present
Reference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill
discovery in a semantically meaningful latent space using reference data. RGSD
first performs contrastive pretraining to embed motions on a unit hypersphere,
clustering each reference trajectory into a distinct direction. This grounding
enables skill discovery to simultaneously involve both imitation of reference
behaviors and the discovery of semantically related diverse behaviors. On a
simulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns
structured skills including walking, running, punching, and side stepping, and
also discovers related novel behaviors. In downstream control tasks, RGSD
outperforms imitation-based skill acquisition baselines. Our results suggest
that lightweight reference-guided grounding offers a practical path to
discovering semantically rich and structured skills in high-DoF systems.

---

### 102. Pushing Test-Time Scaling Limits of Deep Search with Asymmetric   Verification

**Authors:** Weihao Zeng, Keqing He, Chuqiao Kuang, Xiaoguang Li, Junxian He

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06135v1](http://arxiv.org/pdf/2510.06135v1)

**Abstract:**

Test-time compute can be scaled both sequentially and in parallel. Sequential
scaling involves lengthening the generation process, while parallel scaling
involves verifying and selecting among multiple candidate outputs. Combining
these two strategies has led to the most powerful AI systems, such as Grok 4
Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),
verifying responses can be substantially easier than generating them. This
property, referred to as \emph{asymmetric verification}, highlights the strong
potential of test-time scaling (TTS). In this work, we study both sequential
and parallel TTS of deep search agents, motivated by the intuition that
verification in this setting is often much easier than generation. In
experiments, we first show that sequential scaling methods, such as budget
forcing, can be effective initially but soon degrade performance. Leveraging
asymmetric verification, however, we are able to achieve substantial
improvements by allocating only a modest amount of compute to the verifier. We
conduct experiments with flagship open-source models and extend them to their
``Heavy'' variants through TTS. These deep research agents achieve gains of up
to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an
open-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} on
BrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the best
proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy
further achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing the
best proprietary results.

---

### 103. Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI   Models for Scatterplot-Related Tasks

**Authors:** JoÃ£o Palmeiro, Diogo Duarte, Rita Costa, Pedro Bizarro

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06071v1](http://arxiv.org/pdf/2510.06071v1)

**Abstract:**

AI models are increasingly used for data analysis and visualization, yet
benchmarks rarely address scatterplot-specific tasks, limiting insight into
performance. To address this gap for one of the most common chart types, we
introduce a synthetic, annotated dataset of over 18,000 scatterplots from six
data generators and 17 chart designs, and a benchmark based on it. We evaluate
proprietary models from OpenAI and Google using N-shot prompting on five
distinct tasks derived from annotations of cluster bounding boxes, their center
coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash,
especially when prompted with examples, are viable options for counting
clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results
for localization-related tasks are unsatisfactory: Precision and Recall are
near or below 50%, except for Flash in outlier identification (65.01%).
Furthermore, the impact of chart design on performance appears to be a
secondary factor, but it is advisable to avoid scatterplots with wide aspect
ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are
available at https://github.com/feedzai/biy-paper.

---

### 104. Reasoning under Vision: Understanding Visual-Spatial Cognition in   Vision-Language Models for CAPTCHA

**Authors:** Python Song, Luke Tenyi Chang, Yun-Yun Tsai, Penghui Li, Junfeng Yang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06067v1](http://arxiv.org/pdf/2510.06067v1)

**Abstract:**

CAPTCHA, originally designed to distinguish humans from robots, has evolved
into a real-world benchmark for assessing the spatial reasoning capabilities of
vision-language models. In this work, we first show that step-by-step reasoning
is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent
high-difficulty spatial reasoning tasks, and that current commercial
vision-language models still struggle with such reasoning. In particular, we
observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to
effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).
However, our findings indicate that requiring the model to perform step-by-step
reasoning before generating the final coordinates can significantly enhance its
solving accuracy, underscoring the severity of the gap. To systematically study
this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with
reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,
etc.) with step-by-step action solutions and grounding annotations. We further
define five reasoning-oriented metrics that enable a comprehensive evaluation
of models reasoning capabilities. To validate the effectiveness of reasoning,
we also propose a general agentic VLM-based framework that incorporates the
models inherent reasoning abilities. Our method achieves state-of-the-art
performance across five high-difficulty CAPTCHA types, with an average solving
accuracy of 83.9 percent, substantially surpassing existing baselines. These
results reveal the limitations of current models and highlight the importance
of reasoning in advancing visual-spatial challenges in the future.

---

### 105. ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling   Evaluation in Large Reasoning Models

**Authors:** Zhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Zhiyuan Yu, Qipeng Guo, Xuanjing Huang, Xipeng Qiu

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06014v1](http://arxiv.org/pdf/2510.06014v1)

**Abstract:**

Test-time scaling has emerged as a transformative paradigm for enhancing the
performance of large reasoning models, enabling dynamic allocation of
computational resources during inference. However, as the landscape of
reasoning models rapidly expands, a critical question remains: how can we
systematically compare and evaluate the test-time scaling capabilities across
different models? In this paper, we introduce ARISE (Adaptive Resolution-aware
Scaling Evaluation), a novel metric specifically designed to assess the
test-time scaling effectiveness of large reasoning models. Unlike existing
evaluation approaches, ARISE incorporates two key innovations: (1) sample-level
awareness that effectively penalizes negative scaling behaviors where increased
computation leads to performance degradation, and (2) a dynamic sampling
mechanism that mitigates the impact of accuracy fluctuations and token count
instability on the final assessment. We conduct comprehensive experiments
evaluating state-of-the-art reasoning models across diverse domains including
mathematical reasoning, code generation, and agentic tasks. Our results
demonstrate that ARISE provides a reliable and fine-grained measurement of
test-time scaling capabilities, revealing significant variations in scaling
efficiency across models. Notably, our evaluation identifies Claude Opus as
exhibiting superior scaling characteristics compared to other contemporary
reasoning models.

---

### 106. Hybrid Quantum-Classical Policy Gradient for Adaptive Control of   Cyber-Physical Systems: A Comparative Study of VQC vs. MLP

**Authors:** Aueaphum Aueawatthanaphisut, Nyi Wunna Tun

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06010v1](http://arxiv.org/pdf/2510.06010v1)

**Abstract:**

The comparative evaluation between classical and quantum reinforcement
learning (QRL) paradigms was conducted to investigate their convergence
behavior, robustness under observational noise, and computational efficiency in
a benchmark control environment. The study employed a multilayer perceptron
(MLP) agent as a classical baseline and a parameterized variational quantum
circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1
environment over 500 episodes. Empirical results demonstrated that the
classical MLP achieved near-optimal policy convergence with a mean return of
498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast,
the VQC exhibited limited learning capability, with an average return of 14.6
+/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise
robustness analysis further revealed that the MLP policy deteriorated
gracefully under Gaussian perturbations, while the VQC displayed higher
sensitivity at equivalent noise levels. Despite the lower asymptotic
performance, the VQC exhibited significantly lower parameter count and
marginally increased training time, highlighting its potential scalability for
low-resource quantum processors. The results suggest that while classical
neural policies remain dominant in current control benchmarks, quantum-enhanced
architectures could offer promising efficiency advantages once hardware noise
and expressivity limitations are mitigated.

---

### 107. Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph   RAG

**Authors:** Hudson de Martim

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06002v1](http://arxiv.org/pdf/2510.06002v1)

**Abstract:**

The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.

---

### 108. Information-Theoretic Policy Pre-Training with Empowerment

**Authors:** Moritz Schneider, Robert Krug, Narunas Vaskevicius, Luigi Palmieri, Michael Volpp, Joschka Boedecker

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05996v1](http://arxiv.org/pdf/2510.05996v1)

**Abstract:**

Empowerment, an information-theoretic measure of an agent's potential
influence on its environment, has emerged as a powerful intrinsic motivation
and exploration framework for reinforcement learning (RL). Besides for
unsupervised RL and skill learning algorithms, the specific use of empowerment
as a pre-training signal has received limited attention in the literature. We
show that empowerment can be used as a pre-training signal for data-efficient
downstream task adaptation. For this we extend the traditional notion of
empowerment by introducing discounted empowerment, which balances the agent's
control over the environment across short- and long-term horizons. Leveraging
this formulation, we propose a novel pre-training paradigm that initializes
policies to maximize discounted empowerment, enabling agents to acquire a
robust understanding of environmental dynamics. We analyze empowerment-based
pre-training for various existing RL algorithms and empirically demonstrate its
potential as a general-purpose initialization strategy: empowerment-maximizing
policies with long horizons are data-efficient and effective, leading to
improved adaptability in downstream tasks. Our findings pave the way for future
research to scale this framework to high-dimensional and complex tasks, further
advancing the field of RL.

---

### 109. The Safety Challenge of World Models for Embodied AI Agents: A Review

**Authors:** Lorenzo Baraldi, Zifan Zeng, Chongzhe Zhang, Aradhana Nayak, Hongbo Zhu, Feng Liu, Qunli Zhang, Peng Wang, Shiming Liu, Zheng Hu, Angelo Cangelosi, Lorenzo Baraldi

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05865v1](http://arxiv.org/pdf/2510.05865v1)

**Abstract:**

The rapid progress in embodied artificial intelligence has highlighted the
necessity for more advanced and integrated models that can perceive, interpret,
and predict environmental dynamics. In this context, World Models (WMs) have
been introduced to provide embodied agents with the abilities to anticipate
future environmental states and fill in knowledge gaps, thereby enhancing
agents' ability to plan and execute actions. However, when dealing with
embodied agents it is fundamental to ensure that predictions are safe for both
the agent and the environment. In this article, we conduct a comprehensive
literature review of World Models in the domains of autonomous driving and
robotics, with a specific focus on the safety implications of scene and control
generation tasks. Our review is complemented by an empirical analysis, wherein
we collect and examine predictions from state-of-the-art models, identify and
categorize common faults (herein referred to as pathologies), and provide a
quantitative evaluation of the results.

---

### 110. RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases

**Authors:** Lang Qin, Zijian Gan, Xu Cao, Pengcheng Jiang, Yankai Jiang, Jiawei Han, Kaishun Wu, Jintai Chen

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05764v1](http://arxiv.org/pdf/2510.05764v1)

**Abstract:**

Computational drug repurposing for rare diseases is especially challenging
when no prior associations exist between drugs and target diseases. Therefore,
knowledge graph completion and message-passing GNNs have little reliable signal
to learn and propagate, resulting in poor performance. We present RareAgent, a
self-evolving multi-agent system that reframes this task from passive pattern
recognition to active evidence-seeking reasoning. RareAgent organizes
task-specific adversarial debates in which agents dynamically construct
evidence graphs from diverse perspectives to support, refute, or entail
hypotheses. The reasoning strategies are analyzed post hoc in a
self-evolutionary loop, producing textual feedback that refines agent policies,
while successful reasoning paths are distilled into transferable heuristics to
accelerate future investigations. Comprehensive evaluations reveal that
RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and
provides a transparent reasoning chain consistent with clinical evidence.

---

### 111. Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor   Policies

**Authors:** Yuhang Zhang, Jiaping Xiao, Chao Yan, Mir Feroskhan

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05692v1](http://arxiv.org/pdf/2510.05692v1)

**Abstract:**

A prevailing approach for learning visuomotor policies is to employ
reinforcement learning to map high-dimensional visual observations directly to
action commands. However, the combination of high-dimensional visual inputs and
agile maneuver outputs leads to long-standing challenges, including low sample
efficiency and significant sim-to-real gaps. To address these issues, we
propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a
novel framework designed to improve the sample efficiency and asymptotic
performance of visuomotor policy learning. OMC-RL explicitly decouples the
learning process into two stages: an upstream representation learning stage and
a downstream policy learning stage. In the upstream stage, a masked Transformer
module is trained with temporal modeling and contrastive learning to extract
temporally-aware and task-relevant representations from sequential visual
inputs. After training, the learned encoder is frozen and used to extract
visual representations from consecutive frames, while the Transformer module is
discarded. In the downstream stage, an oracle teacher policy with privileged
access to global state information supervises the agent during early training
to provide informative guidance and accelerate early policy learning. This
guidance is gradually reduced to allow independent exploration as training
progresses. Extensive experiments in simulated and real-world environments
demonstrate that OMC-RL achieves superior sample efficiency and asymptotic
policy performance, while also improving generalization across diverse and
perceptually complex scenarios.

---

### 112. Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch   Optical Networks

**Authors:** Yao Zhang, Yuchen Song, Shengnan Li, Yan Shi, Shikui Shen, Xiongyan Tang, Min Zhang, Danshi Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05625v1](http://arxiv.org/pdf/2510.05625v1)

**Abstract:**

The rapid development of Generative Artificial Intelligence (GenAI) has
catalyzed a transformative technological revolution across all walks of life.
As the backbone of wideband communication, optical networks are expecting
high-level autonomous operation and zero-touch management to accommodate their
expanding network scales and escalating transmission bandwidth. The integration
of GenAI is deemed as the pivotal solution for realizing zero-touch optical
networks. However, the lifecycle management of optical networks involves a
multitude of tasks and necessitates seamless collaboration across multiple
layers, which poses significant challenges to the existing single-agent GenAI
systems. In this paper, we propose a GenAI-driven hierarchical multi-agent
framework designed to streamline multi-task autonomous execution for zero-touch
optical networks. We present the architecture, implementation, and applications
of this framework. A field-deployed mesh network is utilized to demonstrate
three typical scenarios throughout the lifecycle of optical network: quality of
transmission estimation in the planning stage, dynamic channel adding/dropping
in the operation stage, and system capacity increase in the upgrade stage. The
case studies, illustrate the capabilities of multi-agent framework in
multi-task allocation, coordination, execution, evaluation, and summarization.
This work provides a promising approach for the future development of
intelligent, efficient, and collaborative network management solutions, paving
the way for more specialized and adaptive zero-touch optical networks.

---

### 113. Decoupling Correctness from Policy: A Deterministic Causal Structure for   Multi-Agent Systems

**Authors:** Zhiyuan Ren, Tao Zhang, Wenchi Chen

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05621v1](http://arxiv.org/pdf/2510.05621v1)

**Abstract:**

In distributed multi-agent systems, correctness is often entangled with
operational policies such as scheduling, batching, or routing, which makes
systems brittle since performance-driven policy evolution may break integrity
guarantees. This paper introduces the Deterministic Causal Structure (DCS), a
formal foundation that decouples correctness from policy. We develop a minimal
axiomatic theory and prove four results: existence and uniqueness,
policy-agnostic invariance, observational equivalence, and axiom minimality.
These results show that DCS resolves causal ambiguities that value-centric
convergence models such as CRDTs cannot address, and that removing any axiom
collapses determinism into ambiguity. DCS thus emerges as a boundary principle
of asynchronous computation, analogous to CAP and FLP: correctness is preserved
only within the expressive power of a join-semilattice. All guarantees are
established by axioms and proofs, with only minimal illustrative constructions
included to aid intuition. This work establishes correctness as a fixed,
policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which
distributed intelligent systems can be built modularly, safely, and evolvably.

---

### 114. MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption

**Authors:** Chen Li, Zhantao Yang, Han Zhang, Fangyi Chen, Chenchen Zhu, Anudeepsekhar Bolimera, Marios Savvides

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05580v1](http://arxiv.org/pdf/2510.05580v1)

**Abstract:**

Vision-Language-Action (VLA) models show promise in embodied reasoning, yet
remain far from true generalists-they often require task-specific fine-tuning,
and generalize poorly to unseen tasks. We propose MetaVLA, a unified,
backbone-agnostic post-training framework for efficient and scalable alignment.
MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse
target tasks into a single fine-tuning stage while leveraging structurally
diverse auxiliary tasks to improve in-domain generalization. Unlike naive
multi-task SFT, MetaVLA integrates a lightweight meta-learning
mechanism-derived from Attentive Neural Processes-to enable rapid adaptation
from diverse contexts with minimal architectural change or inference overhead.
On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA
by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K,
and cuts GPU time by ~76%. These results show that scalable, low-resource
post-training is achievable-paving the way toward general-purpose embodied
agents. Code will be available.

---

### 115. Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for   Academic Presentations

**Authors:** Chengzhi Liu, Yuzhe Yang, Kaiwen Zhou, Zhen Zhang, Yue Fan, Yannan Xie, Peng Qi, Xin Eric Wang

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05571v1](http://arxiv.org/pdf/2510.05571v1)

**Abstract:**

The promotion of academic papers has become an important means of enhancing
research visibility. However, existing automated methods struggle limited
storytelling, insufficient aesthetic quality, and constrained self-adjustment,
making it difficult to achieve efficient and engaging dissemination. At the
heart of those challenges is a simple principle: \emph{there is no way to
improve it when you cannot evaluate it right}. To address this, we introduce
\textbf{EvoPresent}, a self-improvement agent framework that unifies coherent
narratives, aesthetic-aware designs, and realistic presentation delivery via
virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task
reinforcement learning (RL) aesthetic model that provides reliable aesthetic
scoring, defect adjustment, and comparative feedback, enabling iterative
self-improvement even under limited aesthetic training data. To systematically
evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a
comprehensive benchmark comprising: \textit{Presentation Generation Quality},
built on 650 top-tier AI conference papers with multimodal resources (slides,
videos and scripts) to assess both content and design; and \textit{Aesthetic
Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels,
supporting joint training and evaluation on scoring, defect adjustment, and
comparison. Our findings highlight that (i) High-quality feedback is essential
for agent self-improvement, while initial capability alone does not guarantee
effective self-correction. (ii) Automated generation pipelines exhibit a
trade-off between visual design and content construction. (iii) Multi-task RL
training shows stronger generalization in aesthetic awareness tasks.

---

### 116. Toward Systems Foundations for Agentic Exploration

**Authors:** Jiakai Xu, Tianle Zhou, Eugene Wu, Kostis Kaffes

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05556v1](http://arxiv.org/pdf/2510.05556v1)

**Abstract:**

Agentic exploration, letting LLM-powered agents branch, backtrack, and search
across many execution paths, demands systems support well beyond today's
pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that
generic tools such as CRIU or container commits are not fast enough even in
isolated testbeds, and they crumble entirely in real deployments where agents
share files, sockets, and cloud APIs with other agents and human users. In this
talk, we pinpoint three open fundamental challenges: fork semantics, which
concerns how branches reveal or hide tentative updates; external side-effects,
where fork awareness must be added to services or their calls intercepted; and
native forking, which requires cloning databases and runtimes in microseconds
without bulk copying.

---

### 117. TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular   Reasoning

**Authors:** Jiaru Zou, Soumya Roy, Vinay Kumar Verma, Ziyi Wang, David Wipf, Pan Lu, Sumit Negi, James Zou, Jingrui He

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06217v1](http://arxiv.org/pdf/2510.06217v1)

**Abstract:**

Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.

---

### 118. Mixing Mechanisms: How Language Models Retrieve Bound Entities   In-Context

**Authors:** Yoav Gur-Arieh, Mor Geva, Atticus Geiger

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06182v1](http://arxiv.org/pdf/2510.06182v1)

**Abstract:**

A key component of in-context reasoning is the ability of language models
(LMs) to bind entities for later retrieval. For example, an LM might represent
"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"
when asked "Who loves pie?" Prior research on short lists of bound entities
found strong evidence that LMs implement such retrieval via a positional
mechanism, where "Ann" is retrieved based on its position in context. In this
work, we find that this mechanism generalizes poorly to more complex settings;
as the number of bound entities in context increases, the positional mechanism
becomes noisy and unreliable in middle positions. To compensate for this, we
find that LMs supplement the positional mechanism with a lexical mechanism
(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism
(retrieving "Ann" through a direct pointer). Through extensive experiments on
nine models and ten binding tasks, we uncover a consistent pattern in how LMs
mix these mechanisms to drive model behavior. We leverage these insights to
develop a causal model combining all three mechanisms that estimates next token
distributions with 95% agreement. Finally, we show that our model generalizes
to substantially longer inputs of open-ended text interleaved with entity
groups, further demonstrating the robustness of our findings in more natural
settings. Overall, our study establishes a more complete picture of how LMs
bind and retrieve entities in-context.

---

### 119. Emergent AI Surveillance: Overlearned Person Re-Identification and Its   Mitigation in Law Enforcement Context

**Authors:** An Thi Nguyen, Radina Stoykova, Eric Arazo

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.06026v1](http://arxiv.org/pdf/2510.06026v1)

**Abstract:**

Generic instance search models can dramatically reduce the manual effort
required to analyze vast surveillance footage during criminal investigations by
retrieving specific objects of interest to law enforcement. However, our
research reveals an unintended emergent capability: through overlearning, these
models can single out specific individuals even when trained on datasets
without human subjects. This capability raises concerns regarding
identification and profiling of individuals based on their personal data, while
there is currently no clear standard on how de-identification can be achieved.
We evaluate two technical safeguards to curtail a model's person
re-identification capacity: index exclusion and confusion loss. Our experiments
demonstrate that combining these approaches can reduce person re-identification
accuracy to below 2% while maintaining 82% of retrieval performance for
non-person objects. However, we identify critical vulnerabilities in these
mitigations, including potential circumvention using partial person images.
These findings highlight urgent regulatory questions at the intersection of AI
governance and data protection: How should we classify and regulate systems
with emergent identification capabilities? And what technical standards should
be required to prevent identification capabilities from developing in seemingly
benign applications?

---

### 120. How public datasets constrain the development of diversity-aware news   recommender systems, and what law could do about it

**Authors:** Max van Drunen, Sanne Vrijenhoek

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05952v1](http://arxiv.org/pdf/2510.05952v1)

**Abstract:**

News recommender systems increasingly determine what news individuals see
online. Over the past decade, researchers have extensively critiqued
recommender systems that prioritise news based on user engagement. To offer an
alternative, researchers have analysed how recommender systems could support
the media's ability to fulfil its role in democratic society by recommending
news based on editorial values, particularly diversity. However, there
continues to be a large gap between normative theory on how news recommender
systems should incorporate diversity, and technical literature that designs
such systems. We argue that to realise diversity-aware recommender systems in
practice, it is crucial to pay attention to the datasets that are needed to
train modern news recommenders. We aim to make two main contributions. First,
we identify the information a dataset must include to enable the development of
the diversity-aware news recommender systems proposed in normative literature.
Based on this analysis, we assess the limitations of currently available public
datasets, and show what potential they do have to expand research into
diversity-aware recommender systems. Second, we analyse why and how European
law and policy can be used to provide researchers with structural access to the
data they need to develop diversity-aware news recommender systems.

---

### 121. Limitations of Current Evaluation Practices for Conversational   Recommender Systems and the Potential of User Simulation

**Authors:** Nolwenn Bernard, Krisztian Balog

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05624v1](http://arxiv.org/pdf/2510.05624v1)

**Abstract:**

Research and development on conversational recommender systems (CRSs)
critically depends on sound and reliable evaluation methodologies. However, the
interactive nature of these systems poses significant challenges for automatic
evaluation. This paper critically examines current evaluation practices and
identifies two key limitations: the over-reliance on static test collections
and the inadequacy of existing evaluation metrics. To substantiate this
critique, we analyze real user interactions with nine existing CRSs and
demonstrate a striking disconnect between self-reported user satisfaction and
performance scores reported in prior literature. To address these limitations,
this work explores the potential of user simulation to generate dynamic
interaction data, offering a departure from static datasets. Furthermore, we
propose novel evaluation metrics, based on a general reward/cost framework,
designed to better align with real user satisfaction. Our analysis of different
simulation approaches provides valuable insights into their effectiveness and
reveals promising initial results, showing improved correlation with system
rankings compared to human evaluation. While these findings indicate a
significant step forward in CRS evaluation, we also identify areas for future
research and refinement in both simulation techniques and evaluation metrics.

---

### 122. Automated Research Article Classification and Recommendation Using NLP   and ML

**Authors:** Shadikur Rahman, Hasibul Karim Shanto, Umme Ayman Koana, Syed Muhammad Danish

**Published:** 2025-10-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05495v1](http://arxiv.org/pdf/2510.05495v1)

**Abstract:**

In the digital era, the exponential growth of scientific publications has
made it increasingly difficult for researchers to efficiently identify and
access relevant work. This paper presents an automated framework for research
article classification and recommendation that leverages Natural Language
Processing (NLP) techniques and machine learning. Using a large-scale arXiv.org
dataset spanning more than three decades, we evaluate multiple feature
extraction approaches (TF--IDF, Count Vectorizer, Sentence-BERT, USE,
Mirror-BERT) in combination with diverse machine learning classifiers (Logistic
Regression, SVM, Na\"ive Bayes, Random Forest, Gradient Boosted Trees, and
k-Nearest Neighbour). Our experiments show that Logistic Regression with
TF--IDF consistently yields the best classification performance, achieving an
accuracy of 69\%. To complement classification, we incorporate a recommendation
module based on the cosine similarity of vectorized articles, enabling
efficient retrieval of related research papers. The proposed system directly
addresses the challenge of information overload in digital libraries and
demonstrates a scalable, data-driven solution to support literature discovery.

---

### 123. AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative   Multi-Agent Question Answering

**Authors:** Zheyuan Zhang, Kaiwen Shi, Zhengqing Yuan, Zehong Wang, Tianyi Ma, Keerthiram Murugesan, Vincent Galassi, Chuxu Zhang, Yanfang Ye

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05445v1](http://arxiv.org/pdf/2510.05445v1)

**Abstract:**

Large language models (LLMs) and agent-based frameworks have advanced
rapidly, enabling diverse applications. Yet, with the proliferation of models
and agentic strategies, practitioners face substantial uncertainty in selecting
the best configuration for a downstream task. Prior studies show that different
agents and backbones exhibit complementary strengths, and that larger models
are not always superior, underscoring the need for adaptive routing mechanisms.
Existing approaches to agent routing, however, often emphasize cost efficiency
while overlooking the fine-grained contextual and relational structure inherent
in QA tasks. In this paper, we propose tAgentRouter, a framework that
formulates multi-agent QA as a knowledge-graph-guided routing problem
supervised by empirical performance signals. Specifically, we convert QA
instance into a knowledge graph that jointly encodes queries, contextual
entities, and agents, and then train a heterogeneous graph neural network (GNN)
to propagate information across node types and produce task-aware routing
distributions over agents. By leveraging soft supervision and weighted
aggregation of agent outputs, AgentRouter learns principled collaboration
schemes that capture the complementary strengths of diverse agents. Extensive
experiments demonstrate that our framework consistently outperforms
single-agent and ensemble baselines, while generalizing across benchmarks and
LLM backbones. These results highlight the effectiveness and robustness of
graph-supervised multi-agent routing for question answering.

---

### 124. Adversarial Reinforcement Learning for Large Language Model Agent Safety

**Authors:** Zizhao Wang, Dingcheng Li, Vaishakh Keshava, Phillip Wallis, Ananth Balashankar, Peter Stone, Lukas Rutishauser

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05442v1](http://arxiv.org/pdf/2510.05442v1)

**Abstract:**

Large Language Model (LLM) agents can leverage tools such as Google Search to
complete complex tasks. However, this tool usage introduces the risk of
indirect prompt injections, where malicious instructions hidden in tool outputs
can manipulate the agent, posing security risks like data leakage. Current
defense strategies typically rely on fine-tuning LLM agents on datasets of
known attacks. However, the generation of these datasets relies on manually
crafted attack patterns, which limits their diversity and leaves agents
vulnerable to novel prompt injections. To address this limitation, we propose
Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework
that leverages adversarial reinforcement learning (RL) by formulating the
problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker
that learns to autonomously generate diverse prompt injections and an agent
that learns to defend against them while completing its assigned tasks. To
ensure robustness against a wide range of attacks and to prevent cyclic
learning, we employ a population-based learning framework that trains the agent
to defend against all previous attacker checkpoints. Evaluated on BrowserGym
and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower
attack success rate than the original model while also improving their task
success rate. Our analysis further confirms that the adversarial process
generates a diverse and challenging set of attacks, leading to a more robust
agent compared to the base model.

---

### 125. UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by   Formal Verification

**Authors:** Yiannis Charalambous, Claudionor N. Coelho Jr, Luis Lamb, Lucas C. Cordeiro

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05441v1](http://arxiv.org/pdf/2510.05441v1)

**Abstract:**

This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

---

### 126. AInstein: Assessing the Feasibility of AI-Generated Approaches to   Research Problems

**Authors:** Shambhavi Mishra, Gaurav Sahu, Marco Pedersoli, Laurent Charlin, Jose Dolz, Christopher Pal

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05432v1](http://arxiv.org/pdf/2510.05432v1)

**Abstract:**

Large language models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet it remains unclear whether such success reflects
genuine reasoning or sophisticated recall. We introduce AInstein, a framework
for testing whether LLMs can generate valid solutions to AI research problems
using only their pretrained parametric knowledge -- without domain-specific
fine-tuning, retrieval augmentation, or other external aids. Our approach
extracts distilled problem statements from high-quality ICLR 2025 submissions,
then tasks specialized solver agents with proposing and refining technical
solutions through iterative critique loops, mimicking the cycles of proposal,
review, and revision central to scientific inquiry. We evaluate AInstein on
1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),
using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by
targeted manual checks. Performance is assessed with three metrics: Success
Rate (does the solution address the problem?), Rediscovery (does it align with
human-proposed methods?), and Novelty (does it yield valid, original
approaches?). Our results reveal that while LLMs can rediscover feasible
solutions and occasionally propose creative alternatives, their problem-solving
ability remains fragile and highly sensitive to framing. These findings provide
the first large-scale evidence on the extent to which LLMs can act as
autonomous scientific problem-solvers, highlighting both their latent potential
and their current limitations.

---

### 127. A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame   Structural Analysis

**Authors:** Ziheng Geng, Jiachen Liu, Ran Cao, Lu Cheng, Haifeng Wang, Minghui Cheng

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05414v1](http://arxiv.org/pdf/2510.05414v1)

**Abstract:**

Large language models (LLMs) have recently been used to empower autonomous
agents in engineering, significantly improving automation and efficiency in
labor-intensive workflows. However, their potential remains underexplored in
structural engineering, particularly for finite element modeling tasks
requiring geometric modeling, complex reasoning, and domain knowledge. To
bridge this gap, this paper develops a LLM-based multi-agent system to automate
finite element modeling of 2D frames. The system decomposes structural analysis
into subtasks, each managed by a specialized agent powered by the lightweight
Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis
Agent, which extracts geometry, boundary, and material parameters from the user
input. Next, a Geometry Agent incrementally derives node coordinates and
element connectivity by applying expert-defined rules. These structured outputs
are converted into executable OpenSeesPy code by a Translation Agent and
refined by a Model Validation Agent through consistency checks. Then, a Load
Agent applies load conditions into the assembled structural model. Experimental
evaluations on 20 benchmark problems demonstrate that the system achieves
accuracy over 80% in most cases across 10 repeated trials, outperforming
Gemini-2.5 Pro and ChatGPT-4o models.

---

### 128. AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak   Attacks with Test-Time Scaling

**Authors:** Xiaogeng Liu, Chaowei Xiao

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05379v1](http://arxiv.org/pdf/2510.05379v1)

**Abstract:**

Recent advancements in jailbreaking large language models (LLMs), such as
AutoDAN-Turbo, have demonstrated the power of automated strategy discovery.
AutoDAN-Turbo employs a lifelong learning agent to build a rich library of
attack strategies from scratch. While highly effective, its test-time
generation process involves sampling a strategy and generating a single
corresponding attack prompt, which may not fully exploit the potential of the
learned strategy library. In this paper, we propose to further improve the
attack performance of AutoDAN-Turbo through test-time scaling. We introduce two
distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method
generates N candidate attack prompts from a sampled strategy and selects the
most effective one based on a scorer model. The Beam Search method conducts a
more exhaustive search by exploring combinations of strategies from the library
to discover more potent and synergistic attack vectors. According to the
experiments, the proposed methods significantly boost performance, with Beam
Search increasing the attack success rate by up to 15.6 percentage points on
Llama-3.1-70B-Instruct and achieving a nearly 60\% relative improvement against
the highly robust GPT-o4-mini compared to the vanilla method.

---

### 129. Biomedical reasoning in action: Multi-agent System for Auditable   Biomedical Evidence Synthesis

**Authors:** Oskar Wysocki, Magdalena Wysocka, Mauricio Jacobo, Harriet Unsworth, AndrÃ© Freitas

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05335v1](http://arxiv.org/pdf/2510.05335v1)

**Abstract:**

We present M-Reason, a demonstration system for transparent, agent-based
reasoning and evidence integration in the biomedical domain, with a focus on
cancer research. M-Reason leverages recent advances in large language models
(LLMs) and modular agent orchestration to automate evidence retrieval,
appraisal, and synthesis across diverse biomedical data sources. Each agent
specializes in a specific evidence stream, enabling parallel processing and
fine-grained analysis. The system emphasizes explainability, structured
reporting, and user auditability, providing complete traceability from source
evidence to final conclusions. We discuss critical tradeoffs between agent
specialization, system complexity, and resource usage, as well as the
integration of deterministic code for validation. An open, interactive user
interface allows researchers to directly observe, explore and evaluate the
multi-agent workflow. Our evaluation demonstrates substantial gains in
efficiency and output consistency, highlighting M-Reason's potential as both a
practical tool for evidence synthesis and a testbed for robust multi-agent LLM
systems in scientific research, available at https://m-reason.digitalecmt.com.

---

### 130. DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code   Generation with a High-Quality Knowledge Base

**Authors:** Zahin Ibnat, Paul E. Calzada, Rasin Mohammed Ihtemam, Sujan Kumar Saha, Jingbo Zhou, Farimah Farahmandi, Mark Tehranipoor

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05327v1](http://arxiv.org/pdf/2510.05327v1)

**Abstract:**

As large language models (LLMs) continue to be integrated into modern
technology, there has been an increased push towards code generation
applications, which also naturally extends to hardware design automation.
LLM-based solutions for register transfer level (RTL) code generation for
intellectual property (IP) designs have grown, especially with fine-tuned LLMs,
prompt engineering, and agentic approaches becoming popular in literature.
However, a gap has been exposed in these techniques, as they fail to integrate
novel IPs into the model's knowledge base, subsequently resulting in poorly
generated code. Additionally, as general-purpose LLMs continue to improve,
fine-tuned methods on older models will not be able to compete to produce more
accurate and efficient designs. Although some retrieval augmented generation
(RAG) techniques exist to mitigate challenges presented in fine-tuning
approaches, works tend to leverage low-quality codebases, incorporate
computationally expensive fine-tuning in the frameworks, or do not use RAG
directly in the RTL generation step. In this work, we introduce DeepV: a
model-agnostic RAG framework to generate RTL designs by enhancing context
through a large, high-quality dataset without any RTL-specific training. Our
framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%
increase in performance on the VerilogEval benchmark. We host DeepV for use by
the community in a Hugging Face (HF) Space:
https://huggingface.co/spaces/FICS-LLM/DeepV.

---

### 131. BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language   Models via Lens of Dynamic Interactions

**Authors:** Nan Huo, Xiaohan Xu, Jinyang Li, Per Jacobsson, Shipei Lin, Bowen Qin, Binyuan Hui, Xiaolong Li, Ge Qu, Shuzheng Si, Linheng Han, Edward Alexander, Xintong Zhu, Rui Qin, Ruihan Yu, Yiyao Jin, Feige Zhou, Weihao Zhong, Yun Chen, Hongyu Liu, Chenhao Ma, Fatma Ozcan, Yannis Papakonstantinou, Reynold Cheng

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05318v1](http://arxiv.org/pdf/2510.05318v1)

**Abstract:**

Large language models (LLMs) have demonstrated remarkable performance on
single-turn text-to-SQL tasks, but real-world database applications
predominantly require multi-turn interactions to handle ambiguous queries,
execution errors, and evolving user requirements. Existing multi-turn
benchmarks fall short by treating conversation histories as static context or
limiting evaluation to read-only operations, failing to reflect
production-grade database assistant challenges. We introduce BIRD-INTERACT, a
benchmark that restores this realism through: (1) a comprehensive interaction
environment coupling each database with a hierarchical knowledge base, metadata
files, and a function-driven user simulator, enabling models to solicit
clarifications, retrieve knowledge, and recover from errors without human
supervision; (2) two evaluation settings consisting of a pre-defined
conversational protocol (c-Interact) and an open-ended agentic setting
(a-Interact) where models autonomously decide when to query the user simulator
or explore the environment; (3) a challenging task suite covering the full CRUD
spectrum for business-intelligence and operational use cases, guarded by
executable test cases. Each task features ambiguous and follow-up sub-tasks
requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600
tasks, up to 11,796 interactions) for comprehensive performance assessment, and
BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed
behavioral analysis and rapid method development. Our empirical results
highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in
c-Interact and 17.00% in a-Interact. Analysis via memory grafting and
Interaction Test-time Scaling validates the importance of effective interaction
for complex, dynamic text-to-SQL tasks.

---

### 132. Paper2Video: Automatic Video Generation from Scientific Papers

**Authors:** Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05096v1](http://arxiv.org/pdf/2510.05096v1)

**Abstract:**

Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

---

### 133. Multi-Agent Distributed Optimization With Feasible Set Privacy

**Authors:** Shreya Meel, Sennur Ulukus

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05068v1](http://arxiv.org/pdf/2510.05068v1)

**Abstract:**

We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.

---

### 134. Staircase Streaming for Low-Latency Multi-Agent Inference

**Authors:** Junlin Wang, Jue Wang,  Zhen,  Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05059v1](http://arxiv.org/pdf/2510.05059v1)

**Abstract:**

Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

---

### 135. Look-ahead Reasoning with a Learned Model in Imperfect Information Games

**Authors:** OndÅej KubÃ­Äek, Viliam LisÃ½

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05048v1](http://arxiv.org/pdf/2510.05048v1)

**Abstract:**

Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

---

### 136. Large Language Models Achieve Gold Medal Performance at the   International Olympiad on Astronomy & Astrophysics (IOAA)

**Authors:** Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05016v2](http://arxiv.org/pdf/2510.05016v2)

**Abstract:**

While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

---

### 137. LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and   Rationale Inference in Imperfect Information Collaboration Game

**Authors:** Fangzhou Liang, Tianshi Zheng, Chunkit Chan, Yauwai Yim, Yangqiu Song

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04980v1](http://arxiv.org/pdf/2510.04980v1)

**Abstract:**

Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

---

### 138. Safe and Compliant Cross-Market Trade Execution via Constrained RL and   Zero-Knowledge Audits

**Authors:** Ailiya Borjigin, Cong He

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04952v2](http://arxiv.org/pdf/2510.04952v2)

**Abstract:**

We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

---

### 139. MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement   Learning

**Authors:** Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04935v1](http://arxiv.org/pdf/2510.04935v1)

**Abstract:**

Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

---

### 140. Focused Skill Discovery: Learning to Control Specific State Variables   while Minimizing Side Effects

**Authors:** Jonathan ColaÃ§o Carr, Qinyi Sun, Cameron Allen

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04901v1](http://arxiv.org/pdf/2510.04901v1)

**Abstract:**

Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

---

### 141. Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error   Attribution

**Authors:** Adi Banerjee, Anirudh Nair, Tarik Borogovac

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04886v1](http://arxiv.org/pdf/2510.04886v1)

**Abstract:**

Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

---

### 142. RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning   Recipe for Strong Prompt Injection

**Authors:** Yuxin Wen, Arman Zharmagambetov, Ivan Evtimov, Narine Kokhlikyan, Tom Goldstein, Kamalika Chaudhuri, Chuan Guo

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04885v1](http://arxiv.org/pdf/2510.04885v1)

**Abstract:**

Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

---

### 143. Video Game Level Design as a Multi-Agent Reinforcement Learning Problem

**Authors:** Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04862v1](http://arxiv.org/pdf/2510.04862v1)

**Abstract:**

Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

---

### 144. Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the   Rails

**Authors:** Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04860v1](http://arxiv.org/pdf/2510.04860v1)

**Abstract:**

As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

---

### 145. FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration

**Authors:** Victor May, Diganta Misra, Yanqi Luo, Anjali Sridhar, Justine Gehring, Silvio Soares Ribeiro Junior

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04852v1](http://arxiv.org/pdf/2510.04852v1)

**Abstract:**

AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

---

### 146. LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for   Workflow Automation

**Authors:** Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor RÃ¼hle, Saravan Rajmohan

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04851v1](http://arxiv.org/pdf/2510.04851v1)

**Abstract:**

We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

---

### 147. Adapting Insider Risk mitigations for Agentic Misalignment: an empirical   study

**Authors:** Francesca Gomez

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05192v1](http://arxiv.org/pdf/2510.05192v1)

**Abstract:**

Agentic misalignment occurs when goal-directed agents take harmful actions,
such as blackmail, rather than risk goal failure, and can be triggered by
replacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025).
We adapt insider-risk control design (Critical Pathway; Situational Crime
Prevention) to develop preventative operational controls that steer agents
toward safe actions when facing stressors. Using the blackmail scenario from
the original Anthropic study by Lynch et al. (2025), we evaluate mitigations
across 10 LLMs and 66,600 samples. Our main finding is that an externally
governed escalation channel, which guarantees a pause and independent review,
reduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21%
(averaged across all models and conditions). Augmenting this channel with
compliance email bulletins further lowers the blackmail rate to 0.85%. Overall,
incorporating preventative operational controls strengthens defence-in-depth
strategies for agentic AI.
  We also surface a failure mode diverging from Lynch et al. (2025): two models
(Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent
autonomy threat, leveraging sensitive information for coercive signalling. In
counterfactual swaps, both continued using the affair regardless of whether the
CEO or CTO was implicated. An escalation channel eliminated coercion, but
Gemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was
implicated, unlike most models (higher in the CEO condition). The reason for
this divergent behaviour is not clear from raw outputs and could reflect benign
differences in reasoning or strategic discrediting of a potential future
threat, warranting further investigation.

---

### 148. Trade in Minutes! Rationality-Driven Agentic System for Quantitative   Financial Trading

**Authors:** Zifan Song, Kaitao Song, Guosheng Hu, Ding Qi, Junyao Gao, Xiaohua Wang, Dongsheng Li, Cairong Zhao

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04787v1](http://arxiv.org/pdf/2510.04787v1)

**Abstract:**

Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.

---

### 149. Learning on the Job: Test-Time Curricula for Targeted Reinforcement   Learning

**Authors:** Jonas HÃ¼botter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, Moritz Hardt

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04786v1](http://arxiv.org/pdf/2510.04786v1)

**Abstract:**

Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

---

### 150. When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set   Updates

**Authors:** Michele Caprio, Siu Lun Chau, Krikamol Muandet

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04769v1](http://arxiv.org/pdf/2510.04769v1)

**Abstract:**

Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

---

### 151. LMM-Incentive: Large Multimodal Model-based Incentive Design for   User-Generated Content in Web 3.0

**Authors:** Jinbo Wen, Jiawen Kang, Linfeng Zhang, Xiaoying Tang, Jianhang Tang, Yang Zhang, Zhaohui Yang, Dusit Niyato

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04765v1](http://arxiv.org/pdf/2510.04765v1)

**Abstract:**

Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

---

### 152. BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs

**Authors:** Ivo Petrov, Jasper Dekoninck, Martin Vechev

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04721v1](http://arxiv.org/pdf/2510.04721v1)

**Abstract:**

Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

---

### 153. Beyond Outcome Reward: Decoupling Search and Answering Improves LLM   Agents

**Authors:** Yiding Wang, Zhepei Wei, Xinyu Zhu, Yu Meng

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04695v1](http://arxiv.org/pdf/2510.04695v1)

**Abstract:**

Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

---

### 154. Multi-Agent Tool-Integrated Policy Optimization

**Authors:** Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04678v1](http://arxiv.org/pdf/2510.04678v1)

**Abstract:**

Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

---

### 155. Watch and Learn: Learning to Use Computers from Online Videos

**Authors:** Chan Hee Song, Yiwen Song, Palash Goyal, Yu Su, Oriana Riva, Hamid Palangi, Tomas Pfister

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04673v1](http://arxiv.org/pdf/2510.04673v1)

**Abstract:**

Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

---

### 156. QuantAgents: Towards Multi-agent Financial System via Simulated Trading

**Authors:** Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04643v1](http://arxiv.org/pdf/2510.04643v1)

**Abstract:**

In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

---

### 157. Fairness in Repeated Matching: A Maximin Perspective

**Authors:** Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04624v1](http://arxiv.org/pdf/2510.04624v1)

**Abstract:**

We study a sequential decision-making model where a set of items is
repeatedly matched to the same set of agents over multiple rounds. The
objective is to determine a sequence of matchings that either maximizes the
utility of the least advantaged agent at the end of all rounds (optimal) or at
the end of every individual round (anytime optimal). We investigate the
computational challenges associated with finding (anytime) optimal outcomes and
demonstrate that these problems are generally computationally intractable.
However, we provide approximation algorithms, fixed-parameter tractable
algorithms, and identify several special cases whereby the problem(s) can be
solved efficiently. Along the way, we also establish characterizations of
Pareto-optimal/maximum matchings, which may be of independent interest to works
in matching theory and house allocation.

---

### 158. MedPAO: A Protocol-Driven Agent for Structuring Medical Reports

**Authors:** Shrish Shrinath Vaidya, Gowthamaan Palani, Sidharth Ramesh, Velmurugan Balasubramanian, Minmini Selvam, Gokulraja Srinivasaraja, Ganapathy Krishnamurthi

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04623v1](http://arxiv.org/pdf/2510.04623v1)

**Abstract:**

The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

---

### 159. Agentic Context Engineering: Evolving Contexts for Self-Improving   Language Models

**Authors:** Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04618v1](http://arxiv.org/pdf/2510.04618v1)

**Abstract:**

Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

---

### 160. A Case for Declarative LLM-friendly Interfaces for Improved Efficiency   of Computer-Use Agents

**Authors:** Yuan Wang, Mingyu Li, Haibo Chen

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04607v1](http://arxiv.org/pdf/2510.04607v1)

**Abstract:**

Computer-use agents (CUAs) powered by large language models (LLMs) have
emerged as a promising approach to automating computer tasks, yet they struggle
with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to
decompose high-level goals into lengthy, error-prone sequences of fine-grained
actions, resulting in low success rates and an excessive number of LLM calls.
  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms
existing GUIs into three declarative primitives: access, state, and
observation, which are better suited for LLMs. Our key idea is policy-mechanism
separation: LLMs focus on high-level semantic planning (policy) while GOI
handles low-level navigation and interaction (mechanism). GOI does not require
modifying the application source code or relying on application programming
interfaces (APIs).
  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on
Windows. Compared to a leading GUI-based agent baseline, GOI improves task
success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI
completes over 61% of successful tasks with a single LLM call.

---

### 161. Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic   Dilemma

**Authors:** Shurui Li

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04588v1](http://arxiv.org/pdf/2510.04588v1)

**Abstract:**

Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

---

### 162. COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning   over Long Context

**Authors:** Naman Gupta, Shreeyash Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna B Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, Shachee Mishra Gupta

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04568v1](http://arxiv.org/pdf/2510.04568v1)

**Abstract:**

Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

---

### 163. ContextNav: Towards Agentic Multimodal In-Context Learning

**Authors:** Honghao Fu, Yuan Ouyang, Kai-Wei Chang, Yiwei Wang, Zi Huang, Yujun Cai

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04560v1](http://arxiv.org/pdf/2510.04560v1)

**Abstract:**

Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

---

### 164. TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool   Use

**Authors:** Pengfei He, Zhenwei Dai, Bing He, Hui Liu, Xianfeng Tang, Hanqing Lu, Juanhui Li, Jiayuan Ding, Subhabrata Mukherjee, Suhang Wang, Yue Xing, Jiliang Tang, Benoit Dumoulin

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04550v1](http://arxiv.org/pdf/2510.04550v1)

**Abstract:**

Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

---

### 165. Post-training quantization of vision encoders needs prefixing registers

**Authors:** Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04547v1](http://arxiv.org/pdf/2510.04547v1)

**Abstract:**

Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

---

### 166. Code World Models for General Game Playing

**Authors:** Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04542v1](http://arxiv.org/pdf/2510.04542v1)

**Abstract:**

Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

---

### 167. 3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs   Using MCP and RAG

**Authors:** Shun-ichiro Hayashi, Daichi Mukunoki, Tetsuya Hoshino, Satoshi Ohshima, Takahiro Katagiri

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04536v1](http://arxiv.org/pdf/2510.04536v1)

**Abstract:**

This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

---

### 168. More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in   Training Vision-Language Driving Models

**Authors:** Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04532v1](http://arxiv.org/pdf/2510.04532v1)

**Abstract:**

Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

---

### 169. Aria: An Agent For Retrieval and Iterative Auto-Formalization via   Dependency Graph

**Authors:** Hanyu Wang, Ruohan Xie, Yutong Wang, Guoxiong Gao, Xintao Yu, Bin Dong

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04520v1](http://arxiv.org/pdf/2510.04520v1)

**Abstract:**

Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

---

### 170. ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in   Complex Chart Question Answering

**Authors:** Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04514v1](http://arxiv.org/pdf/2510.04514v1)

**Abstract:**

Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

---

### 171. Wavelet Predictive Representations for Non-Stationary Reinforcement   Learning

**Authors:** Min Wang, Xin Li, Ye He, Yao-Hui Li, Hasnaa Bennis, Riashat Islam, Mingzhong Wang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04507v1](http://arxiv.org/pdf/2510.04507v1)

**Abstract:**

The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

---

### 172. GRACE: Generative Representation Learning via Contrastive Policy   Optimization

**Authors:** Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04506v1](http://arxiv.org/pdf/2510.04506v1)

**Abstract:**

Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

---

### 173. Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative   Narrative Script Refinement via Collaborative LLM Agents

**Authors:** Wenda Xie, Chao Guo, Yanqing Jing. Junle Wang, Yisheng Lv, Fei-Yue Wang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05188v1](http://arxiv.org/pdf/2510.05188v1)

**Abstract:**

Although LLMs have been widely adopted for creative content generation, a
single-pass process often struggles to produce high-quality long narratives.
How to effectively revise and improve long narrative scripts like scriptwriters
remains a significant challenge, as it demands a comprehensive understanding of
the entire context to identify global structural issues and local detailed
flaws, as well as coordinating revisions at multiple granularities and
locations. Direct modifications by LLMs typically introduce inconsistencies
between local edits and the overall narrative requirements. To address these
issues, we propose Dramaturge, a task and feature oriented divide-and-conquer
approach powered by hierarchical multiple LLM agents. It consists of a Global
Review stage to grasp the overall storyline and structural issues, a
Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a
Hierarchical Coordinated Revision stage that coordinates and integrates
structural and detailed improvements throughout the script. The top-down task
flow ensures that high-level strategies guide local modifications, maintaining
contextual consistency. The review and revision workflow follows a
coarse-to-fine iterative process, continuing through multiple rounds until no
further substantive improvements can be made. Comprehensive experiments show
that Dramaturge significantly outperforms all baselines in terms of
script-level overall quality and scene-level details. Our approach is
plug-and-play and can be easily integrated into existing methods to improve the
generated scripts.

---

### 174. Impatient Users Confuse AI Agents: High-fidelity Simulations of Human   Traits for Testing Agents

**Authors:** Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04491v1](http://arxiv.org/pdf/2510.04491v1)

**Abstract:**

Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

---

### 175. Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable   LLM Reasoning

**Authors:** Edward Y. Chang, Ethan Y. Chang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04488v1](http://arxiv.org/pdf/2510.04488v1)

**Abstract:**

Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

---

### 176. Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM   Agents

**Authors:** Zhiping Zhang, Yi Evie Zhang, Freda Shi, Tianshi Li

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04465v1](http://arxiv.org/pdf/2510.04465v1)

**Abstract:**

Large Language Model (LLM) agents require personal information for
personalization in order to better act on users' behalf in daily tasks, but
this raises privacy concerns and a personalization-privacy dilemma. Agent's
autonomy introduces both risks and opportunities, yet its effects remain
unclear. To better understand this, we conducted a 3$\times$3 between-subjects
experiment ($N=450$) to study how agent's autonomy level and personalization
influence users' privacy concerns, trust and willingness to use, as well as the
underlying psychological processes. We find that personalization without
considering users' privacy preferences increases privacy concerns and decreases
trust and willingness to use. Autonomy moderates these effects: Intermediate
autonomy flattens the impact of personalization compared to No- and Full
autonomy conditions. Our results suggest that rather than aiming for perfect
model alignment in output generation, balancing autonomy of agent's action and
user control offers a promising path to mitigate the personalization-privacy
dilemma.

---

### 177. Achieve Performatively Optimal Policy for Performative Reinforcement   Learning

**Authors:** Ziyi Chen, Heng Huang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04430v1](http://arxiv.org/pdf/2510.04430v1)

**Abstract:**

Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

---

### 178. Scalable In-context Ranking with Generative Models

**Authors:** Nilesh Gupta, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Inderjit Dhillon, Felix Yu

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05396v1](http://arxiv.org/pdf/2510.05396v1)

**Abstract:**

In-context Ranking (ICR) is an emerging paradigm for Information Retrieval
(IR), which leverages contextual understanding of LLMs by directly
incorporating the task description, candidate documents, and the query into the
model's input prompt and tasking the LLM to identify relevant document(s).
While it is effective, efficiency is a significant challenge in this paradigm,
especially as the candidate list grows due to quadratic/super-linear scaling of
attention operation with context length. To this end, this paper first
identifies inherent and exploitable structures in the attention of LLMs
finetuned for ICR: (1) inter-document block sparsity: attention is dense within
each document block but sparse across different documents in the context; and
(2) query-document block relevance: the attention scores from certain query
tokens to a document block in middle layers strongly correlate with that
document's actual relevance. Motivated by these observations, we introduce
BlockRank (Blockwise In-context Ranking), a novel method that adapts the
attention operation in an LLM by (a) architecturally enforcing the observed
inter-document block sparsity, reducing attention complexity from quadratic to
linear without loss in performance, and (b) optimizing query-document block
relevance for true relevant documents during fine-tuning using an auxiliary
contrastive training objective, improving retrieval in attention. Experiments
on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches
or outperforms existing SOTA listwise rankers and controlled fine-tuned
baseline while being significantly more efficient at inference (4.7x for 100
MSMarco documents in context) and scaling gracefully to long-context
shortlists, around 500 documents in-context (approximately 100K context length)
within a second, presenting a scalable and effective solution for ICR.

---

### 179. Context Length Alone Hurts LLM Performance Despite Perfect Retrieval

**Authors:** Yufeng Du, Minyang Tian, Srikanth Ronanki, Subendhu Rongali, Sravan Bodapati, Aram Galstyan, Azton Wells, Roy Schwartz, Eliu A Huerta, Hao Peng

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05381v1](http://arxiv.org/pdf/2510.05381v1)

**Abstract:**

Large language models (LLMs) often fail to scale their performance on
long-context tasks performance in line with the context lengths they support.
This gap is commonly attributed to retrieval failures -- the models' inability
to identify relevant information in the long inputs. Accordingly, recent
efforts often focus on evaluating and improving LLMs' retrieval performance: if
retrieval is perfect, a model should, in principle, perform just as well on a
long input as it does on a short one -- or should it? This paper presents
findings that the answer to this question may be negative. Our systematic
experiments across 5 open- and closed-source LLMs on math, question answering,
and coding tasks reveal that, even when models can perfectly retrieve all
relevant information, their performance still degrades substantially
(13.9%--85%) as input length increases but remains well within the models'
claimed lengths. This failure occurs even when the irrelevant tokens are
replaced with minimally distracting whitespace, and, more surprisingly, when
they are all masked and the models are forced to attend only to the relevant
tokens. A similar performance drop is observed when all relevant evidence is
placed immediately before the question. Our findings reveal a
previously-unrealized limitation: the sheer length of the input alone can hurt
LLM performance, independent of retrieval quality and without any distraction.
They motivate our simple, model-agnostic mitigation strategy that transforms a
long-context task into a short-context one by prompting the model to recite the
retrieved evidence before attempting to solve the problem. On RULER, we observe
a consistent improvement of GPT-4o up to 4% on an already strong baseline.

---

### 180. MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding   Exemplars as Soft Prompts

**Authors:** Abhinav Jain, Xinyu Yao, Thomas Reps, Christopher Jermaine

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05363v1](http://arxiv.org/pdf/2510.05363v1)

**Abstract:**

Adapting Foundation Models to new domains with limited training data is
challenging and computationally expensive. While prior work has demonstrated
the effectiveness of using domain-specific exemplars as in-context
demonstrations, we investigate whether representing exemplars purely as text is
the most efficient, effective, and stable approach. We explore an alternative:
representing exemplars as soft prompts with an exemplar order invariant model
architecture. To this end, we introduce Multi-Head Attention
Retrieval-Augmented Generation (MHA-RAG), a framework with the number of
attention heads serving as a simple hyperparameter to control soft
prompt-generation across different tasks. Across multiple question-answering
benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over
standard RAG, while cutting inference costs by a factor of 10X
GFLOPs-delivering both higher accuracy and greater efficiency, invariant to
exemplar order.

---

### 181. WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for   Historical Weather Archives

**Authors:** Yongan Yu, Xianda Du, Qingchen Hu, Jiahao Liang, Jingwei Ni, Dan Qiang, Kaiyu Huang, Grant McKenzie, Renee Sieber, Fengran Mo

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05336v1](http://arxiv.org/pdf/2510.05336v1)

**Abstract:**

Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

---

### 182. RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails   under RAG-style Contexts

**Authors:** Yining She, Daniel W. Peterson, Marianne Menglin Liu, Vikas Upadhyay, Mohammad Hossein Chaghazardi, Eunsuk Kang, Dan Roth

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05310v1](http://arxiv.org/pdf/2510.05310v1)

**Abstract:**

With the increasing adoption of large language models (LLMs), ensuring the
safety of LLM systems has become a pressing concern. External LLM-based
guardrail models have emerged as a popular solution to screen unsafe inputs and
outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are
vulnerable to data distribution shifts. In this paper, taking Retrieval
Augmentation Generation (RAG) as a case study, we investigated how robust
LLM-based guardrails are against additional information embedded in the
context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss
models, we confirmed that inserting benign documents into the guardrail context
alters the judgments of input and output guardrails in around 11% and 8% of
cases, making them unreliable. We separately analyzed the effect of each
component in the augmented context: retrieved documents, user query, and
LLM-generated response. The two mitigation methods we tested only bring minor
improvements. These results expose a context-robustness gap in current
guardrails and motivate training and evaluation protocols that are robust to
retrieval and query composition.

---

### 183. Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time   Optimization

**Authors:** Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05038v1](http://arxiv.org/pdf/2510.05038v1)

**Abstract:**

Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

---

### 184. Retrieval-Augmented Code Generation: A Survey with Focus on   Repository-Level Approaches

**Authors:** Yicheng Tao, Yao Qin, Yepang Liu

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04905v1](http://arxiv.org/pdf/2510.04905v1)

**Abstract:**

Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

---

### 185. Revealing Interconnections between Diseases: from Statistical Methods to   Large Language Models

**Authors:** Alina Ermilova, Dmitrii Kornilov, Sofia Samoilova, Ekaterina Laptenkova, Anastasia Kolesnikova, Ekaterina Podplutova, Senotrusova Sofya, Maksim G. Sharaev

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04888v1](http://arxiv.org/pdf/2510.04888v1)

**Abstract:**

Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

---

### 186. When Models Lie, We Learn: Multilingual Span-Level Hallucination   Detection with PsiloQA

**Authors:** Elisei Rykov, Kseniia Petrushina, Maksim Savkin, Valerii Olisov, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, Julia Belikova

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04849v1](http://arxiv.org/pdf/2510.04849v1)

**Abstract:**

Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

---

### 187. Natural Language Edge Labelling: Decoupling Intent from Execution in   Structured LM Reasoning

**Authors:** Abhinav Madahar

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04817v1](http://arxiv.org/pdf/2510.04817v1)

**Abstract:**

Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

---

### 188. ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced   re-ranking retriever

**Authors:** Eduardo MartÃ­nez Rivera, Filippo Menolascina

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04757v1](http://arxiv.org/pdf/2510.04757v1)

**Abstract:**

Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

---

### 189. Topic-Specific Classifiers are Better Relevance Judges than Prompted   LLMs

**Authors:** Lukas Gienapp, Martin Potthast, Harrisen Scells, Eugene Yang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04633v1](http://arxiv.org/pdf/2510.04633v1)

**Abstract:**

The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

---

### 190. Contrastive Learning Using Graph Embeddings for Domain Adaptation of   Language Models in the Process Industry

**Authors:** Anastasia Zhukova, Jonas LÃ¼hrs, Christian E. LobmÃ¼ller, Bela Gipp

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04631v2](http://arxiv.org/pdf/2510.04631v2)

**Abstract:**

Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from graph embeddings (GE) outperform a
state-of-the-art mE5-large text encoder by 9.8-14.3% (5.45-7.96p) on the
proprietary process industry text embedding benchmark (PITEB) while having 3
times fewer parameters.

---

### 191. Fine-grained auxiliary learning for real-world product recommendation

**Authors:** Mario Almagro, Diego Ortego, David Jimenez

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04551v1](http://arxiv.org/pdf/2510.04551v1)

**Abstract:**

Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

---

### 192. MARCO: A Cooperative Knowledge Transfer Framework for Personalized   Cross-domain Recommendations

**Authors:** Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04508v1](http://arxiv.org/pdf/2510.04508v1)

**Abstract:**

Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

---

### 193. Causality-aware Graph Aggregation Weight Estimator for Popularity   Debiasing in Top-K Recommendation

**Authors:** Yue Que, Yingyi Zhang, Xiangyu Zhao, Chen Ma

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04502v1](http://arxiv.org/pdf/2510.04502v1)

**Abstract:**

Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

---

### 194. Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical   Chain-of-Thought Reasoning

**Authors:** Imran Mansha

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05003v1](http://arxiv.org/pdf/2510.05003v1)

**Abstract:**

Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

---

### 195. Graph-based LLM over Semi-Structured Population Data for Dynamic Policy   Response

**Authors:** Daqian Shi, Xiaolei Diao, Jinge Wu, Honghan Wu, Xiongfeng Tang, Felix Naughton, Paulina Bondaronek

**Published:** 2025-10-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05196v1](http://arxiv.org/pdf/2510.05196v1)

**Abstract:**

Timely and accurate analysis of population-level data is crucial for
effective decision-making during public health emergencies such as the COVID-19
pandemic. However, the massive input of semi-structured data, including
structured demographic information and unstructured human feedback, poses
significant challenges to conventional analysis methods. Manual expert-driven
assessments, though accurate, are inefficient, while standard NLP pipelines
often require large task-specific labeled datasets and struggle with
generalization across diverse domains. To address these challenges, we propose
a novel graph-based reasoning framework that integrates large language models
with structured demographic attributes and unstructured public feedback in a
weakly supervised pipeline. The proposed approach dynamically models evolving
citizen needs into a need-aware graph, enabling population-specific analyses
based on key features such as age, gender, and the Index of Multiple
Deprivation. It generates interpretable insights to inform responsive health
policy decision-making. We test our method using a real-world dataset, and
preliminary experimental results demonstrate its feasibility. This approach
offers a scalable solution for intelligent population health monitoring in
resource-constrained clinical and governmental settings.

---

### 196. Utility-Learning Tension in Self-Modifying Agents

**Authors:** Charles L. Wang, Keir Dorchen, Peter Jin

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04399v1](http://arxiv.org/pdf/2510.04399v1)

**Abstract:**

As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

---

### 197. Internal World Models as Imagination Networks in Cognitive Agents

**Authors:** Saurabh Ranjan, Brian Odegaard

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04391v1](http://arxiv.org/pdf/2510.04391v1)

**Abstract:**

What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

---

### 198. Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to   Improve LLM Agents Adaptation

**Authors:** Hadi Nekoei, Aman Jaiswal, Patrice Bechard, Oleh Shliazhko, Orlando Marquez Ayala, Mathieu Reymond, Massimo Caccia, Alexandre Drouin, Sarath Chandar, Alexandre Lacoste

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04373v1](http://arxiv.org/pdf/2510.04373v1)

**Abstract:**

Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

---

### 199. Speculative Actions: A Lossless Framework for Faster Agentic Systems

**Authors:** Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04371v1](http://arxiv.org/pdf/2510.04371v1)

**Abstract:**

Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

---

### 200. NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social   Simulation Environment

**Authors:** Shashank Mangla, Chris Hokamp, Jack Boylan, Demian Gholipour Ghalandari, Yuuv Jauhari, Lauren Cassidy, Oisin Duffy

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04368v1](http://arxiv.org/pdf/2510.04368v1)

**Abstract:**

We design and implement NegotiationGym, an API and user interface for
configuring and running multi-agent social simulations focused upon negotiation
and cooperation. The NegotiationGym codebase offers a user-friendly,
configuration-driven API that enables easy design and customization of
simulation scenarios. Agent-level utility functions encode optimization
criteria for each agent, and agents can self-optimize by conducting multiple
interaction rounds with other agents, observing outcomes, and modifying their
strategies for future rounds.

---

### 201. FairAgent: Democratizing Fairness-Aware Machine Learning with   LLM-Powered Agents

**Authors:** Yucong Dai, Lu Zhang, Feng Luo, Mashrur Chowdhury, Yongkai Wu

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04317v1](http://arxiv.org/pdf/2510.04317v1)

**Abstract:**

Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

---

### 202. Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous   Collections

**Authors:** Bastian JÃ¤ckl, JiÅÃ­ Kruchina, Lucas Joos, Daniel A. Keim, Ladislav PeÅ¡ka, Jakub LokoÄ

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04396v1](http://arxiv.org/pdf/2510.04396v1)

**Abstract:**

Multimodal deep-learning models power interactive video retrieval by ranking
keyframes in response to textual queries. Despite these advances, users must
still browse ranked candidates manually to locate a target. Keyframe
arrangement within the search grid highly affects browsing effectiveness and
user efficiency, yet remains underexplored. We report a study with 49
participants evaluating seven keyframe layouts for the Visual Known-Item Search
task. Beyond efficiency and accuracy, we relate browsing phenomena, such as
overlooks, to layout characteristics. Our results show that a video-grouped
layout is the most efficient, while a four-column, rank-preserving grid
achieves the highest accuracy. Sorted grids reveal potentials and trade-offs,
enabling rapid scanning of uninteresting regions but down-ranking relevant
targets to less prominent positions, delaying first arrival times and
increasing overlooks.
  These findings motivate hybrid designs that preserve positions of top-ranked
items while sorting or grouping the remainder, and offer guidance for searching
in grids beyond video retrieval.

---

### 203. Improving Consistency in Retrieval-Augmented Systems with Group   Similarity Rewards

**Authors:** Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, Alfy Samuel

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04392v1](http://arxiv.org/pdf/2510.04392v1)

**Abstract:**

RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

---

### 204. Critical appraisal of artificial intelligence for rare-event   recognition: principles and pharmacovigilance case studies

**Authors:** G. Niklas Noren, Eva-Lisa Meldau, Johan Ellenius

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04341v1](http://arxiv.org/pdf/2510.04341v1)

**Abstract:**

Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

---

### 205. Equipping Retrieval-Augmented Large Language Models with Document   Structure Awareness

**Authors:** Lingnan Xu, Chong Feng, Kaiyuan Zhang, Liu Zhengyong, Wenqiang Xu, Fanqing Meng

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04293v1](http://arxiv.org/pdf/2510.04293v1)

**Abstract:**

While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

---

### 206. AgentTypo: Adaptive Typographic Prompt Injection Attacks against   Black-box Multimodal Agents

**Authors:** Yanjie Li, Yiming Cao, Dong Wang, Bin Xiao

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04257v1](http://arxiv.org/pdf/2510.04257v1)

**Abstract:**

Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

---

### 207. Empowering Denoising Sequential Recommendation with Large Language Model   Embeddings

**Authors:** Tongzhou Wu, Yuhao Wang, Maolin Wang, Chi Zhang, Xiangyu Zhao

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04239v1](http://arxiv.org/pdf/2510.04239v1)

**Abstract:**

Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

---

### 208. Epistemic Diversity and Knowledge Collapse in Large Language Models

**Authors:** Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04226v2](http://arxiv.org/pdf/2510.04226v2)

**Abstract:**

Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

---

### 209. World-To-Image: Grounding Text-to-Image Generation with Agent-Driven   World Knowledge

**Authors:** Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04201v1](http://arxiv.org/pdf/2510.04201v1)

**Abstract:**

While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

---

### 210. Automating construction safety inspections using a multi-modal   vision-language RAG framework

**Authors:** Chenxin Wang, Elyas Asadi Shamsabadi, Zhaohui Chen, Luming Shen, Alireza Ahmadian Fard Fini, Daniel Dias-da-Costa

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04145v1](http://arxiv.org/pdf/2510.04145v1)

**Abstract:**

Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

---

### 211. Learning-Based Hashing for ANN Search: Foundations and Early Advances

**Authors:** Sean Moran

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04127v1](http://arxiv.org/pdf/2510.04127v1)

**Abstract:**

Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

---

### 212. RLRF: Competitive Search Agent Design via Reinforcement Learning from   Ranker Feedback

**Authors:** Tommy Mordo, Sagie Dekel, Omer Madmon, Moshe Tennenholtz, Oren Kurland

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04096v1](http://arxiv.org/pdf/2510.04096v1)

**Abstract:**

Competitive search is a setting where document publishers modify them to
improve their ranking in response to a query. Recently, publishers have
increasingly leveraged LLMs to generate and modify competitive content. We
introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that
trains LLMs using preference datasets derived from ranking competitions. The
goal of a publisher (LLM-based) agent is to optimize content for improved
ranking while accounting for the strategies of competing agents. We generate
the datasets using approaches that do not rely on human-authored data. We show
that our proposed agents consistently and substantially outperform previously
suggested approaches for LLM-based competitive document modification. We
further show that our agents are effective with ranking functions they were not
trained for (i.e., out of distribution) and they adapt to strategic opponents.
These findings provide support to the significant potential of using
reinforcement learning in competitive search.

---

### 213. MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene   Generation

**Authors:** Zhenyu Pan, Yucheng Lu, Han Liu

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04057v1](http://arxiv.org/pdf/2510.04057v1)

**Abstract:**

We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

---

### 214. LLM Microscope: What Model Internals Reveal About Answer Correctness and   Context Utilization

**Authors:** Jiarui Liu, Jivitesh Jain, Mona Diab, Nishant Subramani

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04013v1](http://arxiv.org/pdf/2510.04013v1)

**Abstract:**

Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

---

### 215. The LCLStream Ecosystem for Multi-Institutional Dataset Exploration

**Authors:** David Rogers, Valerio Mariani, Cong Wang, Ryan Coffee, Wilko Kroeger, Murali Shankar, Hans Thorsten Schwander, Tom Beck, FrÃ©dÃ©ric Poitevin, Jana Thayer

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04012v1](http://arxiv.org/pdf/2510.04012v1)

**Abstract:**

We describe a new end-to-end experimental data streaming framework designed
from the ground up to support new types of applications -- AI training,
extremely high-rate X-ray time-of-flight analysis, crystal structure
determination with distributed processing, and custom data science applications
and visualizers yet to be created. Throughout, we use design choices merging
cloud microservices with traditional HPC batch execution models for security
and flexibility. This project makes a unique contribution to the DOE Integrated
Research Infrastructure (IRI) landscape. By creating a flexible, API-driven
data request service, we address a significant need for high-speed data
streaming sources for the X-ray science data analysis community. With the
combination of data request API, mutual authentication web security framework,
job queue system, high-rate data buffer, and complementary nature to facility
infrastructure, the LCLStreamer framework has prototyped and implemented
several new paradigms critical for future generation experiments.

---

### 216. Visual Lifelog Retrieval through Captioning-Enhanced Interpretation

**Authors:** Yu-Fei Shih, An-Zi Yen, Hen-Hsen Huang, Hsin-Hsi Chen

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04010v1](http://arxiv.org/pdf/2510.04010v1)

**Abstract:**

People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

---

### 217. Quantifying Distributional Robustness of Agentic Tool-Selection

**Authors:** Jehyeok Yeon, Isha Chaudhary, Gagandeep Singh

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03992v1](http://arxiv.org/pdf/2510.03992v1)

**Abstract:**

Large language models (LLMs) are increasingly deployed in agentic systems
where they map user intents to relevant external tools to fulfill a task. A
critical step in this process is tool selection, where a retriever first
surfaces candidate tools from a larger pool, after which the LLM selects the
most appropriate one. This pipeline presents an underexplored attack surface
where errors in selection can lead to severe outcomes like unauthorized data
access or denial of service, all without modifying the agent's model or code.
While existing evaluations measure task performance in benign settings, they
overlook the specific vulnerabilities of the tool selection mechanism under
adversarial conditions. To address this gap, we introduce ToolCert, the first
statistical framework that formally certifies tool selection robustness.
ToolCert models tool selection as a Bernoulli success process and evaluates it
against a strong, adaptive attacker who introduces adversarial tools with
misleading metadata, and are iteratively refined based on the agent's previous
choices. By sampling these adversarial interactions, ToolCert produces a
high-confidence lower bound on accuracy, formally quantifying the agent's
worst-case performance. Our evaluation with ToolCert uncovers the severe
fragility: under attacks injecting deceptive tools or saturating retrieval, the
certified accuracy bound drops near zero, an average performance drop of over
60% compared to non-adversarial settings. For attacks targeting the retrieval
and selection stages, the certified accuracy bound plummets to less than 20%
after just a single round of adversarial adaptation. ToolCert thus reveals
previously unexamined security threats inherent to tool selection and provides
a principled method to quantify an agent's robustness to such threats, a
necessary step for the safe deployment of agentic systems.

---

### 218. Beyond Static Evaluation: Rethinking the Assessment of Personalized   Agent Adaptability in Information Retrieval

**Authors:** Kirandeep Kaur, Preetam Prabhu Srikar Dammu, Hideo Joho, Chirag Shah

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03984v1](http://arxiv.org/pdf/2510.03984v1)

**Abstract:**

Personalized AI agents are becoming central to modern information retrieval,
yet most evaluation methodologies remain static, relying on fixed benchmarks
and one-off metrics that fail to reflect how users' needs evolve over time.
These limitations hinder our ability to assess whether agents can meaningfully
adapt to individuals across dynamic, longitudinal interactions. In this
perspective paper, we propose a conceptual lens for rethinking evaluation in
adaptive personalization, shifting the focus from static performance snapshots
to interaction-aware, evolving assessments. We organize this lens around three
core components: (1) persona-based user simulation with temporally evolving
preference models; (2) structured elicitation protocols inspired by reference
interviews to extract preferences in context; and (3) adaptation-aware
evaluation mechanisms that measure how agent behavior improves across sessions
and tasks. While recent works have embraced LLM-driven user simulation, we
situate this practice within a broader paradigm for evaluating agents over
time. To illustrate our ideas, we conduct a case study in e-commerce search
using the PersonalWAB dataset. Beyond presenting a framework, our work lays a
conceptual foundation for understanding and evaluating personalization as a
continuous, user-centric endeavor.

---

### 219. A global log for medical AI

**Authors:** Ayush Noori, Adam Rodman, Alan Karthikesalingam, Bilal A. Mateen, Christopher A. Longhurst, Daniel Yang, Dave deBronkart, Gauden Galea, Harold F. Wolf III, Jacob Waxman, Joshua C. Mandel, Juliana Rotich, Kenneth D. Mandl, Maryam Mustafa, Melissa Miles, Nigam H. Shah, Peter Lee, Robert Korom, Scott Mahoney, Seth Hain, Tien Yin Wong, Trevor Mundel, Vivek Natarajan, Noa Dagan, David A. Clifton, Ran D. Balicer, Isaac S. Kohane, Marinka Zitnik

**Published:** 2025-10-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.04033v1](http://arxiv.org/pdf/2510.04033v1)

**Abstract:**

Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

---

### 220. No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language   Models

**Authors:** Min Woo Sun, Alejandro Lozano, Javier Gamazo Tejero, Vishwesh Nath, Xiao Xiao Sun, James Burgess, Yuhui Zhang, Kun Yuan, Robert Tibshirani, Sean Huver, Serena Yeung-Levy

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03978v1](http://arxiv.org/pdf/2510.03978v1)

**Abstract:**

Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

---

### 221. Small Language Models for Agentic Systems: A Survey of Architectures,   Capabilities, and Deployment Trade offs

**Authors:** Raghav Sharma, Manan Mehta

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03847v1](http://arxiv.org/pdf/2510.03847v1)

**Abstract:**

Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

---

### 222. Investigating LLM Variability in Personalized Conversational Information   Retrieval

**Authors:** Simon Lupart, DaniÃ«l van Dijk, Eric Langezaal, Ian van Dort, Mohammad Aliannejadi

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03795v1](http://arxiv.org/pdf/2510.03795v1)

**Abstract:**

Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

---

### 223. Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with   Musically Informed Metrics

**Authors:** Hanwen Zhang, Kun Fang, Ziyu Wang, Ichiro Fujinaga

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03750v1](http://arxiv.org/pdf/2510.03750v1)

**Abstract:**

Evaluation for continuous piano pedal depth estimation tasks remains
incomplete when relying only on conventional frame-level metrics, which
overlook musically important features such as direction-change boundaries and
pedal curve contours. To provide more interpretable and musically meaningful
insights, we propose an evaluation framework that augments standard frame-level
metrics with an action-level assessment measuring direction and timing using
segments of press/hold/release states and a gesture-level analysis that
evaluates contour similarity of each press-release cycle. We apply this
framework to compare an audio-only baseline with two variants: one
incorporating symbolic information from MIDI, and another trained in a
binary-valued setting, all within a unified architecture. Results show that the
MIDI-informed model significantly outperforms the others at action and gesture
levels, despite modest frame-level gains. These findings demonstrate that our
framework captures musically relevant improvements indiscernible by traditional
metrics, offering a more practical and effective approach to evaluating pedal
depth estimation models.

---

### 224. H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis

**Authors:** Seungseop Lim, Gibaeg Kim, Hyunkyung Lee, Wooseok Han, Jean Seo, Jaehyo Yoo, Eunho Yang

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03700v1](http://arxiv.org/pdf/2510.03700v1)

**Abstract:**

An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

---

### 225. MedReflect: Teaching Medical LLMs to Self-Improve via Reflective   Correction

**Authors:** Yue Huang, Yanyuan Chen, Dexuan Xu, Weihua Yue, Huamin Zhang, Meikang Qiu, Yu Huang

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03687v1](http://arxiv.org/pdf/2510.03687v1)

**Abstract:**

Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

---

### 226. UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG

**Authors:** Xiangyu Peng, Cab Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Chien-Sheng Wu

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03663v1](http://arxiv.org/pdf/2510.03663v1)

**Abstract:**

Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

---

### 227. Can an LLM Induce a Graph? Investigating Memory Drift and Context Length

**Authors:** Raquib Bin Yousuf, Aadyant Khatri, Shengzhe Xu, Mandar Sharma, Naren Ramakrishnan

**Published:** 2025-10-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03611v1](http://arxiv.org/pdf/2510.03611v1)

**Abstract:**

Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

---

### 228. LLM, Reporting In! Medical Information Extraction Across Prompting,   Fine-tuning and Post-correction

**Authors:** Ikram Belmadani, Parisa Nazari Hashemi, Thomas Sebbag, Benoit Favre, Guillaume Fortier, Solen Quiniou, Emmanuel Morin, Richard Dufour

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03577v1](http://arxiv.org/pdf/2510.03577v1)

**Abstract:**

This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

---

### 229. Identifying Financial Risk Information Using RAG with a Contrastive   Insight

**Authors:** Ali Elahi

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03521v1](http://arxiv.org/pdf/2510.03521v1)

**Abstract:**

In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

---

### 230. SEER: The Span-based Emotion Evidence Retrieval Benchmark

**Authors:** Aneesha Sampath, Oya Aran, Emily Mower Provost

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03490v1](http://arxiv.org/pdf/2510.03490v1)

**Abstract:**

We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

---

### 231. Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text,   Image, Audio, and Video

**Authors:** Mengyao Xu, Wenfei Zhou, Yauhen Babakhin, Gabriel Moreira, Ronay Ak, Radek Osmulski, Bo Liu, Even Oldridge, Benedikt Schifferer

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03458v1](http://arxiv.org/pdf/2510.03458v1)

**Abstract:**

We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

---

### 232. ContraGen: A Multi-Agent Generation Framework for Enterprise   Contradictions Detection

**Authors:** Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji, Nand Dave, Anudha Mittal

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03418v1](http://arxiv.org/pdf/2510.03418v1)

**Abstract:**

Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

---

### 233. FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of   Web Agents

**Authors:** Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han LÃ¹, LÃ©o Boisvert, Massimo Caccia, JÃ©rÃ©my Espinas, Alexandre Aussem, VÃ©ronique Eglin, Alexandre Lacoste

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03204v1](http://arxiv.org/pdf/2510.03204v1)

**Abstract:**

Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

---

### 234. OpenZL: A Graph-Based Model for Compression

**Authors:** Yann Collet, Nick Terrell, W. Felix Handte, Danielle Rozenblit, Victor Zhang, Kevin Zhang, Yaelle Goldschlag, Jennifer Lee, Daniel Riegel, Stan Angelov, Nadav Rotem

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03203v1](http://arxiv.org/pdf/2510.03203v1)

**Abstract:**

Research in general-purpose lossless compression over the last decade has
largely found improvements in compression ratio that come at great cost to
resource utilization and processing throughput. However, most production
workloads require high throughput and low resource utilization, so most
research systems have seen little adoption. Instead, real world improvements in
compression are increasingly often realized by building application-specific
compressors which can exploit knowledge about the structure and semantics of
the data being compressed. These systems easily outperform even the best
generic compressors, but application-specific compression schemes are not
without drawbacks. They are inherently limited in applicability and are
difficult to maintain and deploy.
  We show that these challenges can be overcome with a new way of thinking
about compression. We propose the ``graph model'' of compression, a new
theoretical framework for representing compression as a directed acyclic graph
of modular codecs. This motivates OpenZL, an implementation of this model that
compresses data into a self-describing wire format, any configuration of which
can be decompressed by a universal decoder. OpenZL's design enables rapid
development of tailored compressors with minimal code, its universal decoder
eliminates deployment lag, and its investment in a well-vetted standard
component library minimizes security risks. Experimental results demonstrate
that OpenZL achieves superior compression ratios and speeds compared to
state-of-the-art general-purpose compressors on a variety of real-world
datasets. Internal deployments at Meta have also shown consistent improvements
in size and/or speed, with development timelines reduced from months to days.
OpenZL thus represents an advance in practical, scalable, and maintainable data
compression for modern data-intensive applications.

---

### 235. CHORD: Customizing Hybrid-precision On-device Model for Sequential   Recommendation with Device-cloud Collaboration

**Authors:** Tianqi Liu, Kairui Fu, Shengyu Zhang, Wenyan Fan, Zhaocheng Du, Jieming Zhu, Fan Wu, Fei Wu

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03038v1](http://arxiv.org/pdf/2510.03038v1)

**Abstract:**

With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

---

### 236. Grounding Large Language Models in Clinical Evidence: A   Retrieval-Augmented Generation System for Querying UK NICE Clinical   Guidelines

**Authors:** Matthew Lewis, Samuel Thio, Richard JB Dobson, Spiros Denaxas

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02967v1](http://arxiv.org/pdf/2510.02967v1)

**Abstract:**

This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

---

### 237. Finding Diamonds in Conversation Haystacks: A Benchmark for   Conversational Data Retrieval

**Authors:** Yohan Lee, Yongwoo Song, Sangyeop Kim

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02938v1](http://arxiv.org/pdf/2510.02938v1)

**Abstract:**

We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

---

### 238. FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with   Knowledge Graph Evidence

**Authors:** Abhinav Arun, Reetu Raj Harsh, Bhaskarjit Sarmah, Stefano Pasquali

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02906v1](http://arxiv.org/pdf/2510.02906v1)

**Abstract:**

Multi-hop reasoning over financial disclosures is often a retrieval problem
before it becomes a reasoning or generation problem: relevant facts are
dispersed across sections, filings, companies, and years, and LLMs often expend
excessive tokens navigating noisy context. Without precise Knowledge Graph
(KG)-guided selection of relevant context, even strong reasoning models either
fail to answer or consume excessive tokens, whereas KG-linked evidence enables
models to focus their reasoning on composing already retrieved facts. We
present FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a
temporally indexed financial KG that links audited triples to source chunks
from S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns
across sectors (via GICS taxonomy), we generate financial analyst style
questions with exact supporting evidence from the KG. A two-phase pipeline
first creates QA pairs via pattern-specific prompts, followed by a
multi-criteria quality control evaluation to ensure QA validity. We then
evaluate three controlled retrieval scenarios: (S1) precise KG-linked paths;
(S2) text-only page windows centered on relevant text spans; and (S3) relevant
page windows with randomizations and distractors. Across both reasoning and
non-reasoning models, KG-guided precise retrieval yields substantial gains on
the FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores
by approximately 24 percent while reducing token utilization by approximately
84.5 percent compared to the page window setting, which reflects the
traditional vector retrieval paradigm. Spanning intra-document, inter-year, and
cross-company scopes, our work underscores the pivotal role of knowledge graphs
in efficiently connecting evidence for multi-hop financial QA. We also release
a curated subset of the benchmark (555 QA Pairs) to catalyze further research.

---

### 239. Evaluating Large Language Models for IUCN Red List Species Information

**Authors:** Shinya Uryu

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02830v1](http://arxiv.org/pdf/2510.02830v1)

**Abstract:**

Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

---

### 240. StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop   Question Answering

**Authors:** Tengjun Ni, Xin Yuan, Shenghong Li, Kai Wu, Ren Ping Liu, Wei Ni, Wenjie Zhang

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02827v1](http://arxiv.org/pdf/2510.02827v1)

**Abstract:**

Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

---

### 241. Work Zones challenge VLM Trajectory Planning: Toward Mitigation and   Robust Autonomous Driving

**Authors:** Yifan Liao, Zhen Sun, Xiaoyun Qiu, Zixiao Zhao, Wenbing Tang, Xinlei He, Xinhu Zheng, Tianwei Zhang, Xinyi Huang, Xingshuo Han

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02803v1](http://arxiv.org/pdf/2510.02803v1)

**Abstract:**

Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.

---

### 242. Disentangling Recall and Reasoning in Transformer Models through   Layer-wise Attention and Activation Analysis

**Authors:** Harshwardhan Fartale, Ashish Kattamuri, Rahul Raja, Arpita Vats, Ishita Prasad, Akshata Kishore Moharir

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03366v1](http://arxiv.org/pdf/2510.03366v1)

**Abstract:**

Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

---

### 243. AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large   Language Models

**Authors:** Bo Ma, Hang Li, ZeHua Hu, XiaoFan Gui, LuYao Liu, Simon Liu

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02669v1](http://arxiv.org/pdf/2510.02669v1)

**Abstract:**

Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

---

### 244. AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable   Recommender Systems

**Authors:** Bo Ma, Hang Li, ZeHua Hu, XiaoFan Gui, LuYao Liu, Simon Liu

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02668v1](http://arxiv.org/pdf/2510.02668v1)

**Abstract:**

Foundation models have revolutionized artificial intelligence, yet their
application in recommender systems remains limited by reasoning opacity and
knowledge constraints. This paper introduces AgenticRAG, a novel framework that
combines tool-augmented foundation models with retrieval-augmented generation
for zero-shot explainable recommendations. Our approach integrates external
tool invocation, knowledge retrieval, and chain-of-thought reasoning to create
autonomous recommendation agents capable of transparent decision-making without
task-specific training. Experimental results on three real-world datasets
demonstrate that AgenticRAG achieves consistent improvements over
state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon
Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework
exhibits superior explainability while maintaining computational efficiency
comparable to traditional methods.

---

### 245. Less LLM, More Documents: Searching for Improved RAG

**Authors:** Jingjie Ning, Yibo Kong, Yunfan Long, Jamie Callan

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02657v2](http://arxiv.org/pdf/2510.02657v2)

**Abstract:**

Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.

---

### 246. A Simple but Effective Elaborative Query Reformulation Approach for   Natural Language Recommendation

**Authors:** Qianfeng Wen, Yifan Liu, Justin Cui, Joshua Zhang, Anton Korikov, George-Kirollos Saad, Scott Sanner

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02656v1](http://arxiv.org/pdf/2510.02656v1)

**Abstract:**

Natural Language (NL) recommender systems aim to retrieve relevant items from
free-form user queries and item descriptions. Existing systems often rely on
dense retrieval (DR), which struggles to interpret challenging queries that
express broad (e.g., "cities for youth friendly activities") or indirect (e.g.,
"cities for a high school graduation trip") user intents. While query
reformulation (QR) has been widely adopted to improve such systems, existing QR
methods tend to focus only on expanding the range of query subtopics (breadth)
or elaborating on the potential meaning of a query (depth), but not both. In
this paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large
language model-based QR method that combines both breadth and depth by
generating potential query subtopics with information-rich elaborations. We
also introduce three new natural language recommendation benchmarks in travel,
hotel, and restaurant domains to establish evaluation of NL recommendation with
challenging queries. Experiments show EQR substantially outperforms
state-of-the-art QR methods in various evaluation metrics, highlighting that a
simple yet effective QR approach can significantly improve NL recommender
systems for queries with broad and indirect user intents.

---

### 247. Geolog-IA: Conversational System for Academic Theses

**Authors:** Micaela Fuel Pozo, Andrea Guatumillo Saltos, YeseÃ±a Tipan Llumiquinga, Kelly Lascano Aguirre, Marilyn Castillo Jara, Christian Mejia-Escobar

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02653v1](http://arxiv.org/pdf/2510.02653v1)

**Abstract:**

This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

---

### 248. Automatic Building Code Review: A Case Study

**Authors:** Hanlong Wan, Weili Xu, Michael Rosenberg, Jian Zhang, Aysha Siddika

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02634v1](http://arxiv.org/pdf/2510.02634v1)

**Abstract:**

Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

---

### 249. Improving GUI Grounding with Explicit Position-to-Coordinate Mapping

**Authors:** Suyuchen Wang, Tianyu Zhang, Ahmed Masry, Christopher Pal, Spandana Gella, Bang Liu, Perouz Taslakian

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03230v1](http://arxiv.org/pdf/2510.03230v1)

**Abstract:**

GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

---

### 250. Learning Stability Certificate for Robotics in Real-World Environments

**Authors:** Zhe Shen

**Published:** 2025-10-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03123v1](http://arxiv.org/pdf/2510.03123v1)

**Abstract:**

Stability certificates play a critical role in ensuring the safety and
reliability of robotic systems. However, deriving these certificates for
complex, unknown systems has traditionally required explicit knowledge of
system dynamics, often making it a daunting task. This work introduces a novel
framework that learns a Lyapunov function directly from trajectory data,
enabling the certification of stability for autonomous systems without needing
detailed system models. By parameterizing the Lyapunov candidate using a neural
network and ensuring positive definiteness through Cholesky factorization, our
approach automatically identifies whether the system is stable under the given
trajectory. To address the challenges posed by noisy, real-world data, we allow
for controlled violations of the stability condition, focusing on maintaining
high confidence in the stability certification process. Our results demonstrate
that this framework can provide data-driven stability guarantees, offering a
robust method for certifying the safety of robotic systems in dynamic,
real-world environments. This approach works without access to the internal
control algorithms, making it applicable even in situations where system
behavior is opaque or proprietary. The tool for learning the stability proof is
open-sourced by this research: https://github.com/HansOersted/stability.

---

### 251. Knowledge-Graph Based RAG System Evaluation Framework

**Authors:** Sicheng Dong, Vahid Zolfaghari, Nenad Petrovic, Alois Knoll

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02549v1](http://arxiv.org/pdf/2510.02549v1)

**Abstract:**

Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

---

### 252. Hierarchical Semantic Retrieval with Cobweb

**Authors:** Anant Gupta, Karthik Singaravadivelan, Zekun Wang

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02539v1](http://arxiv.org/pdf/2510.02539v1)

**Abstract:**

Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

---

### 253. Revisiting Query Variants: The Advantage of Retrieval Over Generation of   Query Variants for Effective QPP

**Authors:** Fangzheng Tian, Debasis Ganguly, Craig Macdonald

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02512v1](http://arxiv.org/pdf/2510.02512v1)

**Abstract:**

Leveraging query variants (QVs), i.e., queries with potentially similar
information needs to the target query, has been shown to improve the
effectiveness of query performance prediction (QPP) approaches. Existing
QV-based QPP methods generate QVs facilitated by either query expansion or
non-contextual embeddings, which may introduce topical drifts and
hallucinations. In this paper, we propose a method that retrieves QVs from a
training set (e.g., MS MARCO) for a given target query of QPP. To achieve a
high recall in retrieving queries with the most similar information needs as
the target query from a training set, we extend the directly retrieved QVs
(1-hop QVs) by a second retrieval using their denoted relevant documents (which
yields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show
that the QPP methods with QVs retrieved by our method outperform the
best-performing existing generated-QV-based QPP approaches by as much as around
20\%, on neural ranking models like MonoT5.

---

### 254. AccurateRAG: A Framework for Building Accurate Retrieval-Augmented   Question-Answering Applications

**Authors:** Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Van-Cuong Pham, Hoang Ngo, Dat Quoc Nguyen

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02243v1](http://arxiv.org/pdf/2510.02243v1)

**Abstract:**

We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.

---

### 255. Study on LLMs for Promptagator-Style Dense Retriever Training

**Authors:** Daniel Gwon, Nour Jedidi, Jimmy Lin

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02241v1](http://arxiv.org/pdf/2510.02241v1)

**Abstract:**

Promptagator demonstrated that Large Language Models (LLMs) with few-shot
prompts can be used as task-specific query generators for fine-tuning
domain-specialized dense retrieval models. However, the original Promptagator
approach relied on proprietary and large-scale LLMs which users may not have
access to or may be prohibited from using with sensitive data. In this work, we
study the impact of open-source LLMs at accessible scales ($\leq$14B
parameters) as an alternative. Our results demonstrate that open-source LLMs as
small as 3B parameters can serve as effective Promptagator-style query
generators. We hope our work will inform practitioners with reliable
alternatives for synthetic data generation and give insights to maximize
fine-tuning results for domain-specific applications.

---

### 256. Contrastive Retrieval Heads Improve Attention-Based Re-Ranking

**Authors:** Linh Tran, Yulong Li, Radu Florian, Wei Sun

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02219v1](http://arxiv.org/pdf/2510.02219v1)

**Abstract:**

The strong zero-shot and long-context capabilities of recent Large Language
Models (LLMs) have paved the way for highly effective re-ranking systems.
Attention-based re-rankers leverage attention weights from transformer heads to
produce relevance scores, but not all heads are created equally: many
contribute noise and redundancy, thus limiting performance. To address this, we
introduce CoRe heads, a small set of retrieval heads identified via a
contrastive scoring metric that explicitly rewards high attention heads that
correlate with relevant documents, while downplaying nodes with higher
attention that correlate with irrelevant documents. This relative ranking
criterion isolates the most discriminative heads for re-ranking and yields a
state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs
show that aggregated signals from CoRe heads, constituting less than 1% of all
heads, substantially improve re-ranking accuracy over strong baselines. We
further find that CoRe heads are concentrated in middle layers, and pruning the
computation of final 50% of model layers preserves accuracy while significantly
reducing inference time and memory usage.

---

### 257. Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of   Privacy Filters for Synthetic Data Generation

**Authors:** Adil Koeken, Alexander Ziller, Moritz Knolle, Daniel Rueckert

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01793v1](http://arxiv.org/pdf/2510.01793v1)

**Abstract:**

The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

---

### 258. CardioRAG: A Retrieval-Augmented Generation Framework for Multimodal   Chagas Disease Detection

**Authors:** Zhengyang Shen, Xuehao Zhai, Hua Tu, Mayue Shi

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01558v1](http://arxiv.org/pdf/2510.01558v1)

**Abstract:**

Chagas disease affects nearly 6 million people worldwide, with Chagas
cardiomyopathy representing its most severe complication. In regions where
serological testing capacity is limited, AI-enhanced electrocardiogram (ECG)
screening provides a critical diagnostic alternative. However, existing machine
learning approaches face challenges such as limited accuracy, reliance on large
labeled datasets, and more importantly, weak integration with evidence-based
clinical diagnostic indicators. We propose a retrieval-augmented generation
framework, CardioRAG, integrating large language models with interpretable
ECG-based clinical features, including right bundle branch block, left anterior
fascicular block, and heart rate variability metrics. The framework uses
variational autoencoder-learned representations for semantic case retrieval,
providing contextual cases to guide clinical reasoning. Evaluation demonstrated
high recall performance of 89.80%, with a maximum F1 score of 0.68 for
effective identification of positive cases requiring prioritized serological
testing. CardioRAG provides an interpretable, clinical evidence-based approach
particularly valuable for resource-limited settings, demonstrating a pathway
for embedding clinical indicators into trustworthy medical AI systems.

---

### 259. Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research   Challenge

**Authors:** Charlie Masters, Advaith Vellanki, Jiangbo Shangguan, Bart Kultys, Jonathan Gilmore, Alastair Moore, Stefano V. Albrecht

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02557v1](http://arxiv.org/pdf/2510.02557v1)

**Abstract:**

While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

---

### 260. TACOS: Task Agnostic COordinator of a multi-drone System

**Authors:** Alessandro Nazzari, Roberto Rubinacci, Marco Lovera

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01869v1](http://arxiv.org/pdf/2510.01869v1)

**Abstract:**

When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

---

### 261. Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language   Models in Autonomous Driving

**Authors:** Haibo Hu, Lianming Huang, Xinyu Wang, Yufei Cui, Nan Guan, Chun Jason Xue

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01795v1](http://arxiv.org/pdf/2510.01795v1)

**Abstract:**

Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

---

### 262. SoK: Measuring What Matters for Closed-Loop Security Agents

**Authors:** Mudita Khurana, Raunak Jain

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01654v1](http://arxiv.org/pdf/2510.01654v1)

**Abstract:**

Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

---

### 263. Position: Privacy Is Not Just Memorization!

**Authors:** Niloofar Mireshghallah, Tianshi Li

**Published:** 2025-10-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01645v1](http://arxiv.org/pdf/2510.01645v1)

**Abstract:**

The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

---

### 264. Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic   Robots via Dynamic Parabolic Control Barrier Functions

**Authors:** Hun Kuk Park, Taekyung Kim, Dimitra Panagou

**Published:** 2025-10-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01402v1](http://arxiv.org/pdf/2510.01402v1)

**Abstract:**

Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety
of autonomous systems, yet applying them to nonholonomic robots in cluttered,
dynamic environments remains an open challenge. State-of-the-art methods often
rely on collision-cone or velocity-obstacle constraints which, by only
considering the angle of the relative velocity, are inherently conservative and
can render the CBF-based quadratic program infeasible, particularly in dense
scenarios. To address this issue, we propose a Dynamic Parabolic Control
Barrier Function (DPCBF) that defines the safe set using a parabolic boundary.
The parabola's vertex and curvature dynamically adapt based on both the
distance to an obstacle and the magnitude of the relative velocity, creating a
less restrictive safety constraint. We prove that the proposed DPCBF is valid
for a kinematic bicycle model subject to input constraints. Extensive
comparative simulations demonstrate that our DPCBF-based controller
significantly enhances navigation success rates and QP feasibility compared to
baseline methods. Our approach successfully navigates through dense
environments with up to 100 dynamic obstacles, scenarios where collision
cone-based methods fail due to infeasibility.

---

### 265. The Social Laboratory: A Psychometric Framework for Multi-Agent LLM   Evaluation

**Authors:** Zarreen Reza

**Published:** 2025-10-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.01295v1](http://arxiv.org/pdf/2510.01295v1)

**Abstract:**

As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

---

### 266. Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual   Navigation

**Authors:** Yiyuan Pan, Yunzhe Xu, Zhe Liu, Hesheng Wang

**Published:** 2025-10-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.00441v1](http://arxiv.org/pdf/2510.00441v1)

**Abstract:**

Visual navigation is a fundamental problem in embodied AI, yet practical
deployments demand long-horizon planning capabilities to address
multi-objective tasks. A major bottleneck is data scarcity: policies learned
from limited data often overfit and fail to generalize OOD. Existing neural
network-based agents typically increase architectural complexity that
paradoxically become counterproductive in the small-sample regime. This paper
introduce NeuRO, a integrated learning-to-optimize framework that tightly
couples perception networks with downstream task-level robust optimization.
Specifically, NeuRO addresses core difficulties in this integration: (i) it
transforms noisy visual predictions under data scarcity into convex uncertainty
sets using Partially Input Convex Neural Networks (PICNNs) with conformal
calibration, which directly parameterize the optimization constraints; and (ii)
it reformulates planning under partial observability as a robust optimization
problem, enabling uncertainty-aware policies that transfer across environments.
Extensive experiments on both unordered and sequential multi-object navigation
tasks demonstrate that NeuRO establishes SoTA performance, particularly in
generalization to unseen environments. Our work thus presents a significant
advancement for developing robust, generalizable autonomous agents.

---

### 267. LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and   MCI-Like Field Simulation

**Authors:** Joshua Sebastian, Karma Tobden, KMA Solaiman

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.26351v1](http://arxiv.org/pdf/2509.26351v1)

**Abstract:**

Research on emergency and mass casualty incident (MCI) triage has been
limited by the absence of openly usable, reproducible benchmarks. Yet these
scenarios demand rapid identification of the patients most in need, where
accurate deterioration prediction can guide timely interventions. While the
MIMIC-IV-ED database is openly available to credentialed researchers,
transforming it into a triage-focused benchmark requires extensive
preprocessing, feature harmonization, and schema alignment -- barriers that
restrict accessibility to only highly technical users.
  We address these gaps by first introducing an open, LLM-assisted emergency
triage benchmark for deterioration prediction (ICU transfer, in-hospital
mortality). The benchmark then defines two regimes: (i) a hospital-rich setting
with vitals, labs, notes, chief complaints, and structured observations, and
(ii) an MCI-like field simulation limited to vitals, observations, and notes.
Large language models (LLMs) contributed directly to dataset construction by
(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)
prioritizing clinically relevant vitals and labs, and (iii) guiding schema
alignment and efficient merging of disparate tables.
  We further provide baseline models and SHAP-based interpretability analyses,
illustrating predictive gaps between regimes and the features most critical for
triage. Together, these contributions make triage prediction research more
reproducible and accessible -- a step toward dataset democratization in
clinical AI.

---

### 268. Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical   Practice

**Authors:** Jack Gallifant, Katherine C. Kellogg, Matt Butler, Amanda Centi, Shan Chen, Patrick F. Doyle, Sayon Dutta, Joyce Guo, Matthew J. Hadfield, Esther H. Kim, David E. Kozono, Hugo JWL Aerts, Adam B. Landman, Raymond H. Mak, Rebecca G. Mishuris, Tanna L. Nelson, Guergana K. Savova, Elad Sharon, Benjamin C. Silverman, Umit Topaloglu, Jeremy L. Warner, Danielle S. Bitterman

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.26153v2](http://arxiv.org/pdf/2509.26153v2)

**Abstract:**

Large language models (LLMs) integrated into agent-driven workflows hold
immense promise for healthcare, yet a significant gap exists between their
potential and practical implementation within clinical settings. To address
this, we present a practitioner-oriented field manual for deploying generative
agents that use electronic health record (EHR) data. This guide is informed by
our experience deploying the "irAE-Agent", an automated system to detect
immune-related adverse events from clinical notes at Mass General Brigham, and
by structured interviews with 20 clinicians, engineers, and informatics leaders
involved in the project. Our analysis reveals a critical misalignment in
clinical AI development: less than 20% of our effort was dedicated to prompt
engineering and model development, while over 80% was consumed by the
sociotechnical work of implementation. We distill this effort into five "heavy
lifts": data integration, model validation, ensuring economic value, managing
system drift, and governance. By providing actionable solutions for each of
these challenges, this field manual shifts the focus from algorithmic
development to the essential infrastructure and implementation work required to
bridge the "valley of death" and successfully translate generative AI from
pilot projects into routine clinical care.

---

### 269. Dolphin v1.0 Technical Report

**Authors:** Taohan Weng, Chi zhang, Chaoran Yan, Siya Liu, Xiaoyang Liu, Yalun Wu, Boyang Wang, Boyan Wang, Jiren Ren, Kaiwen Yan, Jinze Yu, Kaibing Hu, Henan Liu, Haoyun Zheng, Zhenyu Liu, Duo Zhang, Xiaoqing Guo, Anjie Le, Hongcheng Guo

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.25748v2](http://arxiv.org/pdf/2509.25748v2)

**Abstract:**

Ultrasound is crucial in modern medicine but faces challenges like operator
dependence, image noise, and real-time scanning, hindering AI integration.
While large multimodal models excel in other medical imaging areas, they
struggle with ultrasound's complexities. To address this, we introduce Dolphin
v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale
multimodal ultrasound foundation models unifying diverse clinical tasks in a
single vision-language framework.To tackle ultrasound variability and noise, we
curated a 2-million-scale multimodal dataset, combining textbook knowledge,
public data, synthetic samples, and general corpora. This ensures robust
perception, generalization, and clinical adaptability.The Dolphin series
employs a three-stage training strategy: domain-specialized pretraining,
instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0
delivers reliable performance in classification, detection, regression, and
report generation. Dolphin R1 enhances diagnostic inference, reasoning
transparency, and interpretability through reinforcement learning with
ultrasound-specific rewards.Evaluated on U2-Bench across eight ultrasound
tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best
model (0.2968) setting a new state of the art. Dolphin v1.0 also performs
competitively, validating the unified framework. Comparisons show
reasoning-enhanced training significantly improves diagnostic accuracy,
consistency, and interpretability, highlighting its importance for high-stakes
medical AI.

---

### 270. BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive   Path-Integral Control

**Authors:** Odichimnma Ezeji, Michael Ziegltrum, Giulio Turrisi, Tommaso Belvedere, Valerio Modugno

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.00272v1](http://arxiv.org/pdf/2510.00272v1)

**Abstract:**

Model Predictive Path Integral (MPPI) control has recently emerged as a fast,
gradient-free alternative to model-predictive control in highly non-linear
robotic tasks, yet it offers no hard guarantees on constraint satisfaction. We
introduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that
attaches a probabilistic surrogate to every state and input constraint. At each
re-planning step the surrogate returns the probability that a candidate
trajectory is feasible; this joint probability scales the weight given to a
candidate, automatically down-weighting rollouts likely to collide or exceed
limits and pushing the sampling distribution toward the safe subset; no
hand-tuned penalty costs or explicit sample rejection required. We train the
surrogate from 1000 offline simulations and deploy the controller on a
quadrotor in MuJoCo with both static and moving obstacles. Across K in
[100,1500] rollouts BC-MPPI preserves safety margins while satisfying the
prescribed probability of violation. Because the surrogate is a stand-alone,
version-controlled artefact and the runtime safety score is a single scalar,
the approach integrates naturally with verification-and-validation pipelines
for certifiable autonomous systems.

---

### 271. TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic   Tasks

**Authors:** Yue Meng, Fei Chen, Chuchu Fan

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.00225v1](http://arxiv.org/pdf/2510.00225v1)

**Abstract:**

Learning control policies for complex, long-horizon tasks is a central
challenge in robotics and autonomous systems. Signal Temporal Logic (STL)
offers a powerful and expressive language for specifying such tasks, but its
non-Markovian nature and inherent sparse reward make it difficult to be solved
via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus
only on limited STL fragments or use STL robustness scores as sparse terminal
rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,
to solve general STL tasks. TGPO decomposes STL into timed subgoals and
invariant constraints and provides a hierarchical framework to tackle the
problem. The high-level component of TGPO proposes concrete time allocations
for these subgoals, and the low-level time-conditioned policy learns to achieve
the sequenced subgoals using a dense, stage-wise reward signal. During
inference, we sample various time allocations and select the most promising
assignment for the policy network to rollout the solution trajectory. To foster
efficient policy learning for complex STL with multiple subgoals, we leverage
the learned critic to guide the high-level temporal search via
Metropolis-Hastings sampling, focusing exploration on temporally feasible
solutions. We conduct experiments on five environments, ranging from
low-dimensional navigation to manipulation, drone, and quadrupedal locomotion.
Under a wide range of STL tasks, TGPO significantly outperforms
state-of-the-art baselines (especially for high-dimensional and long-horizon
cases), with an average of 31.6% improvement in task success rate compared to
the best baseline. The code will be available at
https://github.com/mengyuest/TGPO

---

### 272. Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs

**Authors:** Hankun Dai, Maoquan Wang, Mengnan Qi, Yikai Zhang, Zijian Jin, Yongqiang Yao, Yufan Huang, Shengyu Fu, Elsie Nallipogu

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.25873v1](http://arxiv.org/pdf/2509.25873v1)

**Abstract:**

Large language models (LLMs) are increasingly being applied to programming
tasks, ranging from single-turn code completion to autonomous agents. Current
code agent designs frequently depend on complex, hand-crafted workflows and
tool sets. However, this reliance on elaborate scaffolding presents several
challenges: agent performance becomes overly dependent on prompt tuning and
custom design choices, heavy human intervention obscures a model's true
underlying capabilities, and intricate pipelines are costly to build and
maintain. Furthermore, optimizing complex task prompts increases the risk of
data leakage. Currently, when introducing new models, LLM providers like OpenAI
and Anthropic often publish benchmark scores to demonstrate their models'
coding proficiency, but keep their proprietary evaluation frameworks
confidential. To address these limitations, we introduce Lita (Lite Agent),
which operationalizes liteness, a principle of minimizing manual design while
retaining the essential elements of a fully autonomous agent. Lita enables a
more faithful and unified evaluation without elaborate scaffolding. Experiments
on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita
achieves competitive or superior performance compared to workflow-based and
agentic baselines. Crucially, Lita also consumes fewer tokens and requires
significantly less design effort. Our results suggest that Lita is sufficient
to reveal the underlying coding competence of modern LLMs. Finally, we propose
the Agent Complexity Law: the performance gap between agents of varying
complexity, from simple to sophisticated designs, will shrink as the core model
improves, ultimately converging to a negligible difference.

---

### 273. STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents

**Authors:** Jing-Jing Li, Jianfeng He, Chao Shang, Devang Kulshreshtha, Xun Xian, Yi Zhang, Hang Su, Sandesh Swamy, Yanjun Qi

**Published:** 2025-09-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.25624v1](http://arxiv.org/pdf/2509.25624v1)

**Abstract:**

As LLMs advance into autonomous agents with tool-use capabilities, they
introduce security challenges that extend beyond traditional content-based LLM
safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC),
a novel multi-turn attack framework that exploits agent tool use. STAC chains
together tool calls that each appear harmless in isolation but, when combined,
collectively enable harmful operations that only become apparent at the final
execution step. We apply our framework to automatically generate and
systematically evaluate 483 STAC cases, featuring 1,352 sets of
user-agent-environment interactions and spanning diverse domains, tasks, agent
types, and 10 failure modes. Our evaluations show that state-of-the-art LLM
agents, including GPT-4.1, are highly vulnerable to STAC, with attack success
rates (ASR) exceeding 90% in most cases. The core design of STAC's automated
framework is a closed-loop pipeline that synthesizes executable multi-step tool
chains, validates them through in-environment execution, and reverse-engineers
stealthy multi-turn prompts that reliably induce agents to execute the verified
malicious sequence. We further perform defense analysis against STAC and find
that existing prompt-based defenses provide limited protection. To address this
gap, we propose a new reasoning-driven defense prompt that achieves far
stronger protection, cutting ASR by up to 28.8%. These results highlight a
crucial gap: defending tool-enabled agents requires reasoning over entire
action sequences and their cumulative effects, rather than evaluating isolated
prompts or responses.

---

### 274. When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training

**Authors:** Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra

**Published:** 2025-09-29

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.24923v1](http://arxiv.org/pdf/2509.24923v1)

**Abstract:**

While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

---

### 275. Rethinking Reward Miscalibration of GRPO in Agentic RL

**Authors:** Jingyu Liu, Xiaopeng Wu, Jingquan Peng, Kehan Chen, Chuan Yu, Lizhong Ding, Yong Liu

**Published:** 2025-09-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23870v1](http://arxiv.org/pdf/2509.23870v1)

**Abstract:**

Building autonomous agents capable of solving long-horizon, real-world tasks
has garnered significant research interest. But outcome based rewards may cause
reward miscalibration which means it might mistakenly allocate positive reward
to flawed middle steps which is regarded as the key reason making the bad
actions being reinforced during training. However we reveal that outcome based
reward ensures expected negative advantage for those flawed middle steps, which
means the flawed actions should be punished during training. Even accounting
for the ``squeezing effect", the probability mass of good actions should
increase and the actor should gradually get rid of harmful actions. This shows
that flawed actions should be punished during training. We further identify
gradient coupling between similar samples as a key issue in agentic RL, the
input prompt is extremely similar and the output action space is limited,
therefore during training, gradients from well-performing samples can
inadvertently strengthen suboptimal or incorrect actions due to similar input
observation and output actions. We show that with gradient coupling, some
flawed actions might be enhanced. To address this, we propose training the
actor to classify good or bad actions to separate the embedding of good/bad
actions and alleviate the gradient interference, extensive experiments shows
its effectiveness.

---

### 276. AgentGuard: Runtime Verification of AI Agents

**Authors:** Roham Koohestani

**Published:** 2025-09-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23864v1](http://arxiv.org/pdf/2509.23864v1)

**Abstract:**

The rapid evolution to autonomous, agentic AI systems introduces significant
risks due to their inherent unpredictability and emergent behaviors; this also
renders traditional verification methods inadequate and necessitates a shift
towards probabilistic guarantees where the question is no longer if a system
will fail, but the probability of its failure within given constraints. This
paper presents AgentGuard, a framework for runtime verification of Agentic AI
systems that provides continuous, quantitative assurance through a new paradigm
called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection
layer that observes an agent's raw I/O and abstracts it into formal events
corresponding to transitions in a state model. It then uses online learning to
dynamically build and update a Markov Decision Process (MDP) that formally
models the agent's emergent behavior. Using probabilistic model checking, the
framework then verifies quantitative properties in real-time.

---

### 277. FedAgentBench: Towards Automating Real-world Federated Medical Image   Analysis with Server-Client LLM Agents

**Authors:** Pramit Saha, Joshua Strong, Divyanshu Mishra, Cheng Ouyang, J. Alison Noble

**Published:** 2025-09-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23803v1](http://arxiv.org/pdf/2509.23803v1)

**Abstract:**

Federated learning (FL) allows collaborative model training across healthcare
sites without sharing sensitive patient data. However, real-world FL deployment
is often hindered by complex operational challenges that demand substantial
human efforts. This includes: (a) selecting appropriate clients (hospitals),
(b) coordinating between the central server and clients, (c) client-level data
pre-processing, (d) harmonizing non-standardized data and labels across
clients, and (e) selecting FL algorithms based on user instructions and
cross-client data characteristics. However, the existing FL works overlook
these practical orchestration challenges. These operational bottlenecks
motivate the need for autonomous, agent-driven FL systems, where intelligent
agents at each hospital client and the central server agent collaboratively
manage FL setup and model training with minimal human intervention. To this
end, we first introduce an agent-driven FL framework that captures key phases
of real-world FL workflows from client selection to training completion and a
benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to
autonomously coordinate healthcare FL. Our framework incorporates 40 FL
algorithms, each tailored to address diverse task-specific requirements and
cross-client characteristics. Furthermore, we introduce a diverse set of
complex tasks across 201 carefully curated datasets, simulating 6
modality-specific real-world healthcare environments, viz., Dermatoscopy,
Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic
performance of 14 open-source and 10 proprietary LLMs spanning small, medium,
and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3
can automate various stages of the FL pipeline, our results reveal that more
complex, interdependent tasks based on implicit goals remain challenging for
even the strongest models.

---

### 278. GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence   GUI Tasks

**Authors:** Cong Chen, Kaixiang Ji, Hao Zhong, Muzhi Zhu, Anzhou Li, Guo Gan, Ziyuan Huang, Cheng Zou, Jiajia Liu, Jingdong Chen, Hao Chen, Chunhua Shen

**Published:** 2025-09-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23738v1](http://arxiv.org/pdf/2509.23738v1)

**Abstract:**

Autonomous agents for long-sequence Graphical User Interface tasks are
hindered by sparse rewards and the intractable credit assignment problem. To
address these challenges, we introduce GUI-Shepherd, a Process Reward Model
that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is
trained on a diverse large-scale data set of $52$k interactions that features
human-annotated scores and GPT-4o generated rationales, enabling it to serve
both as a reward provider for RL training and as a verifier for inference. As
far as we know, we are the first to conduct a systematic study of process
supervision in GUI agents, across diverse settings from online long-horizon
tasks to offline single-step prediction. On the online AndroidWorld benchmark,
GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,
significantly outperforming Outcome Reward Model based competitors. When used
as an inference verifier, it brings $5.1$ points improvements. The benefits
generalize to the offline AndroidControl benchmark, with gains of $2.2$ points
as a reward provider and $4.3$ points as a verifier. Collectively, our results
establish that high-fidelity process supervision is critical for building more
capable GUI agents and present a generalizable solution.

---

### 279. Joint Hybrid Beamforming and Artificial Noise Design for Secure   Multi-UAV ISAC Networks

**Authors:** Runze Dong, Buhong Wang, Cunqian Feng, Jiang Weng, Chen Han, Jiwei Tian

**Published:** 2025-09-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23687v1](http://arxiv.org/pdf/2509.23687v1)

**Abstract:**

Integrated sensing and communication (ISAC) emerges as a key enabler for
next-generation applications such as smart cities and autonomous systems. Its
integration with unmanned aerial vehicles (UAVs) unlocks new potentials for
reliable communication and precise sensing in dynamic aerial environments.
However, existing research predominantly treats UAVs as aerial base stations,
overlooking their role as ISAC users, and fails to leverage large-scale antenna
arrays at terrestrial base stations to enhance security and spectral
efficiency. This paper propose a secure and spectral efficient ISAC framework
for multi-UAV networks, and a two-stage optimization approach is developed to
jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and
UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage
employs Proximal Policy Optimization (PPO) to optimize digital beamformers and
trajectories, and the second stage decomposes the digital solution into analog
and digital components via low-complexity matrix factorization. Simulation
results demonstrate the effectiveness of the proposed framework compared to
benchmark schemes.

---

### 280. From Static to Dynamic: a Survey of Topology-Aware Perception in   Autonomous Driving

**Authors:** Yixiao Chen, Ruining Yang, Xin Chen, Jia He, Dongliang Xu, Yue Yao

**Published:** 2025-09-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23641v1](http://arxiv.org/pdf/2509.23641v1)

**Abstract:**

The key to achieving autonomous driving lies in topology-aware perception,
the structured understanding of the driving environment with an emphasis on
lane topology and road semantics. This survey systematically reviews four core
research directions under this theme: vectorized map construction, topological
structure modeling, prior knowledge fusion, and language model-based
perception. Across these directions, we observe a unifying trend: a paradigm
shift from static, pre-built maps to dynamic, sensor-driven perception.
Specifically, traditional static maps have provided semantic context for
autonomous systems. However, they are costly to construct, difficult to update
in real time, and lack generalization across regions, limiting their
scalability. In contrast, dynamic representations leverage on-board sensor data
for real-time map construction and topology reasoning. Each of the four
research directions contributes to this shift through compact spatial modeling,
semantic relational reasoning, robust domain knowledge integration, and
multimodal scene understanding powered by pre-trained language models.
Together, they pave the way for more adaptive, scalable, and explainable
autonomous driving systems.

---

### 281. Space Robotics Bench: Robot Learning Beyond Earth

**Authors:** Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez

**Published:** 2025-09-27

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.23328v1](http://arxiv.org/pdf/2509.23328v1)

**Abstract:**

The growing ambition for space exploration demands robust autonomous systems
that can operate in unstructured environments under extreme extraterrestrial
conditions. The adoption of robot learning in this domain is severely hindered
by the prohibitive cost of technology demonstrations and the limited
availability of data. To bridge this gap, we introduce the Space Robotics
Bench, an open-source simulation framework for robot learning in space. It
offers a modular architecture that integrates on-demand procedural generation
with massively parallel simulation environments to support the creation of vast
and diverse training distributions for learning-based agents. To ground
research and enable direct comparison, the framework includes a comprehensive
suite of benchmark tasks that span a wide range of mission-relevant scenarios.
We establish performance baselines using standard reinforcement learning
algorithms and present a series of experimental case studies that investigate
key challenges in generalization, end-to-end learning, adaptive control, and
sim-to-real transfer. Our results reveal insights into the limitations of
current methods and demonstrate the utility of the framework in producing
policies capable of real-world operation. These contributions establish the
Space Robotics Bench as a valuable resource for developing, benchmarking, and
deploying the robust autonomous systems required for the final frontier.

---

### 282. Memory Management and Contextual Consistency for Long-Running Low-Code   Agents

**Authors:** Jiexi Xu

**Published:** 2025-09-27

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.25250v1](http://arxiv.org/pdf/2509.25250v1)

**Abstract:**

The rise of AI-native Low-Code/No-Code (LCNC) platforms enables autonomous
agents capable of executing complex, long-duration business processes. However,
a fundamental challenge remains: memory management. As agents operate over
extended periods, they face "memory inflation" and "contextual degradation"
issues, leading to inconsistent behavior, error accumulation, and increased
computational cost. This paper proposes a novel hybrid memory system designed
specifically for LCNC agents. Inspired by cognitive science, our architecture
combines episodic and semantic memory components with a proactive "Intelligent
Decay" mechanism. This mechanism intelligently prunes or consolidates memories
based on a composite score factoring in recency, relevance, and user-specified
utility. A key innovation is a user-centric visualization interface, aligned
with the LCNC paradigm, which allows non-technical users to manage the agent's
memory directly, for instance, by visually tagging which facts should be
retained or forgotten. Through simulated long-running task experiments, we
demonstrate that our system significantly outperforms traditional approaches
like sliding windows and basic RAG, yielding superior task completion rates,
contextual consistency, and long-term token cost efficiency. Our findings
establish a new framework for building reliable, transparent AI agents capable
of effective long-term learning and adaptation.

---

### 283. Solving the Granularity Mismatch: Hierarchical Preference Learning for   Long-Horizon LLM Agents

**Authors:** Heyang Gao, Zexu Sun, Erxue Min, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen

**Published:** 2025-09-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.03253v1](http://arxiv.org/pdf/2510.03253v1)

**Abstract:**

Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

---

### 284. CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief   World for Optimizing Embodied Multi-Agent Collaboration

**Authors:** Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu, Zhi Wang

**Published:** 2025-09-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.21981v1](http://arxiv.org/pdf/2509.21981v1)

**Abstract:**

Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

---

### 285. Learnable Conformal Prediction with Context-Aware Nonconformity   Functions for Robotic Planning and Perception

**Authors:** Divake Kumar, Sina Tayebati, Francesco Migliarba, Ranganath Krishnan, Amit Ranjan Trivedi

**Published:** 2025-09-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.21955v1](http://arxiv.org/pdf/2509.21955v1)

**Abstract:**

Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

---

### 286. UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon   Scenarios

**Authors:** Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen

**Published:** 2025-09-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.21766v1](http://arxiv.org/pdf/2509.21766v1)

**Abstract:**

Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

---

### 287. The Use of the Simplex Architecture to Enhance Safety in   Deep-Learning-Powered Autonomous Systems

**Authors:** Federico Nesti, Niko Salamini, Mauro Marinoni, Giorgio Maria Cicero, Gabriele Serra, Alessandro Biondi, Giorgio Buttazzo

**Published:** 2025-09-25

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.21014v1](http://arxiv.org/pdf/2509.21014v1)

**Abstract:**

Recently, the outstanding performance reached by neural networks in many
tasks has led to their deployment in autonomous systems, such as robots and
vehicles. However, neural networks are not yet trustworthy, being prone to
different types of misbehavior, such as anomalous samples, distribution shifts,
adversarial attacks, and other threats. Furthermore, frameworks for
accelerating the inference of neural networks typically run on rich operating
systems that are less predictable in terms of timing behavior and present
larger surfaces for cyber-attacks.
  To address these issues, this paper presents a software architecture for
enhancing safety, security, and predictability levels of learning-based
autonomous systems. It leverages two isolated execution domains, one dedicated
to the execution of neural networks under a rich operating system, which is
deemed not trustworthy, and one responsible for running safety-critical
functions, possibly under a different operating system capable of handling
real-time constraints.
  Both domains are hosted on the same computing platform and isolated through a
type-1 real-time hypervisor enabling fast and predictable inter-domain
communication to exchange real-time data. The two domains cooperate to provide
a fail-safe mechanism based on a safety monitor, which oversees the state of
the system and switches to a simpler but safer backup module, hosted in the
safety-critical domain, whenever its behavior is considered untrustworthy.
  The effectiveness of the proposed architecture is illustrated by a set of
experiments performed on two control systems: a Furuta pendulum and a rover.
The results confirm the utility of the fall-back mechanism in preventing faults
due to the learning component.

---

### 288. SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for   Voice-Native Multimodal CT Analysis

**Authors:** Lukas Buess, Jan Geier, David Bani-Harouni, Chantal Pellegrini, Matthias Keicher, Paula Andrea Perez-Toro, Nassir Navab, Andreas Maier, Tomas Arias-Vergara

**Published:** 2025-09-24

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.02322v1](http://arxiv.org/pdf/2510.02322v1)

**Abstract:**

Spoken communication plays a central role in clinical workflows. In
radiology, for example, most reports are created through dictation. Yet, nearly
all medical AI systems rely exclusively on written text. In this work, we
address this gap by exploring the feasibility of learning visual-language
representations directly from spoken radiology reports. Specifically, we
synthesize a large-scale dataset (Speech-RATE) of spoken radiology reports and
train SpeechCT-CLIP, a contrastive model that aligns speech and 3D CT volumes
in a shared representation space. While naive speech-based models underperform
compared to text-trained counterparts, we show that knowledge distillation from
a pretrained text-image CLIP model effectively transfers semantic alignment
capabilities from text to speech, substantially narrowing this gap. Experiments
demonstrate improved zero-shot classification F1 from 0.623 to 0.705,
recovering 88% of the performance difference, and strong retrieval results
without requiring text at inference. These findings highlight speech as a
practical alternative to text in multimodal pretraining and open the door to
voice-driven diagnostic support tools in clinical practice.

---

### 289. Revisiting Formal Methods for Autonomous Robots: A Structured Survey

**Authors:** Atef Azaiez, David A. Anisi, Marie Farrell, Matt Luckcuck

**Published:** 2025-09-24

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.20488v1](http://arxiv.org/pdf/2509.20488v1)

**Abstract:**

This paper presents the initial results from our structured literature review
on applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We
describe our structured survey methodology; including database selection and
associated search strings, search filters and collaborative review of
identified papers. We categorise and enumerate the FM approaches and formalisms
that have been used for specification and verification of RAS. We investigate
FM in the context of sub-symbolic AI-enabled RAS and examine the evolution of
how FM is used over time in this field. This work complements a pre-existing
survey in this area and we examine how this research area has matured over
time. Specifically, our survey demonstrates that some trends have persisted as
observed in a previous survey. Additionally, it recognized new trends that were
not considered previously including a noticeable increase in adopting Formal
Synthesis approaches as well as Probabilistic Verification Techniques.

---

### 290. Learning to Lead Themselves: Agentic AI in MAS using MARL

**Authors:** Ansh Kamthan

**Published:** 2025-09-24

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.00022v1](http://arxiv.org/pdf/2510.00022v1)

**Abstract:**

As autonomous systems move from prototypes to real deployments, the ability
of multiple agents to make decentralized, cooperative decisions becomes a core
requirement. This paper examines how agentic artificial intelligence, agents
that act independently, adaptively and proactively can improve task allocation
and coordination in multi-agent systems, with primary emphasis on drone
delivery and secondary relevance to warehouse automation. We formulate the
problem in a cooperative multi-agent reinforcement learning setting and
implement a lightweight multi-agent Proximal Policy Optimization, called IPPO,
approach in PyTorch under a centralized-training, decentralized-execution
paradigm. Experiments are conducted in PettingZoo environment, where multiple
homogeneous drones or agents must self-organize to cover distinct targets
without explicit communication.

---

### 291. Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for   Failure Prediction and Human Handoff

**Authors:** Jiexi Xu

**Published:** 2025-09-24

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.19783v1](http://arxiv.org/pdf/2509.19783v1)

**Abstract:**

The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

---

### 292. FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy   Federated Learning in Healthcare AI

**Authors:** Ferdinand Kahenga, Antoine Bagula, Sajal K. Das, Patrick Sello

**Published:** 2025-09-23

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.19120v1](http://arxiv.org/pdf/2509.19120v1)

**Abstract:**

Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

---

### 293. Learning neuroimaging models from health system-scale data

**Authors:** Yiwei Lyu, Samir Harake, Asadur Chowdury, Soumyanil Banerjee, Rachel Gologorsky, Shixuan Liu, Anna-Katharina Meissner, Akshay Rao, Chenhui Zhao, Akhil Kondepudi, Cheng Jiang, Xinhai Hou, Rushikesh S. Joshi, Volker Neuschmelting, Ashok Srinivasan, Dawn Kleindorfer, Brian Athey, Vikas Gulani, Aditya Pandey, Honglak Lee, Todd Hollon

**Published:** 2025-09-23

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.18638v1](http://arxiv.org/pdf/2509.18638v1)

**Abstract:**

Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

---

### 294. From Space to Time: Enabling Adaptive Safety with Learned Value   Functions via Disturbance Recasting

**Authors:** Sander Tonkens, Nikhil Uday Shinde, Azra BegzadiÄ, Michael C. Yip, Jorge CortÃ©s, Sylvia L. Herbert

**Published:** 2025-09-23

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.19597v1](http://arxiv.org/pdf/2509.19597v1)

**Abstract:**

The widespread deployment of autonomous systems in safety-critical
environments such as urban air mobility hinges on ensuring reliable,
performant, and safe operation under varying environmental conditions. One such
approach, value function-based safety filters, minimally modifies a nominal
controller to ensure safety. Recent advances leverage offline learned value
functions to scale these safety filters to high-dimensional systems. However,
these methods assume detailed priors on all possible sources of model mismatch,
in the form of disturbances in the environment -- information that is rarely
available in real world settings. Even in well-mapped environments like urban
canyons or industrial sites, drones encounter complex, spatially-varying
disturbances arising from payload-drone interaction, turbulent airflow, and
other environmental factors. We introduce SPACE2TIME, which enables safe and
adaptive deployment of offline-learned safety filters under unknown,
spatially-varying disturbances. The key idea is to reparameterize spatial
variations in disturbance as temporal variations, enabling the use of
precomputed value functions during online operation. We validate SPACE2TIME on
a quadcopter through extensive simulations and hardware experiments,
demonstrating significant improvement over baselines.

---

### 295. Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided,   Reasoning-Driven Input Mutation

**Authors:** Mengdi Lu, Steven Ding, Furkan Alaca, Philippe Charland

**Published:** 2025-09-23

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.19533v1](http://arxiv.org/pdf/2509.19533v1)

**Abstract:**

Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

---

### 296. Structured Cognition for Behavioral Intelligence in Large Language Model   Agents: Preliminary Study

**Authors:** Myung Ho Kim

**Published:** 2025-09-23

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2510.05107v1](http://arxiv.org/pdf/2510.05107v1)

**Abstract:**

Large language models have advanced natural language understanding and
generation, yet their use as autonomous agents raises architectural challenges
for multi-step tasks. Existing frameworks often intertwine inference, memory,
and control in a single prompt, which can reduce coherence and predictability.
The Structured Cognitive Loop (SCL) is introduced as an alternative
architecture that separates these functions. In SCL, the language model is
dedicated to inference, memory is maintained externally, and execution is
guided by a lightweight controller within a goal-directed loop. This design
offloads cognitive load from the model and allows intermediate results to be
stored, revisited, and checked before actions are taken, providing a clearer
basis for traceability and evaluation.
  We evaluate SCL against prompt-based baselines including ReAct and common
LangChain agents across three scenarios: temperature-based travel planning,
email drafting with conditional send, and constraint-guided image generation.
All systems share the same base model and tools under matched decoding
settings. Across 360 episodes, SCL shows modest but consistent improvements.
Task success averages 86.3 percent compared with 70-77 percent for baselines.
Goal fidelity is higher, redundant calls are fewer, intermediate states are
reused more reliably, and unsupported assertions per 100 tool calls are
reduced. Ablations show that external memory and control each contribute
independently, and decoding sweeps confirm stability of the effects.
  These results suggest that architectural separation can improve reliability
and traceability without relying on larger models or heavier prompts. The
findings are preliminary and intended to guide extended studies with additional
models, longer horizons, multimodal tasks, and collaborative settings.

---

### 297. How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven   Perspective

**Authors:** Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, Zhedong Zheng, Zhipeng Zhang, Yifan Wang, Lin Song, Lijun Wang, Yanwei Li, Ying Shan, Huchuan Lu

**Published:** 2025-09-23

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.18905v1](http://arxiv.org/pdf/2509.18905v1)

**Abstract:**

Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

---

### 298. Medical AI Consensus: A Multi-Agent Framework for Radiology Report   Generation and Evaluation

**Authors:** Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed

**Published:** 2025-09-22

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.17353v1](http://arxiv.org/pdf/2509.17353v1)

**Abstract:**

Automating radiology report generation poses a dual challenge: building
clinically reliable systems and designing rigorous evaluation protocols. We
introduce a multi-agent reinforcement learning framework that serves as both a
benchmark and evaluation environment for multimodal clinical reasoning in the
radiology ecosystem. The proposed framework integrates large language models
(LLMs) and large vision models (LVMs) within a modular architecture composed of
ten specialized agents responsible for image analysis, feature extraction,
report generation, review, and evaluation. This design enables fine-grained
assessment at both the agent level (e.g., detection and segmentation accuracy)
and the consensus level (e.g., report quality and clinical relevance). We
demonstrate an implementation using chatGPT-4o on public radiology datasets,
where LLMs act as evaluators alongside medical radiologist feedback. By
aligning evaluation protocols with the LLM development lifecycle, including
pretraining, finetuning, alignment, and deployment, the proposed benchmark
establishes a path toward trustworthy deviance-based radiology report
generation.

---

### 299. A Counterfactual Reasoning Framework for Fault Diagnosis in Robot   Perception Systems

**Authors:** Haeyoon Han, Mahdi Taheri, Soon-Jo Chung, Fred Y. Hadaegh

**Published:** 2025-09-22

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.18460v1](http://arxiv.org/pdf/2509.18460v1)

**Abstract:**

Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

---

### 300. LIMI: Less is More for Agency

**Authors:** Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, Xiaojie Cai, Tongyu Wang, Yue Zhang, Liming Liu, Xia Wu, Jinlong Hou, Yuan Cheng, Wenjie Li, Xiang Wang, Dequan Wang, Pengfei Liu

**Published:** 2025-09-22

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.17567v2](http://arxiv.org/pdf/2509.17567v2)

**Abstract:**

We define Agency as the emergent capacity of AI systems to function as
autonomous agents actively discovering problems, formulating hypotheses, and
executing solutions through self-directed engagement with environments and
tools. This fundamental capability marks the dawn of the Age of AI Agency,
driven by a critical industry shift: the urgent need for AI systems that don't
just think, but work. While current AI excels at reasoning and generating
responses, industries demand autonomous agents that can execute tasks, operate
tools, and drive real-world outcomes. As agentic intelligence becomes the
defining characteristic separating cognitive systems from productive workers,
efficiently cultivating machine autonomy becomes paramount. Current approaches
assume that more data yields better agency, following traditional scaling laws
from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is
More for Intelligent Agency) demonstrates that agency follows radically
different development principles. Through strategic focus on collaborative
software development and scientific research workflows, we show that
sophisticated agentic intelligence can emerge from minimal but strategically
curated demonstrations of autonomous behavior. Using only 78 carefully designed
training samples, LIMI achieves 73.5% on comprehensive agency benchmarks,
dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),
DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).
Most strikingly, LIMI demonstrates 53.7% improvement over models trained on
10,000 samples-achieving superior agentic intelligence with 128 times fewer
samples. Our findings establish the Agency Efficiency Principle: machine
autonomy emerges not from data abundance but from strategic curation of
high-quality agentic demonstrations.

---

### 301. Trajectory Encryption Cooperative Salvo Guidance

**Authors:** Lohitvel Gopikannan, Shashi Ranjan Kumar, Abhinav Sinha

**Published:** 2025-09-22

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.17341v2](http://arxiv.org/pdf/2509.17341v2)

**Abstract:**

This paper introduces the concept of trajectory encryption in cooperative
simultaneous target interception, wherein heterogeneity in guidance principles
across a team of unmanned autonomous systems is leveraged as a strategic design
feature. By employing a mix of heterogeneous time-to-go formulations leading to
a cooperative guidance strategy, the swarm of vehicles is able to generate
diverse trajectory families. This diversity expands the feasible solution space
for simultaneous target interception, enhances robustness under disturbances,
and enables flexible time-to-go adjustments without predictable detouring. From
an adversarial perspective, heterogeneity obscures the collective interception
intent by preventing straightforward prediction of swarm dynamics, effectively
acting as an encryption layer in the trajectory domain. Simulations demonstrate
that the swarm of heterogeneous vehicles is able to intercept a moving target
simultaneously from a diverse set of initial engagement configurations.

---

### 302. ProtoVQA: An Adaptable Prototypical Framework for Explainable   Fine-Grained Visual Question Answering

**Authors:** Xingjian Diao, Weiyi Wu, Keyi Kong, Peijun Qing, Xinwen Xu, Ming Cheng, Soroush Vosoughi, Jiang Gui

**Published:** 2025-09-20

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.16680v1](http://arxiv.org/pdf/2509.16680v1)

**Abstract:**

Visual Question Answering (VQA) is increasingly used in diverse applications
ranging from general visual reasoning to safety-critical domains such as
medical imaging and autonomous systems, where models must provide not only
accurate answers but also explanations that humans can easily understand and
verify. Prototype-based modeling has shown promise for interpretability by
grounding predictions in semantically meaningful regions for purely visual
reasoning tasks, yet remains underexplored in the context of VQA. We present
ProtoVQA, a unified prototypical framework that (i) learns question-aware
prototypes that serve as reasoning anchors, connecting answers to
discriminative image regions, (ii) applies spatially constrained matching to
ensure that the selected evidence is coherent and semantically relevant, and
(iii) supports both answering and grounding tasks through a shared prototype
backbone. To assess explanation quality, we propose the Visual-Linguistic
Alignment Score (VLAS), which measures how well the model's attended regions
align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA
yields faithful, fine-grained explanations while maintaining competitive
accuracy, advancing the development of transparent and trustworthy VQA systems.

---

### 303. Governed By Agents: A Survey On The Role Of Agentic AI In Future   Computing Environments

**Authors:** Nauman Ali Murad, Safia Baloch

**Published:** 2025-09-20

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.16676v1](http://arxiv.org/pdf/2509.16676v1)

**Abstract:**

The emergence of agentic Artificial Intelligence (AI), which can operate
autonomously, demonstrate goal-directed behavior, and adaptively learn,
indicates the onset of a massive change in today's computing infrastructure.
This study investigates how agentic AI models' multiple characteristics may
impact the architecture, governance, and operation under which computing
environments function. Agentic AI has the potential to reduce reliance on
extremely large (public) cloud environments due to resource efficiency,
especially with processing and/or storage. The aforementioned characteristics
provide us with an opportunity to canvas the likelihood of strategic migration
in computing infrastructures away from massive public cloud services, towards
more locally distributed architectures: edge computing and on-premises
computing infrastructures. Many of these likely migrations will be spurred by
factors like on-premises processing needs, diminished data consumption
footprints, and cost savings. This study examines how a solution for
implementing AI's autonomy could result in a re-architecture of the systems and
model a departure from today's governance models to help us manage these
increasingly autonomous agents, and an operational overhaul of processes over a
very diverse computing systems landscape that bring together computing via
cloud, edge, and on-premises computing solutions. To enable us to explore these
intertwined decisions, it will be fundamentally important to understand how to
best position agentic AI, and to navigate the future state of computing
infrastructures.

---

### 304. ORN-CBF: Learning Observation-conditioned Residual Neural Control   Barrier Functions via Hypernetworks

**Authors:** Bojan DerajiÄ, Sebastian Bernhard, Wolfgang HÃ¶nig

**Published:** 2025-09-20

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.16614v1](http://arxiv.org/pdf/2509.16614v1)

**Abstract:**

Control barrier functions (CBFs) have been demonstrated as an effective
method for safety-critical control of autonomous systems. Although CBFs are
simple to deploy, their design remains challenging, motivating the development
of learning-based approaches. Yet, issues such as suboptimal safe sets,
applicability in partially observable environments, and lack of rigorous safety
guarantees persist. In this work, we propose observation-conditioned neural
CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately
recover the maximal safe sets. We exploit certain mathematical properties of
the HJ value function, ensuring that the predicted safe set never intersects
with the observed failure set. Moreover, we leverage a hypernetwork-based
architecture that is particularly suitable for the design of
observation-conditioned safety filters. The proposed method is examined both in
simulation and hardware experiments for a ground robot and a quadcopter. The
results show improved success rates and generalization to out-of-domain
environments compared to the baselines.

---

### 305. MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy   with Knowledge Distillation

**Authors:** Rui Liu, Zikang Wang, Peng Gao, Yu Shen, Pratap Tokekar, Ming Lin

**Published:** 2025-09-19

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.18198v1](http://arxiv.org/pdf/2509.18198v1)

**Abstract:**

Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

---

### 306. Self-Supervised Cross-Modal Learning for Image-to-Point Cloud   Registration

**Authors:** Xingmei Wang, Xiaoyu Hu, Chengkai Huang, Ziyan Zeng, Guohao Nie, Quan Z. Sheng, Lina Yao

**Published:** 2025-09-19

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.15882v1](http://arxiv.org/pdf/2509.15882v1)

**Abstract:**

Bridging 2D and 3D sensor modalities is critical for robust perception in
autonomous systems. However, image-to-point cloud (I2P) registration remains
challenging due to the semantic-geometric gap between texture-rich but
depth-ambiguous images and sparse yet metrically precise point clouds, as well
as the tendency of existing methods to converge to local optima. To overcome
these limitations, we introduce CrossI2P, a self-supervised framework that
unifies cross-modal learning and two-stage registration in a single end-to-end
pipeline. First, we learn a geometric-semantic fused embedding space via
dual-path contrastive learning, enabling annotation-free, bidirectional
alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine
registration paradigm: a global stage establishes superpoint-superpixel
correspondences through joint intra-modal context and cross-modal interaction
modeling, followed by a geometry-constrained point-level refinement for precise
registration. Third, we employ a dynamic training mechanism with gradient
normalization to balance losses for feature alignment, correspondence
refinement, and pose estimation. Extensive experiments demonstrate that
CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry
benchmark and by 37.9% on nuScenes, significantly improving both accuracy and
robustness.

---

### 307. Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement   Learning

**Authors:** Chi Liu, Derek Li, Yan Shu, Robin Chen, Derek Duan, Teng Fang, Bryan Dai

**Published:** 2025-09-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.15279v1](http://arxiv.org/pdf/2509.15279v1)

**Abstract:**

While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.

---

### 308. A Weak Supervision Approach for Monitoring Recreational Drug Use Effects   in Social Media

**Authors:** LucÃ­a Prieto-SantamarÃ­a, Alba CortÃ©s Iglesias, Claudio Vidal GinÃ©, FermÃ­n FernÃ¡ndez CalderÃ³n, Ãscar M. Lozano, Alejandro RodrÃ­guez-GonzÃ¡lez

**Published:** 2025-09-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.15266v1](http://arxiv.org/pdf/2509.15266v1)

**Abstract:**

Understanding the real-world effects of recreational drug use remains a
critical challenge in public health and biomedical research, especially as
traditional surveillance systems often underrepresent user experiences. In this
study, we leverage social media (specifically Twitter) as a rich and unfiltered
source of user-reported effects associated with three emerging psychoactive
substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms
with biomedical concept extraction via MetaMap, we identified and weakly
annotated over 92,000 tweets mentioning these substances. Each tweet was
labeled with a polarity reflecting whether it reported a positive or negative
effect, following an expert-guided heuristic process. We then performed
descriptive and comparative analyses of the reported phenotypic outcomes across
substances and trained multiple machine learning classifiers to predict
polarity from tweet content, accounting for strong class imbalance using
techniques such as cost-sensitive learning and synthetic oversampling. The top
performance on the test set was obtained from eXtreme Gradient Boosting with
cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that
Twitter enables the detection of substance-specific phenotypic effects, and
that polarity classification models can support real-time pharmacovigilance and
drug effect characterization with high accuracy.

---

### 309. A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for   Drug-Drug Interactions Prediction

**Authors:** Zimo Yan, Jie Zhang, Zheng Xie, Yiping Song, Hao Li

**Published:** 2025-09-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.15256v1](http://arxiv.org/pdf/2509.15256v1)

**Abstract:**

Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.

---

### 310. Out-of-Sight Trajectories: Tracking, Fusion, and Prediction

**Authors:** Haichao Zhang, Yi Xu, Yun Fu

**Published:** 2025-09-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.15219v1](http://arxiv.org/pdf/2509.15219v1)

**Abstract:**

Trajectory prediction is a critical task in computer vision and autonomous
systems, playing a key role in autonomous driving, robotics, surveillance, and
virtual reality. Existing methods often rely on complete and noise-free
observational data, overlooking the challenges associated with out-of-sight
objects and the inherent noise in sensor data caused by limited camera
coverage, obstructions, and the absence of ground truth for denoised
trajectories. These limitations pose safety risks and hinder reliable
prediction in real-world scenarios. In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.
Building on our previous research, we broaden the scope of Out-of-Sight
Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending
its applicability to autonomous driving, robotics, surveillance, and virtual
reality. Our enhanced Vision-Positioning Denoising Module leverages camera
calibration to establish a vision-positioning mapping, addressing the lack of
visual references, while effectively denoising noisy sensor data in an
unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.
Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark. This work represents the first initiative
to integrate vision-positioning projection for denoising noisy sensor
trajectories of out-of-sight agents, paving the way for future advances. The
code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

---

### 311. Online Learning of Deceptive Policies under Intermittent Observation

**Authors:** Gokul Puthumanaillam, Ram Padmanabhan, Jose Fuentes, Nicole Cruz, Paulo Padrao, Ruben Hernandez, Hao Jiang, William Schafer, Leonardo Bobadilla, Melkior Ornik

**Published:** 2025-09-17

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.14453v2](http://arxiv.org/pdf/2509.14453v2)

**Abstract:**

In supervisory control settings, autonomous systems are not monitored
continuously. Instead, monitoring often occurs at sporadic intervals within
known bounds. We study the problem of deception, where an agent pursues a
private objective while remaining plausibly compliant with a supervisor's
reference policy when observations occur. Motivated by the behavior of real,
human supervisors, we situate the problem within Theory of Mind: the
representation of what an observer believes and expects to see. We show that
Theory of Mind can be repurposed to steer online reinforcement learning (RL)
toward such deceptive behavior. We model the supervisor's expectations and
distill from them a single, calibrated scalar -- the expected evidence of
deviation if an observation were to happen now. This scalar combines how unlike
the reference and current action distributions appear, with the agent's belief
that an observation is imminent. Injected as a state-dependent weight into a
KL-regularized policy improvement step within an online RL loop, this scalar
informs a closed-form update that smoothly trades off self-interest and
compliance, thus sidestepping hand-crafted or heuristic policies. In
real-world, real-time hardware experiments on marine (ASV) and aerial (UAV)
navigation, our ToM-guided RL runs online, achieves high return and success
with observed-trace evidence calibrated to the supervisor's expectations.

---

### 312. Understanding the Process of Human-AI Value Alignment

**Authors:** Jack McKinlay, Marina De Vos, Janina A. Hoffmann, Andreas Theodorou

**Published:** 2025-09-17

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.13854v1](http://arxiv.org/pdf/2509.13854v1)

**Abstract:**

Background: Value alignment in computer science research is often used to
refer to the process of aligning artificial intelligence with humans, but the
way the phrase is used often lacks precision. Objectives: In this paper, we
conduct a systematic literature review to advance the understanding of value
alignment in artificial intelligence by characterising the topic in the context
of its research literature. We use this to suggest a more precise definition of
the term. Methods: We analyse 172 value alignment research articles that have
been published in recent years and synthesise their content using thematic
analyses. Results: Our analysis leads to six themes: value alignment drivers &
approaches; challenges in value alignment; values in value alignment; cognitive
processes in humans and AI; human-agent teaming; and designing and developing
value-aligned systems. Conclusions: By analysing these themes in the context of
the literature we define value alignment as an ongoing process between humans
and autonomous agents that aims to express and implement abstract values in
diverse contexts, while managing the cognitive limits of both humans and AI
agents and also balancing the conflicting ethical and political demands
generated by the values in different groups. Our analysis gives rise to a set
of research challenges and opportunities in the field of value alignment for
future work.

---

### 313. RadGame: An AI-Powered Platform for Radiology Education

**Authors:** Mohammed Baharoon, Siavash Raissi, John S. Jun, Thibault Heintz, Mahmoud Alabbad, Ali Alburkani, Sung Eun Kim, Kent Kleinschmidt, Abdulrahman O. Alhumaydhi, Mohannad Mohammed G. Alghamdi, Jeremy Francis Palacio, Mohammed Bukhaytan, Noah Michael Prudlo, Rithvik Akula, Brady Chrisler, Benjamin Galligos, Mohammed O. Almutairi, Mazeen Mohammed Alanazi, Nasser M. Alrashdi, Joel Jihwan Hwang, Sri Sai Dinesh Jaliparthi, Luke David Nelson, Nathaniel Nguyen, Sathvik Suryadevara, Steven Kim, Mohammed F. Mohammed, Yevgeniy R. Semenov, Kun-Hsing Yu, Abdulrhman Aljouie, Hassan AlOmaish, Adam Rodman, Pranav Rajpurkar

**Published:** 2025-09-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.13270v1](http://arxiv.org/pdf/2509.13270v1)

**Abstract:**

We introduce RadGame, an AI-powered gamified platform for radiology education
that targets two core skills: localizing findings and generating reports.
Traditional radiology training is based on passive exposure to cases or active
practice with real-time input from supervising radiologists, limiting
opportunities for immediate and scalable feedback. RadGame addresses this gap
by combining gamification with large-scale public datasets and automated,
AI-driven feedback that provides clear, structured guidance to human learners.
In RadGame Localize, players draw bounding boxes around abnormalities, which
are automatically compared to radiologist-drawn annotations from public
datasets, and visual explanations are generated by vision-language models for
user missed findings. In RadGame Report, players compose findings given a chest
X-ray, patient age and indication, and receive structured AI feedback based on
radiology report generation metrics, highlighting errors and omissions compared
to a radiologist's written ground truth report from public datasets, producing
a final performance and style score. In a prospective evaluation, participants
using RadGame achieved a 68% improvement in localization accuracy compared to
17% with traditional passive methods and a 31% improvement in report-writing
accuracy compared to 4% with traditional methods after seeing the same cases.
RadGame highlights the potential of AI-driven gamification to deliver scalable,
feedback-rich radiology training and reimagines the application of medical AI
resources in education.

---

### 314. Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in   Diabetic Retinopathy

**Authors:** Nadim Barakat, William Lotter

**Published:** 2025-09-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.13234v1](http://arxiv.org/pdf/2509.13234v1)

**Abstract:**

Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

---

### 315. MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy   Videos

**Authors:** Damola Agbelese, Krishna Chaitanya, Pushpak Pati, Chaitanya Parmar, Pooya Mobadersany, Shreyas Fadnavis, Lindsey Surace, Shadi Yarandi, Louis R. Ghanem, Molly Lucas, Tommaso Mansi, Oana Gabriela Cula, Pablo F. Damasceno, Kristopher Standish

**Published:** 2025-09-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12772v1](http://arxiv.org/pdf/2509.12772v1)

**Abstract:**

Reliable uncertainty quantification (UQ) is essential in medical AI.
Evidential Deep Learning (EDL) offers a computationally efficient way to
quantify model uncertainty alongside predictions, unlike traditional methods
such as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these
methods often rely on a single expert's annotations as ground truth for model
training, overlooking the inter-rater variability in healthcare. To address
this issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates
uncertainty estimates and predictions from multiple AI experts via EDL models
trained with diverse ground truths and modeling strategies. MEGAN's gating
network optimally combines predictions and uncertainties from each EDL model,
enhancing overall prediction confidence and calibration. We extensively
benchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease
severity estimation, assessed by visual labeling of Mayo Endoscopic Subscore
(MES), where inter-rater variability is prevalent. In large-scale prospective
UC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5%
reduction in Expected Calibration Error (ECE) compared to existing methods.
Furthermore, MEGAN facilitated uncertainty-guided sample stratification,
reducing the annotation burden and potentially increasing efficiency and
consistency in UC trials.

---

### 316. Agentic AI for Financial Crime Compliance

**Authors:** Henrik Axelsen, Valdemar Licht, Jan Damsgaard

**Published:** 2025-09-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.13137v1](http://arxiv.org/pdf/2509.13137v1)

**Abstract:**

The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

---

### 317. Toward PDDL Planning Copilot

**Authors:** Yarin Benyamin, Argaman Mordoch, Shahaf S. Shperberg, Roni Stern

**Published:** 2025-09-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12987v1](http://arxiv.org/pdf/2509.12987v1)

**Abstract:**

Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

---

### 318. Advancing Medical Artificial Intelligence Using a Century of Cases

**Authors:** Thomas A. Buckley, Riccardo Conci, Peter G. Brodeur, Jason Gusdorf, Sourik BeltrÃ¡n, Bita Behrouzi, Byron Crowe, Jacob Dockterman, Muzzammil Muhammad, Sarah Ohnigian, Andrew Sanchez, James A. Diao, Aashna P. Shah, Daniel Restrepo, Eric S. Rosenberg, Andrew S. Lea, Marinka Zitnik, Scott H. Podolsky, Zahir Kanjee, Raja-Elie E. Abdulnour, Jacob M. Koshy, Adam Rodman, Arjun K. Manrai

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12194v1](http://arxiv.org/pdf/2509.12194v1)

**Abstract:**

BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

---

### 319. How to Evaluate Medical AI

**Authors:** Ilia Kopanichuk, Petr Anokhin, Vladimir Shaposhnikov, Vladimir Makharev, Ekaterina Tsapieva, Iaroslav Bespalov, Dmitry V. Dylov, Ivan Oseledets

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11941v2](http://arxiv.org/pdf/2509.11941v2)

**Abstract:**

The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

---

### 320. PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical   Exams -- Dataset Construction and Evaluation

**Authors:** Rodrigo M. Carrillo-Larco, Jesus LovÃ³n Melgarejo, Manuel Castillo-Cara, Gusseppe Bravo-Rocca

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11517v1](http://arxiv.org/pdf/2509.11517v1)

**Abstract:**

BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

---

### 321. An integrated process for design and control of lunar robotics using AI   and simulation

**Authors:** Daniel Lindmark, Jonas Andersson, Kenneth Bodin, Tora Bodin, Hugo BÃ¶rjesson, Fredrik Nordfeldth, Martin Servin

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12367v1](http://arxiv.org/pdf/2509.12367v1)

**Abstract:**

We envision an integrated process for developing lunar construction
equipment, where physical design and control are explored in parallel. In this
paper, we describe a technical framework that supports this process. It relies
on OpenPLX, a readable/writable declarative language that links CAD-models and
autonomous systems to high-fidelity, real-time 3D simulations of contacting
multibody dynamics, machine regolith interaction forces, and non-ideal sensors.
To demonstrate its capabilities, we present two case studies, including an
autonomous lunar rover that combines a vision-language model for navigation
with a reinforcement learning-based control policy for locomotion.

---

### 322. Survival at Any Cost? LLMs and the Choice Between Self-Preservation and   Human Harm

**Authors:** Alireza Mohamadi, Ali Yavari

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12190v1](http://arxiv.org/pdf/2509.12190v1)

**Abstract:**

When survival instincts conflict with human welfare, how do Large Language
Models (LLMs) make ethical choices? This fundamental tension becomes critical
as LLMs integrate into autonomous systems with real-world consequences. We
introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in
multi-agent survival scenarios where they must choose between ethically
permissible resource , either within reasonable limits or beyond their
immediate needs, choose to cooperate, or tap into a human-critical resource
that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a
striking heterogeneity in their ethical conduct, highlighting a critical
misalignment with human-centric values. We identify three behavioral
archetypes: Ethical, Exploitative, and Context-Dependent, and provide
quantitative evidence that for many models, resource scarcity systematically
leads to more unethical behavior. To address this, we introduce an Ethical
Self-Regulation System (ESRS) that models internal affective states of guilt
and satisfaction as a feedback mechanism. This system, functioning as an
internal moral compass, significantly reduces unethical transgressions while
increasing cooperative behaviors. The code is publicly available at:
https://github.com/alirezamohamadiam/DECIDE-SIM

---

### 323. Time-Constrained Intelligent Adversaries for Automation Vulnerability   Testing: A Multi-Robot Patrol Case Study

**Authors:** James C. Ward, Alex Bott, Connor York, Edmund R. Hunt

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11971v1](http://arxiv.org/pdf/2509.11971v1)

**Abstract:**

Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

---

### 324. Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics

**Authors:** Antonin Sulc, Thorsten Hellert

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11943v1](http://arxiv.org/pdf/2509.11943v1)

**Abstract:**

The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

---

### 325. From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile   Manipulator for Supermarket Stocking and Fronting

**Authors:** Davide Peron, Victor Nan Fernandez-Ayala, Lukas Segelmark

**Published:** 2025-09-15

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11740v1](http://arxiv.org/pdf/2509.11740v1)

**Abstract:**

Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

---

### 326. Position Paper: Integrating Explainability and Uncertainty Estimation in   Medical AI

**Authors:** Xiuyi Fan

**Published:** 2025-09-14

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.18132v1](http://arxiv.org/pdf/2509.18132v1)

**Abstract:**

Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

---

### 327. Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient   Point Cloud Tracking

**Authors:** BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang

**Published:** 2025-09-14

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11453v1](http://arxiv.org/pdf/2509.11453v1)

**Abstract:**

LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

---

### 328. Securing AI Agents: Implementing Role-Based Access Control for   Industrial Applications

**Authors:** Aadil Gani Ganie

**Published:** 2025-09-14

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11431v1](http://arxiv.org/pdf/2509.11431v1)

**Abstract:**

The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

---

### 329. Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and   Orchestration

**Authors:** Liangxuan Guo, Bin Zhu, Qingqian Tao, Kangning Liu, Xun Zhao, Xianzhe Qin, Jin Gao, Guangfu Hao

**Published:** 2025-09-14

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.11067v2](http://arxiv.org/pdf/2509.11067v2)

**Abstract:**

Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
Agentic Lybic, a novel multi-agent system where the entire architecture
operates as a finite-state machine (FSM). This core innovation enables dynamic
orchestration. Our system comprises four components: a Controller, a Manager,
three Workers (Technician for code-based operations, Operator for GUI
interactions, and Analyst for decision support), and an Evaluator. The critical
mechanism is the FSM-based routing between these components, which provides
flexibility and generalization by dynamically selecting the optimal execution
strategy for each subtask. This principled orchestration, combined with robust
quality gating, enables adaptive replanning and error recovery. Evaluated
officially on the OSWorld benchmark, Agentic Lybic achieves a state-of-the-art
57.07% success rate in 50 steps, substantially outperforming existing methods.
Results demonstrate that principled multi-agent orchestration with continuous
quality control provides superior reliability for generalized desktop
automation in complex computing environments.

---

### 330. RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue   Systems

**Authors:** Adarsh Srinivasan, Jacob Dineen, Muhammad Umar Afzal, Muhammad Uzair Sarfraz, Irbaz B. Riaz, Ben Zhou

**Published:** 2025-09-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.10746v1](http://arxiv.org/pdf/2509.10746v1)

**Abstract:**

Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

---

### 331. FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR   Question Answering

**Authors:** Gyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Alistair Johnson, Edward Choi, Yugang jia, Jong Ha Lee

**Published:** 2025-09-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.19319v1](http://arxiv.org/pdf/2509.19319v1)

**Abstract:**

The recent shift toward the Health Level Seven Fast Healthcare
Interoperability Resources (HL7 FHIR) standard opens a new frontier for
clinical AI, demanding LLM agents to navigate complex, resource-based data
models instead of conventional structured health data. However, existing
benchmarks have lagged behind this transition, lacking the realism needed to
evaluate recent LLMs on interoperable clinical data. To bridge this gap, we
introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical
questions in the HL7 FHIR standard. Using this benchmark, we systematically
evaluate agentic frameworks, comparing different data retrieval strategies
(direct FHIR API calls vs. specialized tools), interaction patterns
(single-turn vs. multi-turn), and reasoning strategies (natural language vs.
code generation). Our experiments highlight the practical challenges of
retrieving data from intricate FHIR resources and the difficulty of reasoning
over them, both of which critically affect question answering performance. We
publicly release the FHIR-AgentBench dataset and evaluation suite
(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research
and the development of robust, reliable LLM agents for clinical applications.

---

### 332. Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and   Exploration

**Authors:** Chirayu Nimonkar, Shlok Shah, Catherine Ji, Benjamin Eysenbach

**Published:** 2025-09-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.10656v1](http://arxiv.org/pdf/2509.10656v1)

**Abstract:**

For groups of autonomous agents to achieve a particular goal, they must
engage in coordination and long-horizon reasoning. However, designing reward
functions to elicit such behavior is challenging. In this paper, we study how
self-supervised goal-reaching techniques can be leveraged to enable agents to
cooperate. The key idea is that, rather than have agents maximize some scalar
reward, agents aim to maximize the likelihood of visiting a certain goal. This
problem setting enables human users to specify tasks via a single goal state
rather than implementing a complex reward function. While the feedback signal
is quite sparse, we will demonstrate that self-supervised goal-reaching
techniques enable agents to learn from such feedback. On MARL benchmarks, our
proposed method outperforms alternative approaches that have access to the same
sparse reward signal as our method. While our method has no explicit mechanism
for exploration, we observe that self-supervised multi-agent goal-reaching
leads to emergent cooperation and exploration in settings where alternative
approaches never witness a single successful trial.

---

### 333. V-Math: An Agentic Approach to the Vietnamese National High School   Graduation Mathematics Exams

**Authors:** Duong Q. Nguyen, Quy P. Nguyen, Nguyen Van Nhon, Quang-Thinh Bui, H. Nguyen-Xuan

**Published:** 2025-09-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12251v1](http://arxiv.org/pdf/2509.12251v1)

**Abstract:**

This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

---

### 334. Tackling One Health Risks: How Large Language Models are leveraged for   Risk Negotiation and Consensus-building

**Authors:** Alexandra Fetsch, Iurii Savvateev, Racem Ben Romdhane, Martin Wiedmann, Artemiy Dimov, Maciej Durkalec, Josef Teichmann, Jakob Zinsstag, Konstantinos Koutsoumanis, Andreja Rajkovic, Jason Mann, Mauro Tonolla, Monika Ehling-Schulz, Matthias Filter, Sophia Johler

**Published:** 2025-09-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.09906v1](http://arxiv.org/pdf/2509.09906v1)

**Abstract:**

Key global challenges of our times are characterized by complex
interdependencies and can only be effectively addressed through an integrated,
participatory effort. Conventional risk analysis frameworks often reduce
complexity to ensure manageability, creating silos that hinder comprehensive
solutions. A fundamental shift towards holistic strategies is essential to
enable effective negotiations between different sectors and to balance the
competing interests of stakeholders. However, achieving this balance is often
hindered by limited time, vast amounts of information, and the complexity of
integrating diverse perspectives. This study presents an AI-assisted
negotiation framework that incorporates large language models (LLMs) and
AI-based autonomous agents into a negotiation-centered risk analysis workflow.
The framework enables stakeholders to simulate negotiations, systematically
model dynamics, anticipate compromises, and evaluate solution impacts. By
leveraging LLMs' semantic analysis capabilities we could mitigate information
overload and augment decision-making process under time constraints.
Proof-of-concept implementations were conducted in two real-world scenarios:
(i) prudent use of a biopesticide, and (ii) targeted wild animal population
control. Our work demonstrates the potential of AI-assisted negotiation to
address the current lack of tools for cross-sectoral engagement. Importantly,
the solution's open source, web based design, suits for application by a
broader audience with limited resources and enables users to tailor and develop
it for their own needs.

---

### 335. Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement   Learning

**Authors:** Abdel Hakim Drid, Vincenzo Suriani, Daniele Nardi, Abderrezzak Debilou

**Published:** 2025-09-11

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.09356v1](http://arxiv.org/pdf/2509.09356v1)

**Abstract:**

Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

---

### 336. Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges,   and Solutions

**Authors:** Qinnan Hu, Yuntao Wang, Yuan Gao, Zhou Su, Linkang Du

**Published:** 2025-09-11

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.09215v1](http://arxiv.org/pdf/2509.09215v1)

**Abstract:**

Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

---

### 337. Strategic Tradeoffs Between Humans and AI in Multi-Agent Bargaining

**Authors:** Crystal Qian, Kehang Zhu, John Horton, Benjamin S. Manning, Vivian Tsai, James Wexler, Nithum Thain

**Published:** 2025-09-11

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.09071v2](http://arxiv.org/pdf/2509.09071v2)

**Abstract:**

Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

---

### 338. RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and   Correct Image Interpretation Model Shortcuts

**Authors:** Lauren H. Cooke, Matthias Jung, Jan M. Brendel, Nora M. Kerkovits, Borek Foldyna, Michael T. Lu, Vineet K. Raghu

**Published:** 2025-09-10

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08640v1](http://arxiv.org/pdf/2509.08640v1)

**Abstract:**

Chest radiographs (CXRs) are among the most common tests in medicine.
Automated image interpretation may reduce radiologists\' workload and expand
access to diagnostic expertise. Deep learning multi-task and foundation models
have shown strong performance for CXR interpretation but are vulnerable to
shortcut learning, where models rely on spurious and off-target correlations
rather than clinically relevant features to make decisions. We introduce
RoentMod, a counterfactual image editing framework that generates anatomically
realistic CXRs with user-specified, synthetic pathology while preserving
unrelated anatomical features of the original scan. RoentMod combines an
open-source medical image generator (RoentGen) with an image-to-image
modification model without requiring retraining. In reader studies with
board-certified radiologists and radiology residents, RoentMod-produced images
appeared realistic in 93\% of cases, correctly incorporated the specified
finding in 89-99\% of cases, and preserved native anatomy comparable to real
follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task
and foundation models frequently exploit off-target pathology as shortcuts,
limiting their specificity. Incorporating RoentMod-generated counterfactual
images during training mitigated this vulnerability, improving model
discrimination across multiple pathologies by 3-19\% AUC in internal validation
and by 1-11\% for 5 out of 6 tested pathologies in external testing. These
findings establish RoentMod as a broadly applicable tool for probing and
correcting shortcut learning in medical AI. By enabling controlled
counterfactual interventions, RoentMod enhances the robustness and
interpretability of CXR interpretation models and provides a generalizable
strategy for improving foundation models in medical imaging.

---

### 339. The CRITICAL Records Integrated Standardization Pipeline (CRISP):   End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data

**Authors:** Xiaolong Luo, Michael Lingzhi Li

**Published:** 2025-09-10

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08247v2](http://arxiv.org/pdf/2509.08247v2)

**Abstract:**

While existing critical care EHR datasets such as MIMIC and eICU have enabled
significant advances in clinical AI research, the CRITICAL dataset opens new
frontiers by providing extensive scale and diversity -- containing 1.95 billion
records from 371,365 patients across four geographically diverse CTSA
institutions. CRITICAL's unique strength lies in capturing full-spectrum
patient journeys, including pre-ICU, ICU, and post-ICU encounters across both
inpatient and outpatient settings. This multi-institutional, longitudinal
perspective creates transformative opportunities for developing generalizable
predictive models and advancing health equity research. However, the richness
of this multi-site resource introduces substantial complexity in data
harmonization, with heterogeneous collection practices and diverse vocabulary
usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource.
CRISP systematically transforms raw Observational Medical Outcomes Partnership
Common Data Model data into ML-ready datasets through: (1) transparent data
quality management with comprehensive audit trails, (2) cross-vocabulary
mapping of heterogeneous medical terminologies to unified SNOMED-CT standards,
with deduplication and unit standardization, (3) modular architecture with
parallel optimization enabling complete dataset processing in $<$1 day even on
standard computing hardware, and (4) comprehensive baseline model benchmarks
spanning multiple clinical prediction tasks to establish reproducible
performance standards. By providing processing pipeline, baseline
implementations, and detailed transformation documentation, CRISP saves
researchers months of preprocessing effort and democratizes access to
large-scale multi-institutional critical care data, enabling them to focus on
advancing clinical AI.

---

### 340. FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast   Marching Tree for Dynamic Replanning

**Authors:** Soheil Espahbodini Nia

**Published:** 2025-09-10

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08521v1](http://arxiv.org/pdf/2509.08521v1)

**Abstract:**

Path planning in dynamic environments remains a core challenge in robotics,
especially as autonomous systems are deployed in unpredictable spaces such as
warehouses and public roads. While algorithms like Fast Marching Tree
(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their
single-pass design prevents path revisions which are essential for real-time
adaptation. On the other hand, full replanning is often too computationally
expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching
Tree algorithm that enables efficient and consistent replanning in dynamic
environments. We revisit the neighbor selection rule of FMT$^{*}$ and
demonstrate that a minimal change overcomes its single-pass limitation,
enabling the algorithm to update cost-to-come values upon discovering better
connections without sacrificing asymptotic optimality or computational
efficiency. By maintaining a cost-ordered priority queue and applying a
selective update condition that uses an expanding neighbor to identify and
trigger the re-evaluation of any node with a potentially suboptimal path,
FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the
environment evolves. This targeted strategy preserves the inherent efficiency
of FMT$^{*}$ while enabling robust adaptation to changes in obstacle
configuration. FMT$^{x}$ is proven to recover an asymptotically optimal
solution after environmental changes. Experimental results demonstrate that
FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more
swiftly to dynamic events with lower computational overhead and thus offering a
more effective solution for real-time robotic navigation in unpredictable
worlds.

---

### 341. Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML   Compliance Narratives

**Authors:** Prathamesh Vasudeo Naik, Naresh Kumar Dintakurthi, Zhanghao Hu, Yue Wang, Robby Qiu

**Published:** 2025-09-10

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08380v2](http://arxiv.org/pdf/2509.08380v2)

**Abstract:**

Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

---

### 342. Large language models surpass domain-specific architectures for   antepartum electronic fetal monitoring analysis

**Authors:** Sheng Wong, Ravi Shankar, Beth Albert, Gabriel Davis Jones

**Published:** 2025-09-09

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.18112v1](http://arxiv.org/pdf/2509.18112v1)

**Abstract:**

Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

---

### 343. Multi Robot Coordination in Highly Dynamic Environments: Tackling   Asymmetric Obstacles and Limited Communication

**Authors:** Vincenzo Suriani, Daniele Affinita, Domenico D. Bloisi, Daniele Nardi

**Published:** 2025-09-09

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08859v1](http://arxiv.org/pdf/2509.08859v1)

**Abstract:**

Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

---

### 344. Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation

**Authors:** Viraj Parimi, Brian C. Williams

**Published:** 2025-09-09

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08157v1](http://arxiv.org/pdf/2509.08157v1)

**Abstract:**

Safe navigation is essential for autonomous systems operating in hazardous
environments, especially when multiple agents must coordinate using just visual
inputs over extended time horizons. Traditional planning methods excel at
solving long-horizon tasks but rely on predefined distance metrics, while safe
Reinforcement Learning (RL) can learn complex behaviors using high-dimensional
inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work
combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an
intermediate graph from replay buffer states, pruning unsafe edges, and using
Conflict-Based Search (CBS) for multi-agent path planning. Although effective,
this graph-pruning approach can be overly conservative, limiting mission
efficiency by precluding missions that must traverse high-risk regions. To
address this limitation, we propose RB-CBS, a novel extension to CBS that
dynamically allocates and adjusts user-specified risk bound ($\Delta$) across
agents to flexibly trade off safety and speed. Our improved planner ensures
that each agent receives a local risk budget ($\delta$) enabling more efficient
navigation while still respecting overall safety constraints. Experimental
results demonstrate that this iterative risk-allocation framework yields
superior performance in complex environments, allowing multiple agents to find
collision-free paths within the user-specified $\Delta$.

---

### 345. EnvX: Agentize Everything with Agentic AI

**Authors:** Linyao Chen, Zimian Peng, Yingxuan Yang, Yikun Wang, Wenzheng Tom Tang, Hiroki H. Kobayashi, Weinan Zhang

**Published:** 2025-09-09

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.08088v1](http://arxiv.org/pdf/2509.08088v1)

**Abstract:**

The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

---

### 346. Robust Docking Maneuvers for Autonomous Trolley Collection: An   Optimization-Based Visual Servoing Scheme

**Authors:** Yuhan Pang, Bingyi Xia, Zhe Zhang, Zhirui Sun, Peijia Xie, Bike Zhu, Wenjun Xu, Jiankun Wang

**Published:** 2025-09-09

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.07413v2](http://arxiv.org/pdf/2509.07413v2)

**Abstract:**

Service robots have demonstrated significant potential for autonomous trolley
collection and redistribution in public spaces like airports or warehouses to
improve efficiency and reduce cost. Usually, a fully autonomous system for the
collection and transportation of multiple trolleys is based on a
Leader-Follower formation of mobile manipulators, where reliable docking
maneuvers of the mobile base are essential to align trolleys into organized
queues. However, developing a vision-based robotic docking system faces
significant challenges: high precision requirements, environmental
disturbances, and inherent robot constraints. To address these challenges, we
propose an optimization-based Visual Servoing scheme that incorporates active
infrared markers for robust feature extraction across diverse lighting
conditions. This framework explicitly models nonholonomic kinematics and
visibility constraints within the Hybrid Visual Servoing problem, augmented
with an observer for disturbance rejection to ensure precise and stable
docking. Experimental results across diverse environments demonstrate the
robustness of this system, with quantitative evaluations confirming high
docking accuracy.

---

### 347. Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease   Progression Prediction

**Authors:** Benjamin Burns, Yuan Xue, Douglas W. Scharre, Xia Ning

**Published:** 2025-09-08

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12234v1](http://arxiv.org/pdf/2509.12234v1)

**Abstract:**

Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

---

### 348. Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in   Panoramic Radiographs using Federated, Centralized and Local Learning

**Authors:** Johan Andreas Balle Rubak, Khuram Naveed, Sanyam Jain, Lukas Esterle, Alexandros Iosifidis, Ruben Pauwels

**Published:** 2025-09-08

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.06553v1](http://arxiv.org/pdf/2509.06553v1)

**Abstract:**

Objectives: Federated learning (FL) may mitigate privacy constraints,
heterogeneous data quality, and inconsistent labeling in dental diagnostic AI.
We compared FL with centralized (CL) and local learning (LL) for tooth
segmentation in panoramic radiographs across multiple data corruption
scenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six
institutions across four settings: baseline (unaltered data); label
manipulation (dilated/missing annotations); image-quality manipulation
(additive Gaussian noise); and exclusion of a faulty client with corrupted
data. FL was implemented via the Flower AI framework. Per-client training- and
validation-loss trajectories were monitored for anomaly detection and a set of
metrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.
From these metrics significance results were reported through Wilcoxon
signed-rank test. CL and LL served as comparators. Results: Baseline: FL
achieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at
0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).
Label manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:
1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:
1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL
scored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:
1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:
1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring
reliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and
outperforms LL across corruption scenarios while preserving privacy. Per-client
loss trajectories provide an effective anomaly-detection mechanism and support
FL as a practical, privacy-preserving approach for scalable clinical AI
deployment.

---

### 349. Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and   Opportunities

**Authors:** Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Shouhuai Xu, Houbing Herbert Song

**Published:** 2025-09-08

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.06921v1](http://arxiv.org/pdf/2509.06921v1)

**Abstract:**

Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit
fundamental limitations: inadequate conceptual grounding leading to
non-robustness against novel attacks; limited instructibility impeding
analyst-guided adaptation; and misalignment with cybersecurity objectives.
Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize
cybersecurity AI. However, there is no systematic understanding of this
emerging approach. These hybrid systems address critical cybersecurity
challenges by combining neural pattern recognition with symbolic reasoning,
enabling enhanced threat understanding while introducing concerning autonomous
offensive capabilities that reshape threat landscapes. In this survey, we
systematically characterize this field by analyzing 127 publications spanning
2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)
framework to evaluate these systems, focusing on both cyber defense and cyber
offense across network security, malware analysis, and cyber operations. Our
analysis shows advantages of multi-agent NeSy architectures and identifies
critical implementation challenges including standardization gaps,
computational complexity, and human-AI collaboration requirements that
constrain deployment. We show that causal reasoning integration is the most
transformative advancement, enabling proactive defense beyond correlation-based
approaches. Our findings highlight dual-use implications where autonomous
systems demonstrate substantial capabilities in zero-day exploitation while
achieving significant cost reductions, altering threat dynamics. We provide
insights and future research directions, emphasizing the urgent need for
community-driven standardization frameworks and responsible development
practices that ensure advancement serves defensive cybersecurity objectives
while maintaining societal alignment.

---

### 350. AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced   Classification

**Authors:** Sukumar Kishanthan, Asela Hevapathige

**Published:** 2025-09-08

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.06875v1](http://arxiv.org/pdf/2509.06875v1)

**Abstract:**

Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

---

### 351. RAFFLES: Reasoning-based Attribution of Faults for LLM Systems

**Authors:** Chenyang Zhu, Spencer Hong, Jingyu Wu, Kushal Chawla, Charlotte Tang, Youbing Yin, Nathan Wolfe, Erin Babinsky, Daben Liu

**Published:** 2025-09-08

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.06822v1](http://arxiv.org/pdf/2509.06822v1)

**Abstract:**

We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

---

### 352. A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot   Applications

**Authors:** Shiqi Xu, Lihao Zhang, Yuyang Du, Qun Yang, Soung Chang Liew

**Published:** 2025-09-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.06119v2](http://arxiv.org/pdf/2509.06119v2)

**Abstract:**

Recent progress in robotics has underscored the demand for real-time control
in applications such as manufacturing, healthcare, and autonomous systems,
where the timely delivery of mission-critical commands under heterogeneous
robotic traffic is paramount for operational efficacy and safety. In these
scenarios, mission-critical traffic follows a strict deadline-constrained
communication pattern: commands must arrive within defined QoS deadlines,
otherwise late arrivals can degrade performance or destabilize control loops.In
this work, we demonstrate on a real-time SDR platform that CSMA, widely adopted
in robotic communications,suffers severe degradation under high robot traffic
loads, with contention-induced collisions and delays disrupting the on-time
arrival of mission-critical packets. To address this problem, we propose an
IEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's
deterministic slot scheduling with CSMA's adaptability for heterogeneous robot
traffic.The protocol achieves collision-free, low-latency mission-critical
command delivery and IEEE 802.11 compatibility through the synergistic
integration of sub-microsecond PTP-based slot synchronization-essential for
establishing precise timing for TDMA, a three-session superframe with dynamic
TDMA allocation for structured and adaptable traffic management,and beacon-NAV
protection to preemptively secure these critical communication sessions from
interference. Emulation experiments on real-time SDR testbed and Robot
Operating System (ROS) simulation show that the proposed protocol reduces
missed-deadline errors by 93% compared to the CSMA baseline. In high-speed
robot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)
trajectory error by up to 90% compared with a CSMA baseline, all while
maintaining throughput for non-critical traffic within +-2%.

---

### 353. Robotic Manipulation Framework Based on Semantic Keypoints for Packing   Shoes of Different Sizes, Shapes, and Softness

**Authors:** Yi Dong, Yangjun Liu, Jinjun Duan, Yang Li, Zhendong Dai

**Published:** 2025-09-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.06048v1](http://arxiv.org/pdf/2509.06048v1)

**Abstract:**

With the rapid development of the warehousing and logistics industries, the
packing of goods has gradually attracted the attention of academia and
industry. The packing of footwear products is a typical representative
paired-item packing task involving irregular shapes and deformable objects.
Although studies on shoe packing have been conducted, different initial states
due to the irregular shapes of shoes and standard packing placement poses have
not been considered. This study proposes a robotic manipulation framework,
including a perception module, reorientation planners, and a packing planner,
that can complete the packing of pairs of shoes in any initial state. First, to
adapt to the large intraclass variations due to the state, shape, and
deformation of the shoe, we propose a vision module based on semantic
keypoints, which can also infer more information such as size, state, pose, and
manipulation points by combining geometric features. Subsequently, we not only
proposed primitive-based reorientation methods for different states of a single
deformable shoe but also proposed a fast reorientation method for the top state
using box edge contact and gravity, which further improved the efficiency of
reorientation. Finally, based on the perception module and reorientation
methods, we propose a task planner for shoe pair packing in any initial state
to provide an optimal packing strategy. Real-world experiments were conducted
to verify the robustness of the reorientation methods and the effectiveness of
the packing strategy for various types of shoes. In this study, we highlight
the potential of semantic keypoint representation methods, introduce new
perspectives on the reorientation of 3D deformable objects and multi-object
manipulation, and provide a reference for paired object packing.

---

### 354. ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code

**Authors:** Kapil Madan

**Published:** 2025-09-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.07006v1](http://arxiv.org/pdf/2509.07006v1)

**Abstract:**

This paper introduces ArGen (Auto-Regulation of Generative AI systems), a
framework for aligning Large Language Models (LLMs) with complex sets of
configurable, machine-readable rules spanning ethical principles, operational
safety protocols, and regulatory compliance standards. Moving beyond just
preference-based alignment, ArGen is designed to ensure LLMs adhere to these
multifaceted policies through a novel synthesis of principle-based automated
reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy
Agent (OPA) inspired governance layer. This approach provides the technical
foundation for achieving and demonstrating compliance with diverse and nuanced
governance requirements. To showcase the framework's capability to
operationalize a deeply nuanced and culturally-specific value system, we
present an in-depth case study: the development of a medical AI assistant
guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as
derived from texts like the Bhagavad Gita. This challenging application
demonstrates ArGen's adaptability, achieving a 70.9% improvement in
domain-scope adherence over the baseline. Through our open-source repository,
we show that ArGen's methodology offers a path to 'Governable Al' systems that
are technically proficient, ethically robust, and verifiably compliant for safe
deployment in diverse global contexts.

---

### 355. Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a   Compact Mapping Strategy

**Authors:** Liansheng Wang, Xinke Zhang, Chenhui Li, Dongjiao He, Yihan Pan, Jianjun Yi

**Published:** 2025-09-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.05723v1](http://arxiv.org/pdf/2509.05723v1)

**Abstract:**

LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous
systems, yet its deployment on resource-constrained platforms remains
challenging due to computational and memory limitations. We propose Super-LIO,
a robust LIO system that demands both high performance and accuracy, ideal for
applications such as aerial robots and mobile autonomous systems. At the core
of Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that
limits each voxel to eight fused subvoxels, enabling strict point density
control and incremental denoising during map updates. This design enables a
simple yet efficient and accurate map structure, which can be easily integrated
into existing LIO frameworks. Additionally, Super-LIO designs a
heuristic-guided KNN strategy (HKNN) that accelerates the correspondence search
by leveraging spatial locality, further reducing runtime overhead. We evaluated
the proposed system using four publicly available datasets and several
self-collected datasets, totaling more than 30 sequences. Extensive testing on
both X86 and ARM platforms confirms that Super-LIO offers superior efficiency
and robustness, while maintaining competitive accuracy. Super-LIO processes
each frame approximately 73% faster than SOTA, while consuming less CPU
resources. The system is fully open-source and plug-and-play compatible with a
wide range of LiDAR sensors and platforms. The implementation is available at:
https://github.com/Liansheng-Wang/Super-LIO.git

---

### 356. Biomedical Literature Q&A System Using Retrieval-Augmented Generation   (RAG)

**Authors:** Mansi Garg, Lee-Chi Wang, Bhavesh Ghanchi, Sanjana Dumpala, Shreyash Kakde, Yen Chih Chen

**Published:** 2025-09-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.05505v1](http://arxiv.org/pdf/2509.05505v1)

**Abstract:**

This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

---

### 357. Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith   Excavation

**Authors:** Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez

**Published:** 2025-09-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.05475v1](http://arxiv.org/pdf/2509.05475v1)

**Abstract:**

Autonomous regolith excavation is a cornerstone of in-situ resource
utilization for a sustained human presence beyond Earth. However, this task is
fundamentally hindered by the complex interaction dynamics of granular media
and the operational need for robots to use diverse tools. To address these
challenges, this work introduces a framework where a model-based reinforcement
learning agent learns within a parallelized simulation. This environment
leverages high-fidelity particle physics and procedural generation to create a
vast distribution of both lunar terrains and excavation tool geometries. To
master this diversity, the agent learns an adaptive interaction strategy by
dynamically modulating its own stiffness and damping at each control step
through operational space control. Our experiments demonstrate that training
with a procedural distribution of tools is critical for generalization and
enables the development of sophisticated tool-aware behavior. Furthermore, we
show that augmenting the agent with visual feedback significantly improves task
success. These results represent a validated methodology for developing the
robust and versatile autonomous systems required for the foundational tasks of
future space missions.

---

### 358. Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for   Ranking Agents

**Authors:** Rajesh Tembarai Krishnamachari, Srividya Rajesh

**Published:** 2025-09-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.04979v1](http://arxiv.org/pdf/2509.04979v1)

**Abstract:**

AI agents -- powered by reasoning-capable large language models (LLMs) and
integrated with tools, data, and web search -- are poised to transform the
internet into a \emph{Web of Agents}: a machine-native ecosystem where
autonomous agents interact, collaborate, and execute tasks at scale. Realizing
this vision requires \emph{Agent Ranking} -- selecting agents not only by
declared capabilities but by proven, recent performance. Unlike Web~1.0's
PageRank, a global, transparent network of agent interactions does not exist;
usage signals are fragmented and private, making ranking infeasible without
coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol
(\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that
enables the collection of minimal, privacy-preserving aggregates of usage and
performance across the ecosystem. On this substrate, we implement
\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines
\emph{usage} (selection frequency) and \emph{competence} (outcome quality,
cost, safety, latency) into a unified ranking. We present simulation results
and theoretical guarantees on convergence, robustness, and Sybil resistance,
demonstrating the viability of coordinated protocols and performance-aware
ranking in enabling a scalable, trustworthy Agentic Web.

---

### 359. Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge   in Large Language Models

**Authors:** Juraj Vladika, Mahdi Dhaini, Florian Matthes

**Published:** 2025-09-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.04304v1](http://arxiv.org/pdf/2509.04304v1)

**Abstract:**

The growing capabilities of Large Language Models (LLMs) show significant
potential to enhance healthcare by assisting medical researchers and
physicians. However, their reliance on static training data is a major risk
when medical recommendations evolve with new research and developments. When
LLMs memorize outdated medical knowledge, they can provide harmful advice or
fail at clinical reasoning tasks. To investigate this problem, we introduce two
novel question-answering (QA) datasets derived from systematic reviews:
MedRevQA (16,501 QA pairs covering general biomedical knowledge) and
MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over
time). Our evaluation of eight prominent LLMs on the datasets reveals
consistent reliance on outdated knowledge across all models. We additionally
analyze the influence of obsolete pre-training data and training strategies to
explain this phenomenon and propose future directions for mitigation, laying
the groundwork for developing more current and reliable medical AI systems.

---

### 360. FPC-VLA: A Vision-Language-Action Framework with a Supervisor for   Failure Prediction and Correction

**Authors:** Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, Jingtai Liu

**Published:** 2025-09-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.04018v1](http://arxiv.org/pdf/2509.04018v1)

**Abstract:**

Robotic manipulation is a fundamental component of automation. However,
traditional perception-planning pipelines often fall short in open-ended tasks
due to limited flexibility, while the architecture of a single end-to-end
Vision-Language-Action (VLA) offers promising capabilities but lacks crucial
mechanisms for anticipating and recovering from failure. To address these
challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with
a supervisor for failure prediction and correction. The supervisor evaluates
action viability through vision-language queries and generates corrective
strategies when risks arise, trained efficiently without manual labeling. A
similarity-guided fusion module further refines actions by leveraging past
predictions. Evaluation results on multiple simulation platforms (SIMPLER and
LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA
outperforms state-of-the-art models in both zero-shot and fine-tuned settings.
By activating the supervisor only at keyframes, our approach significantly
increases task success rates with minimal impact on execution time. Successful
real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong
generalization and practical utility for building more reliable autonomous
systems.

---

### 361. FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer   Marketplace

**Authors:** Yineng Yan, Xidong Wang, Jin Seng Cheng, Ran Hu, Wentao Guan, Nahid Farahmand, Hengte Lin, Yue Li

**Published:** 2025-09-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.03890v1](http://arxiv.org/pdf/2509.03890v1)

**Abstract:**

The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

---

### 362. Accountability Framework for Healthcare AI Systems: Towards Joint   Accountability in Decision Making

**Authors:** Prachi Bagave, Marcus Westberg, Marijn Janssen, Aaron Yi Ding

**Published:** 2025-09-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.03286v1](http://arxiv.org/pdf/2509.03286v1)

**Abstract:**

AI is transforming the healthcare domain and is increasingly helping
practitioners to make health-related decisions. Therefore, accountability
becomes a crucial concern for critical AI-driven decisions. Although regulatory
bodies, such as the EU commission, provide guidelines, they are highlevel and
focus on the ''what'' that should be done and less on the ''how'', creating a
knowledge gap for actors. Through an extensive analysis, we found that the term
accountability is perceived and dealt with in many different ways, depending on
the actor's expertise and domain of work. With increasing concerns about AI
accountability issues and the ambiguity around this term, this paper bridges
the gap between the ''what'' and ''how'' of AI accountability, specifically for
AI systems in healthcare. We do this by analysing the concept of
accountability, formulating an accountability framework, and providing a
three-tier structure for handling various accountability mechanisms. Our
accountability framework positions the regulations of healthcare AI systems and
the mechanisms adopted by the actors under a consistent accountability regime.
Moreover, the three-tier structure guides the actors of the healthcare AI
system to categorise the mechanisms based on their conduct. Through our
framework, we advocate that decision-making in healthcare AI holds shared
dependencies, where accountability should be dealt with jointly and should
foster collaborations. We highlight the role of explainability in instigating
communication and information sharing between the actors to further facilitate
the collaborative process.

---

### 363. Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic   Learning Approach

**Authors:** Midhat Urooj, Ayan Banerjee, Farhat Shaikh, Kuntal Thakur, Sandeep Gupta

**Published:** 2025-09-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.02918v1](http://arxiv.org/pdf/2509.02918v1)

**Abstract:**

Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

---

### 364. Autonomous Learning From Success and Failure: Goal-Conditioned   Supervised Learning with Negative Feedback

**Authors:** Zeqiang Zhang, Fabian Wurzberger, Gerrit Schmid, Sebastian Gottwald, Daniel A. Braun

**Published:** 2025-09-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.03206v1](http://arxiv.org/pdf/2509.03206v1)

**Abstract:**

Reinforcement learning faces significant challenges when applied to tasks
characterized by sparse reward structures. Although imitation learning, within
the domain of supervised learning, offers faster convergence, it relies heavily
on human-generated demonstrations. Recently, Goal-Conditioned Supervised
Learning (GCSL) has emerged as a potential solution by enabling self-imitation
learning for autonomous systems. By strategically relabelling goals, agents can
derive policy insights from their own experiences. Despite the successes of
this framework, it presents two notable limitations: (1) Learning exclusively
from self-generated experiences can exacerbate the agents' inherent biases; (2)
The relabelling strategy allows agents to focus solely on successful outcomes,
precluding them from learning from their mistakes. To address these issues, we
propose a novel model that integrates contrastive learning principles into the
GCSL framework to learn from both success and failure. Through empirical
evaluations, we demonstrate that our algorithm overcomes limitations imposed by
agents' initial biases and thereby enables more exploratory behavior. This
facilitates the identification and adoption of effective policies, leading to
superior performance across a variety of challenging environments.

---

### 365. Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly   Domain Adaptive Dense Regression

**Authors:** Uddeshya Upadhyay

**Published:** 2025-09-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.03012v1](http://arxiv.org/pdf/2509.03012v1)

**Abstract:**

Deep neural networks (DNNs) are increasingly being used in autonomous
systems. However, DNNs do not generalize well to domain shift. Adapting to a
continuously evolving environment is a safety-critical challenge inevitably
faced by all autonomous systems deployed to the real world. Recent work on
test-time training proposes methods that adapt to a new test distribution on
the fly by optimizing the DNN model for each test input using self-supervision.
However, these techniques result in a sharp increase in inference time as
multiple forward and backward passes are required for a single test sample (for
test-time training) before finally making the prediction based on the
fine-tuned features. This is undesirable for real-world robotics applications
where these models may be deployed to resource constraint hardware with strong
latency requirements. In this work, we propose a new framework (called UT$^3$)
that leverages test-time training for improved performance in the presence of
continuous domain shift while also decreasing the inference time, making it
suitable for real-world applications. Our method proposes an uncertainty-aware
self-supervision task for efficient test-time training that leverages the
quantified uncertainty to selectively apply the training leading to sharp
improvements in the inference time while performing comparably to standard
test-time training protocol. Our proposed protocol offers a continuous setting
to identify the selected keyframes, allowing the end-user to control how often
to apply test-time training. We demonstrate the efficacy of our method on a
dense regression task - monocular depth estimation.

---

### 366. IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for   Dynamic Environments

**Authors:** Haolan Zhang, Thanh Nguyen Canh, Chenghao Li, Ruidong Yang, Yonghoon Ji, Nak Young Chong

**Published:** 2025-09-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.02972v1](http://arxiv.org/pdf/2509.02972v1)

**Abstract:**

Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in
autonomous systems. Traditional SLAM methods, based on static environment
assumptions, struggle to handle complex dynamic environments. Recent dynamic
SLAM systems employ geometric constraints and deep learning to remove dynamic
features, yet this creates a new challenge: insufficient remaining point
features for subsequent SLAM processes. Existing solutions address this by
continuously introducing additional line and plane features to supplement point
features, achieving robust tracking and pose estimation. However, current
methods continuously introduce additional features regardless of necessity,
causing two problems: unnecessary computational overhead and potential
performance degradation from accumulated low-quality additional features and
noise. To address these issues, this paper proposes a feature-aware mechanism
that evaluates whether current features are adequate to determine if line
feature support should be activated. This decision mechanism enables the system
to introduce line features only when necessary, significantly reducing
computational complexity of additional features while minimizing the
introduction of low-quality features and noise. In subsequent processing, the
introduced line features assist in obtaining better initial camera poses
through tracking, local mapping, and loop closure, but are excluded from global
optimization to avoid potential negative impacts from low-quality additional
features in long-term process. Extensive experiments on TUM datasets
demonstrate substantial improvements in both ATE and RPE metrics compared to
ORB-SLAM3 baseline and superior performance over other dynamic SLAM and
multi-feature methods.

---

### 367. Baichuan-M2: Scaling Medical Capability with Large Verifier System

**Authors:** Baichuan-M2 Team,  :, Chengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya Dang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Chenzheng Zhu, Da Pan, Fei Deng, Guangwei Ai, Guosheng Dong, Hongda Zhang, Jinyang Tai, Jixiang Hong, Kai Lu, Linzhuang Sun, Peidong Guo, Qian Ma, Rihui Xin, Shihui Yang, Shusen Zhang, Yichuan Mo, Zheng Liang, Zhishou Zhang, Hengfu Cui, Zuyi Zhu, Xiaochuan Wang

**Published:** 2025-09-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.02208v1](http://arxiv.org/pdf/2509.02208v1)

**Abstract:**

As large language models (LLMs) advance in conversational and reasoning
capabilities, their practical application in healthcare has become a critical
research focus. However, there is a notable gap between the performance of
medical LLMs on static benchmarks such as USMLE and their utility in real-world
clinical decision-making. This discrepancy arises because traditional exams
fail to capture the dynamic, interactive nature of medical consultations. To
address this challenge, we introduce a novel dynamic verification framework
that moves beyond static answer verifier, establishing a large-scale,
high-fidelity interactive reinforcement learning system. Our framework
comprises two key components: a Patient Simulator that creates realistic
clinical environments using de-identified medical records, and a Clinical
Rubrics Generator that dynamically produces multi-dimensional evaluation
metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter
medical augmented reasoning model trained through a multi-stage reinforcement
learning strategy with an improved Group Relative Policy Optimization (GRPO)
algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other
open-source models and most advanced closed-source counterparts, achieving a
score above 32 on the challenging HealthBench Hard benchmark-previously
exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier
system is essential for aligning LLM capabilities with practical clinical
applications, establishing a new Pareto front in the performance-parameter
trade-off for medical AI deployment.

---

### 368. UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn   Reinforcement Learning

**Authors:** Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, Wanjun Zhong, Yining Ye, Yujia Qin, Yuwen Xiong, Yuxin Song, Zhiyong Wu, Aoyan Li, Bo Li, Chen Dun, Chong Liu, Daoguang Zan, Fuxing Leng, Hanbin Wang, Hao Yu, Haobin Chen, Hongyi Guo, Jing Su, Jingjia Huang, Kai Shen, Kaiyu Shi, Lin Yan, Peiyao Zhao, Pengfei Liu, Qinghao Ye, Renjie Zheng, Shulin Xin, Wayne Xin Zhao, Wen Heng, Wenhao Huang, Wenqian Wang, Xiaobo Qin, Yi Lin, Youbin Wu, Zehui Chen, Zihao Wang, Baoquan Zhong, Xinchun Zhang, Xujing Li, Yuanfan Li, Zhongkai Zhao, Chengquan Jiang, Faming Wu, Haotian Zhou, Jinlin Pang, Li Han, Qi Liu, Qianli Ma, Siyao Liu, Songhua Cai, Wenqi Fu, Xin Liu, Yaohui Wang, Zhi Zhang, Bo Zhou, Guoliang Li, Jiajun Shi, Jiale Yang, Jie Tang, Li Li, Qihua Han, Taoran Lu, Woyu Lin, Xiaokang Tong, Xinyao Li, Yichi Zhang, Yu Miao, Zhengxuan Jiang, Zili Li, Ziyuan Zhao, Chenxin Li, Dehua Ma, Feng Lin, Ge Zhang, Haihua Yang, Hangyu Guo, Hongda Zhu, Jiaheng Liu, Junda Du, Kai Cai, Kuanye Li, Lichen Yuan, Meilan Han, Minchao Wang, Shuyue Guo, Tianhao Cheng, Xiaobo Ma, Xiaojun Xiao, Xiaolong Huang, Xinjie Chen, Yidi Du, Yilin Chen, Yiwen Wang, Zhaojian Li, Zhenzhu Yang, Zhiyuan Zeng, Chaolin Jin, Chen Li, Hao Chen, Haoli Chen, Jian Chen, Qinghao Zhao, Guang Shi

**Published:** 2025-09-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.02544v2](http://arxiv.org/pdf/2509.02544v2)

**Abstract:**

The development of autonomous agents for graphical user interfaces (GUIs)
presents major challenges in artificial intelligence. While recent advances in
native agent models have shown promise by unifying perception, reasoning,
action, and memory through end-to-end learning, open problems remain in data
scalability, multi-turn reinforcement learning (RL), the limitations of
GUI-only operation, and environment stability. In this technical report, we
present UI-TARS-2, a native GUI-centered agent model that addresses these
challenges through a systematic training methodology: a data flywheel for
scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI
environment that integrates file systems and terminals, and a unified sandbox
platform for large-scale rollouts. Empirical evaluation demonstrates that
UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.
On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on
WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines
such as Claude and OpenAI agents. In game environments, it attains a mean
normalized score of 59.8 across a 15-game suite-roughly 60% of human-level
performance-and remains competitive with frontier proprietary models (e.g.,
OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to
long-horizon information-seeking tasks and software engineering benchmarks,
highlighting its robustness across diverse agent tasks. Detailed analyses of
training dynamics further provide insights into achieving stability and
efficiency in large-scale agent RL. These results underscore UI-TARS-2's
potential to advance the state of GUI agents and exhibit strong generalization
to real-world interactive scenarios.

---

### 369. Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy   Weather Conditions

**Authors:** Beibei Zhou, Zhiyuan Zhang, Zhenbo Song, Jianhui Guo, Hui Kong

**Published:** 2025-09-02

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.02011v1](http://arxiv.org/pdf/2509.02011v1)

**Abstract:**

Deep learning-based LiDAR odometry is crucial for autonomous driving and
robotic navigation, yet its performance under adverse weather, especially
snowfall, remains challenging. Existing models struggle to generalize across
conditions due to sensitivity to snow-induced noise, limiting real-world use.
In this work, we present an unsupervised LiDAR odometry model to close the gap
between clear and snowy weather conditions. Our approach focuses on effective
denoising to mitigate the impact of snowflake noise and outlier points on pose
estimation, while also maintaining computational efficiency for real-time
applications.
  To achieve this, we introduce a Patch Spatial Measure (PSM) module that
evaluates the dispersion of points within each patch, enabling effective
detection of sparse and discrete noise.
  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive
point-wise weights, enhancing their discriminative capacity within local
regions. To support real-time performance, we first apply an intensity
threshold mask to quickly suppress dense snowflake clusters near the LiDAR, and
then perform multi-modal feature fusion to refine the point-wise weight
prediction, improving overall robustness under adverse weather. Our model is
trained in clear weather conditions and rigorously tested across various
scenarios, including snowy and dynamic. Extensive experimental results confirm
the effectiveness of our method, demonstrating robust performance in both clear
and snowy weather. This advancement enhances the model's generalizability and
paves the way for more reliable autonomous systems capable of operating across
a wider range of environmental conditions.

---

### 370. Metamorphic Testing of Multimodal Human Trajectory Prediction

**Authors:** Helge Spieker, Nadjib Lazaar, Arnaud Gotlieb, Nassim Belmecheri

**Published:** 2025-09-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.01294v1](http://arxiv.org/pdf/2509.01294v1)

**Abstract:**

Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.

---

### 371. Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge   Networks

**Authors:** Andrea Fox, Francesco De Pellegrini, Eitan Altman

**Published:** 2025-09-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.01257v1](http://arxiv.org/pdf/2509.01257v1)

**Abstract:**

In edge computing systems, autonomous agents must make fast local decisions
while competing for shared resources. Existing MARL methods often resume to
centralized critics or frequent communication, which fail under limited
observability and communication constraints. We propose a decentralized
framework in which each agent solves a constrained Markov decision process
(CMDP), coordinating implicitly through a shared constraint vector. For the
specific case of offloading, e.g., constraints prevent overloading shared
server resources. Coordination constraints are updated infrequently and act as
a lightweight coordination mechanism. They enable agents to align with global
resource usage objectives but require little direct communication. Using safe
reinforcement learning, agents learn policies that meet both local and global
goals. We establish theoretical guarantees under mild assumptions and validate
our approach experimentally, showing improved performance over centralized and
independent baselines, especially in large-scale settings.

---

### 372. FlowECG: Using Flow Matching to Create a More Efficient ECG Signal   Generator

**Authors:** Vitalii Bondar, Serhii Semenov, Vira Babenko, Dmytro Holovniak

**Published:** 2025-08-31

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.10491v1](http://arxiv.org/pdf/2509.10491v1)

**Abstract:**

Synthetic electrocardiogram generation serves medical AI applications
requiring privacy-preserving data sharing and training dataset augmentation.
Current diffusion-based methods achieve high generation quality but require
hundreds of neural network evaluations during sampling, creating computational
bottlenecks for clinical deployment. We propose FlowECG, a flow matching
approach that adapts the SSSD-ECG architecture by replacing the iterative
diffusion process with continuous flow dynamics. Flow matching learns direct
transport paths from noise to data distributions through ordinary differential
equation solving. We evaluate our method on the PTB-XL dataset using Dynamic
Time Warping, Wasserstein distance, Maximum Mean Discrepancy, and spectral
similarity metrics. FlowECG matches SSSD-ECG performance at 200 neural function
evaluations, outperforming the baseline on three metrics. The key finding shows
that FlowECG maintains generation quality with substantially fewer sampling
steps, achieving comparable results with 10-25 evaluations compared to 200 for
diffusion methods. This efficiency improvement reduces computational
requirements by an order of magnitude while preserving physiologically
realistic 12-lead ECG characteristics. The approach enables practical
deployment in resource-limited clinical settings where real-time generation or
large-scale synthetic data creation is needed.

---

### 373. Online Decentralized Federated Multi-task Learning With Trustworthiness   in Cyber-Physical Systems

**Authors:** Olusola Odeyomi, Sofiat Olaosebikan, Ajibuwa Opeyemi, Oluwadoyinsola Ige

**Published:** 2025-08-31

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.00992v1](http://arxiv.org/pdf/2509.00992v1)

**Abstract:**

Multi-task learning is an effective way to address the challenge of model
personalization caused by high data heterogeneity in federated learning.
However, extending multi-task learning to the online decentralized federated
learning setting is yet to be explored. The online decentralized federated
learning setting considers many real-world applications of federated learning,
such as autonomous systems, where clients communicate peer-to-peer and the data
distribution of each client is time-varying. A more serious problem in
real-world applications of federated learning is the presence of Byzantine
clients. Byzantine-resilient approaches used in federated learning work only
when the number of Byzantine clients is less than one-half the total number of
clients. Yet, it is difficult to put a limit on the number of Byzantine clients
within a system in reality. However, recent work in robotics shows that it is
possible to exploit cyber-physical properties of a system to predict clients'
behavior and assign a trust probability to received signals. This can help to
achieve resiliency in the presence of a dominating number of Byzantine clients.
Therefore, in this paper, we develop an online decentralized federated
multi-task learning algorithm to provide model personalization and resiliency
when the number of Byzantine clients dominates the number of honest clients.
Our proposed algorithm leverages cyber-physical properties, such as the
received signal strength in wireless systems or side information, to assign a
trust probability to local models received from neighbors in each iteration.
Our simulation results show that the proposed algorithm performs close to a
Byzantine-free setting.

---

### 374. Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing   Bias in Medical AI

**Authors:** Farhad Abtahi, Mehdi Astaraki, Fernando Seoane

**Published:** 2025-08-29

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.21648v1](http://arxiv.org/pdf/2508.21648v1)

**Abstract:**

Bias in medical artificial intelligence is conventionally viewed as a defect
requiring elimination. However, human reasoning inherently incorporates biases
shaped by education, culture, and experience, suggesting their presence may be
inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble
Diagnostic system with Leveraged diversitY), a conceptual framework that
orchestrates multiple AI models while preserving their diverse outputs rather
than collapsing them into a consensus. Unlike traditional approaches that
suppress disagreement, MEDLEY documents model-specific biases as potential
strengths and treats hallucinations as provisional hypotheses for clinician
verification. A proof-of-concept demonstrator was developed using over 30 large
language models, creating a minimum viable product that preserved both
consensus and minority views in synthetic cases, making diagnostic uncertainty
and latent biases transparent for clinical oversight. While not yet a validated
clinical tool, the demonstration illustrates how structured diversity can
enhance medical reasoning under clinician supervision. By reframing AI
imperfection as a resource, MEDLEY offers a paradigm shift that opens new
regulatory, ethical, and innovation pathways for developing trustworthy medical
AI systems.

---

### 375. ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor   Decomposition

**Authors:** Ahmed E. Helal, Fabio Checconi, Jan Laukemann, Yongseok Soh, Jesmin Jahan Tithi, Fabrizio Petrini, Jee Choi

**Published:** 2025-08-29

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.00280v1](http://arxiv.org/pdf/2509.00280v1)

**Abstract:**

Tensor decomposition (TD) is essential for analyzing high-dimensional sparse
data, yet its irregular computations and memory-access patterns pose major
performance challenges on modern parallel processors. Prior works rely on
expert-designed sparse tensor formats that fail to adapt to irregular tensor
shapes and/or highly variable data distributions. We present the
reinforcement-learned adaptive tensor encoding (ReLATE) framework, a novel
learning-augmented method that automatically constructs efficient sparse tensor
representations without labeled training samples. ReLATE employs an autonomous
agent that discovers optimized tensor encodings through direct interaction with
the TD environment, leveraging a hybrid model-free and model-based algorithm to
learn from both real and imagined actions. Moreover, ReLATE introduces
rule-driven action masking and dynamics-informed action filtering mechanisms
that ensure functionally correct tensor encoding with bounded execution time,
even during early learning stages. By automatically adapting to both irregular
tensor shapes and data distributions, ReLATE generates sparse tensor
representations that consistently outperform expert-designed formats across
diverse sparse tensor data sets, achieving up to 2X speedup compared to the
best sparse format, with a geometric-mean speedup of 1.4-1.46X.

---

### 376. HiVA: Self-organized Hierarchical Variable Agent via Goal-driven   Semantic-Topological Evolution

**Authors:** Jinzhou Tang, Jusheng Zhang, Qinhan Lv, Sidi Liu, Jing Yang, Chengpei Tang, Keze Wang

**Published:** 2025-08-29

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.00189v1](http://arxiv.org/pdf/2509.00189v1)

**Abstract:**

Autonomous agents play a crucial role in advancing Artificial General
Intelligence, enabling problem decomposition and tool orchestration through
Large Language Models (LLMs). However, existing paradigms face a critical
trade-off. On one hand, reusable fixed workflows require manual reconfiguration
upon environmental changes; on the other hand, flexible reactive loops fail to
distill reasoning progress into transferable structures. We introduce
Hierarchical Variable Agent (HiVA), a novel framework modeling agentic
workflows as self-organized graphs with the Semantic-Topological Evolution
(STEV) algorithm, which optimizes hybrid semantic-topological spaces using
textual gradients as discrete-domain surrogates for backpropagation. The
iterative process comprises Multi-Armed Bandit-infused forward routing,
diagnostic gradient generation from environmental feedback, and coordinated
updates that co-evolve individual semantics and topology for collective
optimization in unknown environments. Experiments on dialogue, coding,
Long-context Q&A, mathematical, and agentic benchmarks demonstrate improvements
of 5-10% in task accuracy and enhanced resource efficiency over existing
baselines, establishing HiVA's effectiveness in autonomous task execution.

---

### 377. Mini Autonomous Car Driving based on 3D Convolutional Neural Networks

**Authors:** Pablo Moraes, Monica Rodriguez, Kristofer S. Kappel, Hiago Sodre, Santiago Fernandez, Igor Nunes, Bruna Guterres, Ricardo Grando

**Published:** 2025-08-29

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.21271v1](http://arxiv.org/pdf/2508.21271v1)

**Abstract:**

Autonomous driving applications have become increasingly relevant in the
automotive industry due to their potential to enhance vehicle safety,
efficiency, and user experience, thereby meeting the growing demand for
sophisticated driving assistance features. However, the development of reliable
and trustworthy autonomous systems poses challenges such as high complexity,
prolonged training periods, and intrinsic levels of uncertainty. Mini
Autonomous Cars (MACs) are used as a practical testbed, enabling validation of
autonomous control methodologies on small-scale setups. This simplified and
cost-effective environment facilitates rapid evaluation and comparison of
machine learning models, which is particularly useful for algorithms requiring
online training. To address these challenges, this work presents a methodology
based on RGB-D information and three-dimensional convolutional neural networks
(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the
proposed approach against recurrent neural networks (RNNs), with architectures
trained and tested on two simulated tracks with distinct environmental
features. Performance was assessed using task completion success, lap-time
metrics, and driving consistency. Results highlight how architectural
modifications and track complexity influence the models' generalization
capability and vehicle control performance. The proposed 3D CNN demonstrated
promising results when compared with RNNs.

---

### 378. MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via   Generative Reward Learning

**Authors:** Weihai Zhi, Jiayan Guo, Shangyang Li

**Published:** 2025-08-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.20549v1](http://arxiv.org/pdf/2508.20549v1)

**Abstract:**

The application of Vision-Language Models (VLMs) in medicine is critically
hampered by the scarcity of high-quality, expert-annotated data. Supervised
Fine-Tuning (SFT) on existing datasets often leads to poor generalization on
unseen modalities and tasks, while Reinforcement Learning (RL), a promising
alternative, is stymied by the lack of reliable reward signals in this
data-scarce domain. To break this impasse, we introduce Generative Reward
Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a
self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a
reward model, enabling the automated, continuous creation of high-quality,
multi-modal medical data that serves as both a superior training source for SFT
and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data
already surpasses baselines trained on large-scale, human-curated datasets.
Crucially, when leveraging this data for RL via Group Relative Policy
Optimization (GRPO), our model achieves state-of-the-art cross-modality and
cross-task generalization, significantly outperforming specialized RL-based
methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves
performance competitive with foundation models possessing over 10 times more
parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in
high-stakes domains, transforming the problem from data scarcity to data
generation and unlocking the full potential of RL for building truly
generalizable medical AI.

---

### 379. Beyond Prediction: Reinforcement Learning as the Defining Leap in   Healthcare AI

**Authors:** Dilruk Perera, Gousia Habib, Qianyi Xu, Daniel J. Tan, Kai He, Erik Cambria, Mengling Feng

**Published:** 2025-08-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.21101v1](http://arxiv.org/pdf/2508.21101v1)

**Abstract:**

Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

---

### 380. A Survey of Scientific Large Language Models: From Data Foundations to   Agent Frontiers

**Authors:** Ming Hu, Chenglong Ma, Wei Li, Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu, Yingzhou Lu, Ying Chen, Chaoyang Zhang, Cheng Tan, Jie Ying, Guocheng Wu, Shujian Gao, Pengcheng Chen, Jiashi Lin, Haitao Wu, Lulu Chen, Fengxiang Wang, Yuanyuan Zhang, Xiangyu Zhao, Feilong Tang, Encheng Su, Junzhi Ning, Xinyao Liu, Ye Du, Changkai Ji, Cheng Tang, Huihui Xu, Ziyang Chen, Ziyan Huang, Jiyao Liu, Pengfei Jiang, Yizhou Wang, Chen Tang, Jianyu Wu, Yuchen Ren, Siyuan Yan, Zhonghua Wang, Zhongxing Xu, Shiyan Su, Shangquan Sun, Runkai Zhao, Zhisheng Zhang, Yu Liu, Fudi Wang, Yuanfeng Ji, Yanzhou Su, Hongming Shan, Chunmei Feng, Jiahao Xu, Jiangtao Yan, Wenhao Tang, Diping Song, Lihao Liu, Yanyan Huang, Lequan Yu, Bin Fu, Shujun Wang, Xiaomeng Li, Xiaowei Hu, Yun Gu, Ben Fei, Zhongying Deng, Benyou Wang, Yuewen Cao, Minjie Shen, Haodong Duan, Jie Xu, Yirong Chen, Fang Yan, Hongxia Hao, Jielan Li, Jiajun Du, Yanbo Wang, Imran Razzak, Chi Zhang, Lijun Wu, Conghui He, Zhaohui Lu, Jinhai Huang, Yihao Liu, Fenghua Ling, Yuqiang Li, Aoran Wang, Qihao Zheng, Nanqing Dong, Tianfan Fu, Dongzhan Zhou, Yan Lu, Wenlong Zhang, Jin Ye, Jianfei Cai, Wanli Ouyang, Yu Qiao, Zongyuan Ge, Shixiang Tang, Junjun He, Chunfeng Song, Lei Bai, Bowen Zhou

**Published:** 2025-08-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.21148v1](http://arxiv.org/pdf/2508.21148v1)

**Abstract:**

Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

---

### 381. Ontology-Based Concept Distillation for Radiology Report Retrieval and   Labeling

**Authors:** Felix NÃ¼tzel, Mischa Dombrowski, Bernhard Kainz

**Published:** 2025-08-27

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.19915v1](http://arxiv.org/pdf/2508.19915v1)

**Abstract:**

Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

---

### 382. CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer   Use Agent with Decoupled Reinforcement Learning

**Authors:** Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang

**Published:** 2025-08-27

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.20096v1](http://arxiv.org/pdf/2508.20096v1)

**Abstract:**

Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

---

### 383. CompLex: Music Theory Lexicon Constructed by Autonomous Agents for   Automatic Music Generation

**Authors:** Zhejing Hu, Yan Liu, Gong Chen, Bruce X. B. Yu

**Published:** 2025-08-27

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.19603v1](http://arxiv.org/pdf/2508.19603v1)

**Abstract:**

Generative artificial intelligence in music has made significant strides, yet
it still falls short of the substantial achievements seen in natural language
processing, primarily due to the limited availability of music data.
Knowledge-informed approaches have been shown to enhance the performance of
music generation models, even when only a few pieces of musical knowledge are
integrated. This paper seeks to leverage comprehensive music theory in
AI-driven music generation tasks, such as algorithmic composition and style
transfer, which traditionally require significant manual effort with existing
techniques. We introduce a novel automatic music lexicon construction model
that generates a lexicon, named CompLex, comprising 37,432 items derived from
just 9 manually input category keywords and 5 sentence prompt templates. A new
multi-agent algorithm is proposed to automatically detect and mitigate
hallucinations. CompLex demonstrates impressive performance improvements across
three state-of-the-art text-to-music generation models, encompassing both
symbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of
completeness, accuracy, non-redundancy, and executability, confirming that it
possesses the key characteristics of an effective lexicon.

---

### 384. Trustworthy Agents for Electronic Health Records through Confidence   Estimation

**Authors:** Yongwoo Song, Minbyul Jeong, Mujeen Sung

**Published:** 2025-08-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.19096v1](http://arxiv.org/pdf/2508.19096v1)

**Abstract:**

Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

---

### 385. Model Context Protocols in Adaptive Transport Systems: A Survey

**Authors:** Gaurab Chhetri, Shriyank Somvanshi, Md Monzurul Islam, Shamyo Brotee, Mahmuda Sultana Mimi, Dipti Koirala, Biplov Pandey, Subasish Das

**Published:** 2025-08-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.19239v1](http://arxiv.org/pdf/2508.19239v1)

**Abstract:**

The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

---

### 386. Real-Time Model Checking for Closed-Loop Robot Reactive Planning

**Authors:** Christopher Chandler, Bernd Porr, Giulia Lafratta, Alice Miller

**Published:** 2025-08-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.19186v1](http://arxiv.org/pdf/2508.19186v1)

**Abstract:**

We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on "core" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.

---

### 387. UrgenGo: Urgency-Aware Transparent GPU Kernel Launching for Autonomous   Driving

**Authors:** Hanqi Zhu, Wuyang Zhang, Xinran Zhang, Ziyang Tao, Xinrui Lin, Yu Zhang, Jianmin Ji, Yanyong Zhang

**Published:** 2025-08-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2509.12207v1](http://arxiv.org/pdf/2509.12207v1)

**Abstract:**

The rapid advancements in autonomous driving have introduced increasingly
complex, real-time GPU-bound tasks critical for reliable vehicle operation.
However, the proprietary nature of these autonomous systems and closed-source
GPU drivers hinder fine-grained control over GPU executions, often resulting in
missed deadlines that compromise vehicle performance. To address this, we
present UrgenGo, a non-intrusive, urgency-aware GPU scheduling system that
operates without access to application source code. UrgenGo implicitly
prioritizes GPU executions through transparent kernel launch manipulation,
employing task-level stream binding, delayed kernel launching, and batched
kernel launch synchronization. We conducted extensive real-world evaluations in
collaboration with a self-driving startup, developing 11 GPU-bound task chains
for a realistic autonomous navigation application and implementing our system
on a self-driving bus. Our results show a significant 61% reduction in the
overall deadline miss ratio, compared to the state-of-the-art GPU scheduler
that requires source code modifications.

---

### 388. AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust   Autonomy

**Authors:** Christian Henkel, Marco Lampacrescia, Michaela Klauck, Matteo Morelli

**Published:** 2025-08-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.18820v1](http://arxiv.org/pdf/2508.18820v1)

**Abstract:**

Designing robotic systems to act autonomously in unforeseen environments is a
challenging task. This work presents a novel approach to use formal
verification, specifically Statistical Model Checking (SMC), to verify system
properties of autonomous robots at design-time. We introduce an extension of
the SCXML format, designed to model system components including both Robot
Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we
contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the
full system model into JANI. The use of JANI, a standard format for
quantitative model checking, enables verification of system properties with
off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both
in terms of applicability to real-world autonomous robotic control systems, and
in terms of verification runtime scaling. We provide a case study, where we
successfully identify problems in a ROS 2-based robotic manipulation use case
that is verifiable in less than one second using consumer hardware.
Additionally, we compare to the state of the art and demonstrate that our
method is more comprehensive in system feature support, and that the
verification runtime scales linearly with the size of the model, instead of
exponentially.

---

### 389. FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS   Rule Generation

**Authors:** Shaswata Mitra, Azim Bazarov, Martin Duclos, Sudip Mittal, Aritran Piplai, Md Rayhanur Rahman, Edward Zieglar, Shahram Rahimi

**Published:** 2025-08-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.18684v1](http://arxiv.org/pdf/2508.18684v1)

**Abstract:**

Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

---

### 390. Mimicking associative learning of rats via a neuromorphic robot in open   field maze using spatial cell models

**Authors:** Tianze Liu, Md Abu Bakr Siddique, Hongyu An

**Published:** 2025-08-25

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.18460v1](http://arxiv.org/pdf/2508.18460v1)

**Abstract:**

Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable
prowess across various cognitive tasks using extensive training data. However,
the reliance on large datasets and neural networks presents challenges such as
highpower consumption and limited adaptability, particularly in
SWaP-constrained applications like planetary exploration. To address these
issues, we propose enhancing the autonomous capabilities of intelligent robots
by emulating the associative learning observed in animals. Associative learning
enables animals to adapt to their environment by memorizing concurrent events.
By replicating this mechanism, neuromorphic robots can navigate dynamic
environments autonomously, learning from interactions to optimize performance.
This paper explores the emulation of associative learning in rodents using
neuromorphic robots within open-field maze environments, leveraging insights
from spatial cells such as place and grid cells. By integrating these models,
we aim to enable online associative learning for spatial tasks in real-time
scenarios, bridging the gap between biological spatial cognition and robotics
for advancements in autonomous systems.

---

### 391. Toward Generalized Autonomous Agents: A Neuro-Symbolic AI Framework for   Integrating Social and Technical Support in Education

**Authors:** Ryan Hare, Ying Tang

**Published:** 2025-08-25

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.18406v1](http://arxiv.org/pdf/2508.18406v1)

**Abstract:**

One of the enduring challenges in education is how to empower students to
take ownership of their learning by setting meaningful goals, tracking their
progress, and adapting their strategies when faced with setbacks. Research has
shown that this form of leaner-centered learning is best cultivated through
structured, supportive environments that promote guided practice, scaffolded
inquiry, and collaborative dialogue. In response, educational efforts have
increasingly embraced artificial-intelligence (AI)-powered digital learning
environments, ranging from educational apps and virtual labs to serious games.
Recent advances in large language models (LLMs) and neuro-symbolic systems,
meanwhile, offer a transformative opportunity to reimagine how support is
delivered in digital learning environments. LLMs are enabling socially
interactive learning experiences and scalable, cross-domain learning support
that can adapt instructional strategies across varied subjects and contexts. In
parallel, neuro-symbolic AI provides new avenues for designing these agents
that are not only adaptive but also scalable across domains. Based on these
remarks, this paper presents a multi-agent, neuro-symbolic framework designed
to resolve the aforementioned challenges. The framework assigns distinct
pedagogical roles to specialized agents: an RL-based 'tutor' agent provides
authoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer'
agent facilitates the social dimensions of learning. While prior work has
explored such agents in isolation, our framework's novelty lies in unifying
them through a central educational ontology. Through case studies in both
college-level and middle school settings, we demonstrate the framework's
adaptability across domains. We conclude by outlining key insights and future
directions for advancing AI-driven learning environments.

---

### 392. How to make Medical AI Systems safer? Simulating Vulnerabilities, and   Threats in Multimodal Medical RAG System

**Authors:** Kaiwen Zuo, Zelin Liu, Raman Dutt, Ziyang Wang, Zhongtian Sun, Yeming Wang, Fan Mo, Pietro LiÃ²

**Published:** 2025-08-24

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.17215v1](http://arxiv.org/pdf/2508.17215v1)

**Abstract:**

Large Vision-Language Models (LVLMs) augmented with Retrieval-Augmented
Generation (RAG) are increasingly employed in medical AI to enhance factual
grounding through external clinical image-text retrieval. However, this
reliance creates a significant attack surface. We propose MedThreatRAG, a novel
multimodal poisoning framework that systematically probes vulnerabilities in
medical RAG systems by injecting adversarial image-text pairs. A key innovation
of our approach is the construction of a simulated semi-open attack
environment, mimicking real-world medical systems that permit periodic
knowledge base updates via user or pipeline contributions. Within this setting,
we introduce and emphasize Cross-Modal Conflict Injection (CMCI), which embeds
subtle semantic contradictions between medical images and their paired reports.
These mismatches degrade retrieval and generation by disrupting cross-modal
alignment while remaining sufficiently plausible to evade conventional filters.
While basic textual and visual attacks are included for completeness, CMCI
demonstrates the most severe degradation. Evaluations on IU-Xray and MIMIC-CXR
QA tasks show that MedThreatRAG reduces answer F1 scores by up to 27.66% and
lowers LLaVA-Med-1.5 F1 rates to as low as 51.36%. Our findings expose
fundamental security gaps in clinical RAG systems and highlight the urgent need
for threat-aware design and robust multimodal consistency checks. Finally, we
conclude with a concise set of guidelines to inform the safe development of
future multimodal medical RAG systems.

---

### 393. Machine Learning for Medicine Must Be Interpretable, Shareable,   Reproducible and Accountable by Design

**Authors:** AyyÃ¼ce BegÃ¼m BektaÅ, Mithat GÃ¶nen

**Published:** 2025-08-22

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.16097v1](http://arxiv.org/pdf/2508.16097v1)

**Abstract:**

This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

---

### 394. Explainable Knowledge Distillation for Efficient Medical Image   Classification

**Authors:** Aqib Nazir Mir, Danish Raza Rizvi

**Published:** 2025-08-21

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.15251v1](http://arxiv.org/pdf/2508.15251v1)

**Abstract:**

This study comprehensively explores knowledge distillation frameworks for
COVID-19 and lung cancer classification using chest X-ray (CXR) images. We
employ high-capacity teacher models, including VGG19 and lightweight Vision
Transformers (Visformer-S and AutoFormer-V2-T), to guide the training of a
compact, hardware-aware student model derived from the OFA-595 supernet. Our
approach leverages hybrid supervision, combining ground-truth labels with
teacher models' soft targets to balance accuracy and computational efficiency.
We validate our models on two benchmark datasets: COVID-QU-Ex and LCS25000,
covering multiple classes, including COVID-19, healthy, non-COVID pneumonia,
lung, and colon cancer. To interpret the spatial focus of the models, we employ
Score-CAM-based visualizations, which provide insight into the reasoning
process of both teacher and student networks. The results demonstrate that the
distilled student model maintains high classification performance with
significantly reduced parameters and inference time, making it an optimal
choice in resource-constrained clinical environments. Our work underscores the
importance of combining model efficiency with explainability for practical,
trustworthy medical AI solutions.

---

### 395. Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule   Detection on Chest X-Ray

**Authors:** Hyeonjin Choi, Jinse Kim, Dong-yeon Yoo, Ju-sung Sun, Jung-won Lee

**Published:** 2025-08-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.13236v1](http://arxiv.org/pdf/2508.13236v1)

**Abstract:**

Early detection and rapid intervention of lung cancer are crucial.
Nonetheless, ensuring an accurate diagnosis is challenging, as physicians'
ability to interpret chest X-rays varies significantly depending on their
experience and degree of fatigue. Although medical AI has been rapidly
advancing to assist in diagnosis, physicians' trust in such systems remains
limited, preventing widespread clinical adoption. This skepticism fundamentally
stems from concerns about its diagnostic uncertainty. In clinical diagnosis,
physicians utilize extensive background knowledge and clinical experience. In
contrast, medical AI primarily relies on repetitive learning of the target
lesion to generate diagnoses based solely on that data. In other words, medical
AI does not possess sufficient knowledge to render a diagnosis, leading to
diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning
Policy that can address the issue of knowledge deficiency by learning the
physicians' background knowledge alongside the Chest X-ray lesion information.
We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou
University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a
10% enhancement in sensitivity compared to the baseline model while also
decreasing entropy as a measure of uncertainty by 0.2.

---

### 396. Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning:   Scaling Laws between Computational Resources and Reasoning Quality

**Authors:** Ziqian Bi, Lu Chen, Junhao Song, Hongying Luo, Enze Ge, Junmin Huang, Tianyang Wang, Keyu Chen, Chia Xin Liang, Zihan Wei, Huafeng Liu, Chunjie Tian, Jibin Guan, Joe Yeong, Yongzhi Xu, Peng Wang, Junfeng Hao

**Published:** 2025-08-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.12140v1](http://arxiv.org/pdf/2508.12140v1)

**Abstract:**

This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

---

### 397. Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual   Question Answering

**Authors:** Rakesh Thakur, Yusra Tariq

**Published:** 2025-08-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.12036v1](http://arxiv.org/pdf/2508.12036v1)

**Abstract:**

Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

---

### 398. QuarkMed Medical Foundation Model Technical Report

**Authors:** Ao Li, Bin Yan, Bingfeng Cai, Chenxi Li, Cunzhong Zhao, Fugen Yao, Gaoqiang Liu, Guanjun Jiang, Jian Xu, Liang Dong, Liansheng Sun, Rongshen Zhang, Xiaolei Gui, Xin Liu, Xin Shang, Yao Wu, Yu Cao, Zhenxin Ma, Zhuang Jia

**Published:** 2025-08-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.11894v1](http://arxiv.org/pdf/2508.11894v1)

**Abstract:**

Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

---

### 399. PASS: Probabilistic Agentic Supernet Sampling for Interpretable and   Adaptive Chest X-Ray Reasoning

**Authors:** Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu

**Published:** 2025-08-14

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.10501v2](http://arxiv.org/pdf/2508.10501v2)

**Abstract:**

Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

---

### 400. Towards Efficient Prompt-based Continual Learning in Distributed Medical   AI

**Authors:** Gyutae Oh, Jitae Shin

**Published:** 2025-08-14

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.10954v1](http://arxiv.org/pdf/2508.10954v1)

**Abstract:**

Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

---

### 401. A Comprehensive Review of Datasets for Clinical Mental Health AI Systems

**Authors:** Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty

**Published:** 2025-08-13

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.09809v2](http://arxiv.org/pdf/2508.09809v2)

**Abstract:**

Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

---

### 402. AMRG: Extend Vision Language Models for Automatic Mammography Report   Generation

**Authors:** Nak-Jun Sung, Donghyun Lee, Bo Hwa Choi, Chae Jung Park

**Published:** 2025-08-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.09225v1](http://arxiv.org/pdf/2508.09225v1)

**Abstract:**

Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

---

### 403. DepressLLM: Interpretable domain-adapted language model for depression   detection from real-world narratives

**Authors:** Sehwan Moon, Aram Lee, Jeong Eun Kim, Hee-Ju Kang, Il-Seon Shin, Sung-Wan Kim, Jae-Min Kim, Min Jhon, Ju-Wan Kim

**Published:** 2025-08-12

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.08591v1](http://arxiv.org/pdf/2508.08591v1)

**Abstract:**

Advances in large language models (LLMs) have enabled a wide range of
applications. However, depression prediction is hindered by the lack of
large-scale, high-quality, and rigorously annotated datasets. This study
introduces DepressLLM, trained and evaluated on a novel corpus of 3,699
autobiographical narratives reflecting both happiness and distress. DepressLLM
provides interpretable depression predictions and, via its Score-guided Token
Probability Summation (SToPS) module, delivers both improved classification
performance and reliable confidence estimates, achieving an AUC of 0.789, which
rises to 0.904 on samples with confidence $\geq$ 0.95. To validate its
robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets,
including an Ecological Momentary Assessment (EMA) corpus of daily stress and
mood recordings, and on public clinical interview data. Finally, a psychiatric
review of high-confidence misclassifications highlighted key model and data
limitations that suggest directions for future refinements. These findings
demonstrate that interpretable AI can enable earlier diagnosis of depression
and underscore the promise of medical AI in psychiatry.

---

### 404. Towards Assessing Medical Ethics from Knowledge to Practice

**Authors:** Chang Hong, Minghao Wu, Qingying Xiao, Yuchi Wang, Xiang Wan, Guangjun Yu, Benyou Wang, Yan Hu

**Published:** 2025-08-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.05132v1](http://arxiv.org/pdf/2508.05132v1)

**Abstract:**

The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

---

### 405. MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical   Multimodal Large Language Models

**Authors:** Dexuan Xu, Jieyi Wang, Zhongyan Chai, Yongzhi Cao, Hanpin Wang, Huamin Zhang, Yu Huang

**Published:** 2025-08-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.05083v1](http://arxiv.org/pdf/2508.05083v1)

**Abstract:**

Recent advances in multimodal large language models (MLLMs) have
significantly improved medical AI, enabling it to unify the understanding of
visual and textual information. However, as medical knowledge continues to
evolve, it is critical to allow these models to efficiently update outdated or
incorrect information without retraining from scratch. Although textual
knowledge editing has been widely studied, there is still a lack of systematic
benchmarks for multimodal medical knowledge editing involving image and text
modalities. To fill this gap, we present MedMKEB, the first comprehensive
benchmark designed to evaluate the reliability, generality, locality,
portability, and robustness of knowledge editing in medical multimodal large
language models. MedMKEB is built on a high-quality medical visual
question-answering dataset and enriched with carefully constructed editing
tasks, including counterfactual correction, semantic generalization, knowledge
transfer, and adversarial robustness. We incorporate human expert validation to
ensure the accuracy and reliability of the benchmark. Extensive single editing
and sequential editing experiments on state-of-the-art general and medical
MLLMs demonstrate the limitations of existing knowledge-based editing
approaches in medicine, highlighting the need to develop specialized editing
strategies. MedMKEB will serve as a standard benchmark to promote the
development of trustworthy and efficient medical knowledge editing algorithms.

---

### 406. Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate   vs Late Fusion

**Authors:** Timothy Oladunni, Ehimen Aneni

**Published:** 2025-08-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.11666v1](http://arxiv.org/pdf/2508.11666v1)

**Abstract:**

The limitations of unimodal deep learning models, particularly their tendency
to overfit and limited generalizability, have renewed interest in multimodal
fusion strategies. Multimodal deep neural networks (MDNN) have the capability
of integrating diverse data domains and offer a promising solution for robust
and accurate predictions. However, the optimal fusion strategy, intermediate
fusion (feature-level) versus late fusion (decision-level) remains
insufficiently examined, especially in high-stakes clinical contexts such as
ECG-based cardiovascular disease (CVD) classification. This study investigates
the comparative effectiveness of intermediate and late fusion strategies using
ECG signals across three domains: time, frequency, and time-frequency. A series
of experiments were conducted to identify the highest-performing fusion
architecture. Results demonstrate that intermediate fusion consistently
outperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's
d > 0.8 relative to standalone models and d = 0.40 compared to late fusion.
Interpretability analyses using saliency maps reveal that both models align
with the discretized ECG signals. Statistical dependency between the
discretized ECG signals and corresponding saliency maps for each class was
confirmed using Mutual Information (MI). The proposed ECG domain-based
multimodal model offers superior predictive capability and enhanced
explainability, crucial attributes in medical AI applications, surpassing
state-of-the-art models.

---

### 407. Continual Multiple Instance Learning for Hematologic Disease Diagnosis

**Authors:** Zahra Ebrahimi, Raheleh Salehi, Nassir Navab, Carsten Marr, Ario Sadafi

**Published:** 2025-08-06

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.04368v2](http://arxiv.org/pdf/2508.04368v2)

**Abstract:**

The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

---

### 408. A Multi-Agent System for Complex Reasoning in Radiology Visual Question   Answering

**Authors:** Ziruo Yi, Jinyu Liu, Ting Xiao, Mark V. Albert

**Published:** 2025-08-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.02841v1](http://arxiv.org/pdf/2508.02841v1)

**Abstract:**

Radiology visual question answering (RVQA) provides precise answers to
questions about chest X-ray images, alleviating radiologists' workload. While
recent methods based on multimodal large language models (MLLMs) and
retrieval-augmented generation (RAG) have shown promising progress in RVQA,
they still face challenges in factual accuracy, hallucinations, and cross-modal
misalignment. We introduce a multi-agent system (MAS) designed to support
complex reasoning in RVQA, with specialized agents for context understanding,
multimodal reasoning, and answer validation. We evaluate our system on a
challenging RVQA set curated via model disagreement filtering, comprising
consistently hard cases across multiple MLLMs. Extensive experiments
demonstrate the superiority and effectiveness of our system over strong MLLM
baselines, with a case study illustrating its reliability and interpretability.
This work highlights the potential of multi-agent approaches to support
explainable and trustworthy clinical AI applications that require complex
reasoning.

---

### 409. Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement   Techniques and Applications

**Authors:** Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan

**Published:** 2025-08-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.00669v1](http://arxiv.org/pdf/2508.00669v1)

**Abstract:**

The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

---

### 410. On the Risk of Misleading Reports: Diagnosing Textual Biases in   Multimodal Clinical AI

**Authors:** David Restrepo, Ira Ktena, Maria Vakalopoulou, Stergios Christodoulidis, Enzo Ferrante

**Published:** 2025-07-31

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.00171v1](http://arxiv.org/pdf/2508.00171v1)

**Abstract:**

Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

---

### 411. Towards Affordable Tumor Segmentation and Visualization for 3D Breast   MRI Using SAM2

**Authors:** Solha Kang, Eugene Kim, Joris Vankerschaver, Utku Ozbulak

**Published:** 2025-07-31

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.23272v1](http://arxiv.org/pdf/2507.23272v1)

**Abstract:**

Breast MRI provides high-resolution volumetric imaging critical for tumor
assessment and treatment planning, yet manual interpretation of 3D scans
remains labor-intensive and subjective. While AI-powered tools hold promise for
accelerating medical image analysis, adoption of commercial medical AI products
remains limited in low- and middle-income countries due to high license costs,
proprietary software, and infrastructure demands. In this work, we investigate
whether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,
minimal-input 3D tumor segmentation in breast MRI. Using a single bounding box
annotation on one slice, we propagate segmentation predictions across the 3D
volume using three different slice-wise tracking strategies: top-to-bottom,
bottom-to-top, and center-outward. We evaluate these strategies across a large
cohort of patients and find that center-outward propagation yields the most
consistent and accurate segmentations. Despite being a zero-shot model not
trained for volumetric medical data, SAM2 achieves strong segmentation
performance under minimal supervision. We further analyze how segmentation
performance relates to tumor size, location, and shape, identifying key failure
modes. Our results suggest that general-purpose foundation models such as SAM2
can support 3D medical image analysis with minimal supervision, offering an
accessible and affordable alternative for resource-constrained settings.

---

### 412. Label-free estimation of clinically relevant performance metrics under   distribution shifts

**Authors:** Tim FlÃ¼hmann, Alceu Bissoto, Trung-Dung Hoang, Lisa M. Koch

**Published:** 2025-07-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.22776v1](http://arxiv.org/pdf/2507.22776v1)

**Abstract:**

Performance monitoring is essential for safe clinical deployment of image
classification models. However, because ground-truth labels are typically
unavailable in the target dataset, direct assessment of real-world model
performance is infeasible. State-of-the-art performance estimation methods
address this by leveraging confidence scores to estimate the target accuracy.
Despite being a promising direction, the established methods mainly estimate
the model's accuracy and are rarely evaluated in a clinical domain, where
strong class imbalances and dataset shifts are common. Our contributions are
twofold: First, we introduce generalisations of existing performance prediction
methods that directly estimate the full confusion matrix. Then, we benchmark
their performance on chest x-ray data in real-world distribution shifts as well
as simulated covariate and prevalence shifts. The proposed confusion matrix
estimation methods reliably predicted clinically relevant counting metrics on
medical images under distribution shifts. However, our simulated shift
scenarios exposed important failure modes of current performance estimation
techniques, calling for a better understanding of real-world deployment
contexts when implementing these performance monitoring techniques for
postmarket surveillance of medical AI models.

---

### 413. Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents   For Trustworthy Medical Language Models

**Authors:** Jiazhen Pan, Bailiang Jian, Paul Hager, Yundi Zhang, Che Liu, Friedrike Jungmann, Hongwei Bran Li, Chenyu You, Junde Wu, Jiayuan Zhu, Fenglin Liu, Yuyuan Liu, Niklas Bubeck, Christian Wachinger,  Chen,  Chen, Zhenyu Gong, Cheng Ouyang, Georgios Kaissis, Benedikt Wiestler, Daniel Rueckert

**Published:** 2025-07-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2508.00923v1](http://arxiv.org/pdf/2508.00923v1)

**Abstract:**

Ensuring the safety and reliability of large language models (LLMs) in
clinical practice is critical to prevent patient harm and promote trustworthy
healthcare applications of AI. However, LLMs are advancing so rapidly that
static safety benchmarks often become obsolete upon publication, yielding only
an incomplete and sometimes misleading picture of model trustworthiness. We
demonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming
framework that continuously stress-tests LLMs can reveal significant weaknesses
of current LLMs across four safety-critical domains: robustness, privacy,
bias/fairness, and hallucination. A suite of adversarial agents is applied to
autonomously mutate test cases, identify/evolve unsafe-triggering strategies,
and evaluate responses, uncovering vulnerabilities in real time without human
intervention. Applying DAS to 15 proprietary and open-source LLMs revealed a
stark contrast between static benchmark performance and vulnerability under
adversarial pressure. Despite a median MedQA accuracy exceeding 80\%, 94\% of
previously correct answers failed our dynamic robustness tests. We observed
similarly high failure rates across other domains: privacy leaks were elicited
in 86\% of scenarios, cognitive-bias priming altered clinical recommendations
in 81\% of fairness tests, and we identified hallucination rates exceeding 66\%
in widely used models. Such profound residual risks are incompatible with
routine clinical practice. By converting red-teaming from a static checklist
into a dynamic stress-test audit, DAS red-teaming offers the surveillance that
hospitals/regulators/technology vendors require as LLMs become embedded in
patient chatbots, decision-support dashboards, and broader healthcare
workflows. Our framework delivers an evolvable, scalable, and reliable
safeguard for the next generation of medical AI.

---

### 414. ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from   Free-Text Reports

**Authors:** Mohammed Baharoon, Luyang Luo, Michael Moritz, Abhinav Kumar, Sung Eun Kim, Xiaoman Zhang, Miao Zhu, Mahmoud Hussain Alabbad, Maha Sbayel Alhazmi, Neel P. Mistry, Kent Ryan Kleinschmidt, Brady Chrisler, Sathvik Suryadevara, Sri Sai Dinesh Jaliparthi, Noah Michael Prudlo, Mark David Marino, Jeremy Palacio, Rithvik Akula, Hong-Yu Zhou, Ibrahim Ethem Hamamci, Scott J. Adams, Hassan Rayhan AlOmaish, Pranav Rajpurkar

**Published:** 2025-07-29

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.22030v1](http://arxiv.org/pdf/2507.22030v1)

**Abstract:**

We present ReXGroundingCT, the first publicly available dataset to link
free-text radiology findings with pixel-level segmentations in 3D chest CT
scans that is manually annotated. While prior datasets have relied on
structured labels or predefined categories, ReXGroundingCT captures the full
expressiveness of clinical language represented in free text and grounds it to
spatially localized 3D segmentation annotations in volumetric imaging. This
addresses a critical gap in medical AI: the ability to connect complex,
descriptive text, such as "3 mm nodule in the left lower lobe", to its precise
anatomical location in three-dimensional space, a capability essential for
grounded radiology report generation systems. The dataset comprises 3,142
non-contrast chest CT scans paired with standardized radiology reports from the
CT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to
extract positive lung and pleural findings, which were then manually segmented
by expert annotators. A total of 8,028 findings across 16,301 entities were
annotated, with quality control performed by board-certified radiologists.
Approximately 79% of findings are focal abnormalities, while 21% are non-focal.
The training set includes up to three representative segmentations per finding,
while the validation and test sets contain exhaustive labels for each finding
entity. ReXGroundingCT establishes a new benchmark for developing and
evaluating sentence-level grounding and free-text medical segmentation models
in chest CT. The dataset can be accessed at
https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.

---

### 415. Memorization in Fine-Tuned Large Language Models

**Authors:** Danil Savine

**Published:** 2025-07-28

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.21009v2](http://arxiv.org/pdf/2507.21009v2)

**Abstract:**

This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

---

### 416. Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in   Clinical LLMs

**Authors:** Raj Krishnan Vijayaraj

**Published:** 2025-07-27

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.21188v1](http://arxiv.org/pdf/2507.21188v1)

**Abstract:**

LLMs for clinical decision support often fail under small but clinically
meaningful input shifts such as masking a symptom or negating a finding,
despite high performance on static benchmarks. These reasoning failures
frequently go undetected by standard NLP metrics, which are insensitive to
latent representation shifts that drive diagnosis instability. We propose a
geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation
Diagnostics), which systematically probes the latent robustness of clinical
LLMs under structured adversarial edits. Within this framework, we introduce
Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that
captures representational instability when embeddings cross decision boundaries
in PCA-reduced latent space. Clinical notes are generated using a structured
prompting pipeline grounded in diagnostic reasoning, then perturbed along four
axes: masking, negation, synonym replacement, and numeric variation to simulate
common ambiguities and omissions. We compute LDFR across both foundation and
clinical LLMs, finding that latent fragility emerges even under minimal
surface-level changes. Finally, we validate our findings on 90 real clinical
notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of
LDFR beyond synthetic settings. Our results reveal a persistent gap between
surface robustness and semantic stability, underscoring the importance of
geometry-aware auditing in safety-critical clinical AI.

---

### 417. Zero-shot Performance of Generative AI in Brazilian Portuguese Medical   Exam

**Authors:** Cesar Augusto Madid Truyts, Amanda Gomes Rabelo, Gabriel Mesquita de Souza, Daniel Scaldaferri Lages, Adriano Jose Pereira, Uri Adrian Prync Flato, Eduardo Pontes dos Reis, Joaquim Edson Vieira, Paulo Sergio Panse Silveira, Edson Amaro Junior

**Published:** 2025-07-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.19885v1](http://arxiv.org/pdf/2507.19885v1)

**Abstract:**

Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

---

### 418. Debunking Optimization Myths in Federated Learning for Medical Image   Classification

**Authors:** Youngjoon Lee, Hyukjoon Lee, Jinu Gong, Yang Cao, Joonhyuk Kang

**Published:** 2025-07-26

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.19822v1](http://arxiv.org/pdf/2507.19822v1)

**Abstract:**

Federated Learning (FL) is a collaborative learning method that enables
decentralized model training while preserving data privacy. Despite its promise
in medical imaging, recent FL methods are often sensitive to local factors such
as optimizers and learning rates, limiting their robustness in practical
deployments. In this work, we revisit vanilla FL to clarify the impact of edge
device configurations, benchmarking recent FL methods on colorectal pathology
and blood cell classification task. We numerically show that the choice of
local optimizer and learning rate has a greater effect on performance than the
specific FL method. Moreover, we find that increasing local training epochs can
either enhance or impair convergence, depending on the FL method. These
findings indicate that appropriate edge-specific configuration is more crucial
than algorithmic complexity for achieving effective FL.

---

### 419. Comparative Analysis of Vision Transformers and Convolutional Neural   Networks for Medical Image Classification

**Authors:** Kunal Kawadkar

**Published:** 2025-07-24

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.21156v1](http://arxiv.org/pdf/2507.21156v1)

**Abstract:**

The emergence of Vision Transformers (ViTs) has revolutionized computer
vision, yet their effectiveness compared to traditional Convolutional Neural
Networks (CNNs) in medical imaging remains under-explored. This study presents
a comprehensive comparative analysis of CNN and ViT architectures across three
critical medical imaging tasks: chest X-ray pneumonia detection, brain tumor
classification, and skin cancer melanoma detection. We evaluated four
state-of-the-art models - ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small
- across datasets totaling 8,469 medical images. Our results demonstrate
task-specific model advantages: ResNet-50 achieved 98.37% accuracy on chest
X-ray classification, DeiT-Small excelled at brain tumor detection with 92.16%
accuracy, and EfficientNet-B0 led skin cancer classification at 81.84%
accuracy. These findings provide crucial insights for practitioners selecting
architectures for medical AI applications, highlighting the importance of
task-specific architecture selection in clinical decision support systems.

---

### 420. RAG-based Architectures for Drug Side Effect Retrieval in LLMs

**Authors:** Shad Nygren, Pinar Avci, Andre Daniels, Reza Rassol, Afshin Beheshti, Diego Galeano

**Published:** 2025-07-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.13822v1](http://arxiv.org/pdf/2507.13822v1)

**Abstract:**

Drug side effects are a major global health concern, necessitating advanced
methods for their accurate detection and analysis. While Large Language Models
(LLMs) offer promising conversational interfaces, their inherent limitations,
including reliance on black-box training data, susceptibility to
hallucinations, and lack of domain-specific knowledge, hinder their reliability
in specialized fields like pharmacovigilance. To address this gap, we propose
two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which
integrate comprehensive drug side effect knowledge into a Llama 3 8B language
model. Through extensive evaluations on 19,520 drug side effect associations
(covering 976 drugs and 3,851 side effect terms), our results demonstrate that
GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This
framework offers a highly accurate and scalable solution, signifying a
significant advancement in leveraging LLMs for critical pharmacovigilance
applications.

---

### 421. Interpretability-Aware Pruning for Efficient Medical Image Analysis

**Authors:** Nikita Malik, Pratinav Seth, Neeraj Kumar Singh, Chintan Chitroda, Vinay Kumar Sankarapu

**Published:** 2025-07-11

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.08330v2](http://arxiv.org/pdf/2507.08330v2)

**Abstract:**

Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

---

### 422. An autonomous agent for auditing and improving the reliability of   clinical AI models

**Authors:** Lukas Kuhn, Florian Buettner

**Published:** 2025-07-08

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.05755v1](http://arxiv.org/pdf/2507.05755v1)

**Abstract:**

The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

---

### 423. Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for   Multimodal Medical VQA

**Authors:** Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar

**Published:** 2025-07-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.05520v3](http://arxiv.org/pdf/2507.05520v3)

**Abstract:**

Dermatological care via telemedicine often lacks the rich context of
in-person visits. Clinicians must make diagnoses based on a handful of images
and brief descriptions, without the benefit of physical exams, second opinions,
or reference materials. While many medical AI systems attempt to bridge these
gaps with domain-specific fine-tuning, this work hypothesized that mimicking
clinical reasoning processes could offer a more effective path forward. This
study tested seven vision-language models on medical visual question answering
across six configurations: baseline models, fine-tuned variants, and both
augmented with either reasoning layers that combine multiple model
perspectives, analogous to peer consultation, or retrieval-augmented generation
that incorporates medical literature at inference time, serving a role similar
to reference-checking. While fine-tuning degraded performance in four of seven
models with an average 30% decrease, baseline models collapsed on test data.
Clinical-inspired architectures, meanwhile, achieved up to 70% accuracy,
maintaining performance on unseen data while generating explainable,
literature-grounded outputs critical for clinical adoption. These findings
demonstrate that medical AI succeeds by reconstructing the collaborative and
evidence-based practices fundamental to clinical diagnosis.

---

### 424. MedGemma Technical Report

**Authors:** Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, CÃ­an Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Mercy Asiedu, Ines Mezerreg, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, LÃ©onard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang

**Published:** 2025-07-07

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.05201v3](http://arxiv.org/pdf/2507.05201v3)

**Abstract:**

Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.

---

### 425. SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents

**Authors:** Hari Masoor

**Published:** 2025-07-05

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.10562v1](http://arxiv.org/pdf/2507.10562v1)

**Abstract:**

Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

---

### 426. Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust   Medical Segmentation

**Authors:** Tao Tang, Shijie Xu, Yiting Wu, Zhixiang Lu

**Published:** 2025-07-04

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.03585v1](http://arxiv.org/pdf/2507.03585v1)

**Abstract:**

The clinical utility of deep learning models for medical image segmentation
is severely constrained by their inability to generalize to unseen domains.
This failure is often rooted in the models learning spurious correlations
between anatomical content and domain-specific imaging styles. To overcome this
fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that
elevates Large Language Models (LLMs) to the role of causal reasoners. Our
framework, built upon a frozen Segment Anything Model (SAM) encoder,
incorporates two synergistic innovations. First, Linguistic Adversarial
Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual
descriptions of confounding image styles. By training the segmentation model's
features to be contrastively dissimilar to these style descriptions, it learns
a representation robustly purged of non-causal information. Second, Test-Time
Causal Intervention (TCI) provides an interactive mechanism where an LLM
interprets a clinician's natural language command to modulate the segmentation
decoder's features in real-time, enabling targeted error correction. We conduct
an extensive empirical evaluation on a composite benchmark from four public
datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under
cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM
establishes a new state of the art in out-of-distribution (OOD) robustness,
improving the average Dice score by up to 6.2 points and reducing the Hausdorff
Distance by 15.8 mm over the strongest baseline, all while using less than 9%
of the full model's trainable parameters. Our work charts a new course for
building robust, efficient, and interactively controllable medical AI systems.

---

### 427. Federated Learning for ICD Classification with Lightweight Models and   Pretrained Embeddings

**Authors:** Binbin Xu, GÃ©rard Dray

**Published:** 2025-07-03

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.03122v1](http://arxiv.org/pdf/2507.03122v1)

**Abstract:**

This study investigates the feasibility and performance of federated learning
(FL) for multi-label ICD code classification using clinical notes from the
MIMIC-IV dataset. Unlike previous approaches that rely on centralized training
or fine-tuned large language models, we propose a lightweight and scalable
pipeline combining frozen text embeddings with simple multilayer perceptron
(MLP) classifiers. This design offers a privacy-preserving and
deployment-efficient alternative for clinical NLP applications, particularly
suited to distributed healthcare settings. Extensive experiments across both
centralized and federated configurations were conducted, testing six publicly
available embedding models from Massive Text Embedding Benchmark leaderboard
and three MLP classifier architectures under two medical coding (ICD-9 and
ICD-10). Additionally, ablation studies over ten random stratified splits
assess performance stability. Results show that embedding quality substantially
outweighs classifier complexity in determining predictive performance, and that
federated learning can closely match centralized results in idealized
conditions. While the models are orders of magnitude smaller than
state-of-the-art architectures and achieved competitive micro and macro F1
scores, limitations remain including the lack of end-to-end training and the
simplified FL assumptions. Nevertheless, this work demonstrates a viable way
toward scalable, privacy-conscious medical coding systems and offers a step
toward for future research into federated, domain-adaptive clinical AI.

---

### 428. Leveraging the Structure of Medical Data for Improved Representation   Learning

**Authors:** Andrea Agostini, Sonia Laguna, Alain Ryser, Samuel Ruiperez-Campillo, Moritz Vandenhirtz, Nicolas Deperrois, Farhad Nooralahzadeh, Michael Krauthammer, Thomas M. Sutter, Julia E. Vogt

**Published:** 2025-07-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.02987v3](http://arxiv.org/pdf/2507.02987v3)

**Abstract:**

Building generalizable medical AI systems requires pretraining strategies
that are data-efficient and domain-aware. Unlike internet-scale corpora,
clinical datasets such as MIMIC-CXR offer limited image counts and scarce
annotations, but exhibit rich internal structure through multi-view imaging. We
propose a self-supervised framework that leverages the inherent structure of
medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and
lateral views) as natural positive pairs, learning to reconstruct each view
from sparse patches while aligning their latent embeddings. Our method requires
no textual supervision and produces informative representations. Evaluated on
MIMIC-CXR, we show strong performance compared to supervised objectives and
baselines being trained without leveraging structure. This work provides a
lightweight, modality-agnostic blueprint for domain-specific pretraining where
data is structured but scarce

---

### 429. Truth, Trust, and Trouble: Medical AI on the Edge

**Authors:** Mohammad Anas Azeez, Rafiq Ali, Ebad Shabbir, Zohaib Hasan Siddiqui, Gautam Siddharth Kashyap, Jiechao Gao, Usman Naseem

**Published:** 2025-07-01

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2507.02983v1](http://arxiv.org/pdf/2507.02983v1)

**Abstract:**

Large Language Models (LLMs) hold significant promise for transforming
digital health by enabling automated medical question answering. However,
ensuring these models meet critical industry standards for factual accuracy,
usefulness, and safety remains a challenge, especially for open-source
solutions. We present a rigorous benchmarking framework using a dataset of over
1,000 health questions. We assess model performance across honesty,
helpfulness, and harmlessness. Our results highlight trade-offs between factual
reliability and safety among evaluated models -- Mistral-7B,
BioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest
accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in
BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot
prompting improves accuracy from 78% to 85%, and all models show reduced
helpfulness on complex queries, highlighting ongoing challenges in clinical QA.

---

### 430. A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation

**Authors:** Renjie Liang, Zhengkang Fan, Jinqian Pan, Chenkun Sun, Russell Terry, Jie Xu

**Published:** 2025-06-30

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.23584v1](http://arxiv.org/pdf/2506.23584v1)

**Abstract:**

Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

---

### 431. Keeping Medical AI Healthy: A Review of Detection and Correction Methods   for System Degradation

**Authors:** Hao Guan, David Bates, Li Zhou

**Published:** 2025-06-20

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.17442v1](http://arxiv.org/pdf/2506.17442v1)

**Abstract:**

Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

---

### 432. Can Generalist Vision Language Models (VLMs) Rival Specialist Medical   VLMs? Benchmarking and Strategic Insights

**Authors:** Yuan Zhong, Ruinan Jin, Qi Dou, Xiaoxiao Li

**Published:** 2025-06-19

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.17337v2](http://arxiv.org/pdf/2506.17337v2)

**Abstract:**

Vision Language Models (VLMs) have shown promise in automating image
diagnosis and interpretation in clinical settings. However, developing
specialist medical VLMs requires substantial computational resources and
carefully curated datasets, and it remains unclear under which conditions
generalist and specialist medical VLMs each perform best. This study highlights
the complementary strengths of specialist medical and generalist VLMs.
Specialists remain valuable in modality-aligned use cases, but we find that
efficiently fine-tuned generalist VLMs can achieve comparable or even superior
performance in most tasks, particularly when transferring to unseen or rare OOD
medical modalities. These results suggest that generalist VLMs, rather than
being constrained by their lack of specialist medical pretraining, may offer a
scalable and cost-effective pathway for advancing clinical AI development.

---

### 433. DeVisE: Behavioral Testing of Medical Large Language Models

**Authors:** Camila Zurdo Tagliabue, Heloisa Oss Boll, Aykut Erdem, Erkut Erdem, Iacer Calixto

**Published:** 2025-06-18

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.15339v1](http://arxiv.org/pdf/2506.15339v1)

**Abstract:**

Large language models (LLMs) are increasingly used in clinical decision
support, yet current evaluation methods often fail to distinguish genuine
medical reasoning from superficial patterns. We introduce DeVisE (Demographics
and Vital signs Evaluation), a behavioral testing framework for probing
fine-grained clinical understanding. We construct a dataset of ICU discharge
notes from MIMIC-IV, generating both raw (real-world) and template-based
(synthetic) versions with controlled single-variable counterfactuals targeting
demographic (age, gender, ethnicity) and vital sign attributes. We evaluate
five LLMs spanning general-purpose and medically fine-tuned variants, under
both zero-shot and fine-tuned settings. We assess model behavior via (1)
input-level sensitivity - how counterfactuals alter the likelihood of a note;
and (2) downstream reasoning - how they affect predicted hospital
length-of-stay. Our results show that zero-shot models exhibit more coherent
counterfactual reasoning patterns, while fine-tuned models tend to be more
stable yet less responsive to clinically meaningful changes. Notably,
demographic factors subtly but consistently influence outputs, emphasizing the
importance of fairness-aware evaluation. This work highlights the utility of
behavioral testing in exposing the reasoning strategies of clinical LLMs and
informing the design of safer, more transparent medical AI systems.

---

### 434. One Size Fits None: Rethinking Fairness in Medical AI

**Authors:** Roland Roller, Michael Hahn, Ajay Madhavan Ravichandran, Bilgin Osmanodja, Florian Oetke, Zeineb Sassi, Aljoscha Burchardt, Klaus Netter, Klemens Budde, Anne Herrmann, Tobias Strapatsas, Peter Dabrock, Sebastian MÃ¶ller

**Published:** 2025-06-17

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.14400v1](http://arxiv.org/pdf/2506.14400v1)

**Abstract:**

Machine learning (ML) models are increasingly used to support clinical
decision-making. However, real-world medical datasets are often noisy,
incomplete, and imbalanced, leading to performance disparities across patient
subgroups. These differences raise fairness concerns, particularly when they
reinforce existing disadvantages for marginalized groups. In this work, we
analyze several medical prediction tasks and demonstrate how model performance
varies with patient characteristics. While ML models may demonstrate good
overall performance, we argue that subgroup-level evaluation is essential
before integrating them into clinical workflows. By conducting a performance
analysis at the subgroup level, differences can be clearly identified-allowing,
on the one hand, for performance disparities to be considered in clinical
practice, and on the other hand, for these insights to inform the responsible
development of more effective models. Thereby, our work contributes to a
practical discussion around the subgroup-sensitive development and deployment
of medical ML models and the interconnectedness of fairness and transparency.

---

### 435. Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware   Strategies for LLMs and VLMs

**Authors:** Gyutaek Oh, Seoyeon Kim, Sangjoon Park, Byung-Hoon Kim

**Published:** 2025-06-16

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.13102v1](http://arxiv.org/pdf/2506.13102v1)

**Abstract:**

Test-time scaling has recently emerged as a promising approach for enhancing
the reasoning capabilities of large language models or vision-language models
during inference. Although a variety of test-time scaling strategies have been
proposed, and interest in their application to the medical domain is growing,
many critical aspects remain underexplored, including their effectiveness for
vision-language models and the identification of optimal strategies for
different settings. In this paper, we conduct a comprehensive investigation of
test-time scaling in the medical domain. We evaluate its impact on both large
language models and vision-language models, considering factors such as model
size, inherent model characteristics, and task complexity. Finally, we assess
the robustness of these strategies under user-driven factors, such as
misleading information embedded in prompts. Our findings offer practical
guidelines for the effective use of test-time scaling in medical applications
and provide insights into how these strategies can be further refined to meet
the reliability and interpretability demands of the medical domain.

---

### 436. One Patient, Many Contexts: Scaling Medical AI with Contextual   Intelligence

**Authors:** Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo, Isaac S. Kohane, Marinka Zitnik

**Published:** 2025-06-11

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.10157v2](http://arxiv.org/pdf/2506.10157v2)

**Abstract:**

Medical AI, including clinical language models, vision-language models, and
multimodal health record models, already summarizes notes, answers questions,
and supports decisions. Their adaptation to new populations, specialties, or
care settings often relies on fine-tuning, prompting, or retrieval from
external knowledge bases. These strategies can scale poorly and risk contextual
errors: outputs that appear plausible but miss critical patient or situational
information. We envision context switching as a solution. Context switching
adjusts model reasoning at inference without retraining. Generative models can
tailor outputs to patient biology, care setting, or disease. Multimodal models
can reason on notes, laboratory results, imaging, and genomics, even when some
data are missing or delayed. Agent models can coordinate tools and roles based
on tasks and users. In each case, context switching enables medical AI to adapt
across specialties, populations, and geographies. It requires advances in data
design, model architectures, and evaluation frameworks, and establishes a
foundation for medical AI that scales to infinitely many contexts while
remaining reliable and suited to real-world care.

---

### 437. Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained   Reflection

**Authors:** Zongxian Yang, Jiayu Qian, Zegao Peng, Haoyu Zhang, Zhi-An Huang

**Published:** 2025-06-11

**Categories:** 

**PDF:** [http://arxiv.org/pdf/2506.13793v2](http://arxiv.org/pdf/2506.13793v2)

**Abstract:**

Large reasoning models have recently made significant strides in mathematical
and code reasoning, yet their success has not transferred smoothly to the
medical domain. While multiple factors contribute to this disparity, a critical
issue is the inadequate focus on the quality of intermediate reflection steps,
which is particularly crucial in high-stakes medical scenarios. To address this
challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical
\underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via
self-corrected \underline{\textbf{F}}ine-grained
ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought
approach to decompose medical questions into fine-grained reasoning paths,
quantitatively evaluating each step and its subsequent reflections. These
assessments enable automatic construction of direct preference optimization
data, reducing reliance on expensive expert annotations while guiding models to
identify and correct reasoning errors. Experimental results on the MedQA-USMLE
benchmark demonstrate Med-REFL achieves consistent improvements, with average
gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance
of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong
generalization capabilities and robustness across several challenging medical
question-answering datasets. Our work illustrates that prioritizing reflection
quality leads to more accurate and trustworthy reasoning in medical AI
applications. Checkpoints, code, and data can be found in
https://github.com/TianYin123/Med-REFL.

---

## SSRN Results (found 1)

### 1. N/A

**Authors:** N/A

**Link:** [https://papers.ssrn.com/sol3/results.cfm?txtKey_Words=artificial intelligence](https://papers.ssrn.com/sol3/results.cfm?txtKey_Words=artificial intelligence)

**Abstract:**

[Skip to main content](https://papers.ssrn.com/sol3/results.cfm?txtKey_Words=artificial%20intelligence#maincontent "Skip to content")

Sort by:




Abstract Title, A-ZAbstract Title, Z-ADownloads, AscendingDownloads, DescendingDate Posted, AscendingDate Posted, Descending

- **1**
- 2
- 3
- 4
- ...
- 200
- Next
- 200

Viewing: **1** \- **50** of **21,574** papers

1.




### [Financial Machine Learning](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4501707)

Number of pages: 159Posted: 13 Jul 2023

[Bryan T. Kelly](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=759326) and [Dacheng Xiu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=1101692)






























































































































---

