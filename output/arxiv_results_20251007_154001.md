# arXiv Search Results

**Search Query:** `cat:cs.AI OR cat:cs.LG OR cat:cs.CL`

**Date Range:** Papers published after 2025-08-08

**Generated:** 2025-10-07 15:40:01

**Total Papers Found:** 100

---

## 1. TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration

**Authors:** Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin Ding

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, cs.CG, math.AT, stat.ML, 55N31, 68T05, 62R40, 05C, 68R05, I.2.6; G.2.2; I.5.1

**PDF:** [http://arxiv.org/pdf/2510.05102v1](http://arxiv.org/pdf/2510.05102v1)

**Abstract:**

Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

---

## 2. Paper2Video: Automatic Video Generation from Scientific Papers

**Authors:** Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI, cs.CL, cs.MA, cs.MM

**PDF:** [http://arxiv.org/pdf/2510.05096v1](http://arxiv.org/pdf/2510.05096v1)

**Abstract:**

Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

---

## 3. From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models

**Authors:** Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05095v1](http://arxiv.org/pdf/2510.05095v1)

**Abstract:**

Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

---

## 4. Learning to Interpret Weight Differences in Language Models

**Authors:** Avichal Goel, Yoon Kim, Nir Shavit, Tony T. Wang

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05092v1](http://arxiv.org/pdf/2510.05092v1)

**Abstract:**

Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

---

## 5. Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models

**Authors:** Runchu Tian, Junxia Cui, Xueqiang Xu, Feng Yao, Jingbo Shang

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05090v1](http://arxiv.org/pdf/2510.05090v1)

**Abstract:**

Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

---

## 6. TeachLM: Post-Training LLMs for Education Using Authentic Learning Data

**Authors:** Janos Perczel, Jin Chow, Dorottya Demszky

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05087v1](http://arxiv.org/pdf/2510.05087v1)

**Abstract:**

The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

---

## 7. SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder

**Authors:** Ronen Kamenetsky, Sara Dorfman, Daniel Garibi, Roni Paiss, Or Patashnik, Daniel Cohen-Or

**Published:** 2025-10-06

**Categories:** cs.GR, cs.AI, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.05081v1](http://arxiv.org/pdf/2510.05081v1)

**Abstract:**

Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

---

## 8. MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis

**Authors:** Yangyang Wang, Tayo Fabusuyi

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05080v1](http://arxiv.org/pdf/2510.05080v1)

**Abstract:**

This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

---

## 9. Slm-mux: Orchestrating small language models for reasoning

**Authors:** Chenyu Wang, Zishen Wan, Hao Kang, Emma Chen, Zhiqiang Xie, Tushar Krishna, Vijay Janapa Reddi, Yilun Du

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05077v1](http://arxiv.org/pdf/2510.05077v1)

**Abstract:**

With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

---

## 10. ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning

**Authors:** Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan

**Published:** 2025-10-06

**Categories:** cs.RO, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05070v1](http://arxiv.org/pdf/2510.05070v1)

**Abstract:**

Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

---

## 11. SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs

**Authors:** Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05069v1](http://arxiv.org/pdf/2510.05069v1)

**Abstract:**

Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

---

## 12. Boomerang Distillation Enables Zero-Shot Model Size Interpolation

**Authors:** Sara Kangaslahti, Nihal V. Nayak, Jonathan Geuter, Marco Fumero, Francesco Locatello, David Alvarez-Melis

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05064v1](http://arxiv.org/pdf/2510.05064v1)

**Abstract:**

Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

---

## 13. ResCP: Reservoir Conformal Prediction for Time Series Forecasting

**Authors:** Roberto Neglia, Andrea Cini, Michael M. Bronstein, Filippo Maria Bianchi

**Published:** 2025-10-06

**Categories:** cs.LG, math.ST, stat.ML, stat.TH

**PDF:** [http://arxiv.org/pdf/2510.05060v1](http://arxiv.org/pdf/2510.05060v1)

**Abstract:**

Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

---

## 14. Staircase Streaming for Low-Latency Multi-Agent Inference

**Authors:** Junlin Wang, Jue Wang, Zhen, Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05059v1](http://arxiv.org/pdf/2510.05059v1)

**Abstract:**

Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

---

## 15. Modeling Student Learning with 3.8 Million Program Traces

**Authors:** Alexis Ross, Megha Srivastava, Jeremiah Blanchard, Jacob Andreas

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05056v1](http://arxiv.org/pdf/2510.05056v1)

**Abstract:**

As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

---

## 16. HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model

**Authors:** Peter Van Katwyk, Karianne J. Bergen

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05054v1](http://arxiv.org/pdf/2510.05054v1)

**Abstract:**

Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

---

## 17. Proactive defense against LLM Jailbreak

**Authors:** Weiliang Zhao, Jinjun Peng, Daniel Ben-Levi, Zhou Yu, Junfeng Yang

**Published:** 2025-10-06

**Categories:** cs.CR, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05052v1](http://arxiv.org/pdf/2510.05052v1)

**Abstract:**

The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

---

## 18. KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings

**Authors:** Ahmed Elhussein, Paul Meddeb, Abigail Newbury, Jeanne Mirone, Martin Stoll, Gamze Gursoy

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05049v1](http://arxiv.org/pdf/2510.05049v1)

**Abstract:**

Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

---

## 19. A Unified Optimization Framework for Multiclass Classification with Structured Hyperplane Arrangements

**Authors:** Víctor Blanco, Harshit Kothari, James Luedtke

**Published:** 2025-10-06

**Categories:** math.OC, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05047v1](http://arxiv.org/pdf/2510.05047v1)

**Abstract:**

In this paper, we propose a new mathematical optimization model for
multiclass classification based on arrangements of hyperplanes. Our approach
preserves the core support vector machine (SVM) paradigm of maximizing class
separation while minimizing misclassification errors, and it is computationally
more efficient than a previous formulation. We present a kernel-based extension
that allows it to construct nonlinear decision boundaries. Furthermore, we show
how the framework can naturally incorporate alternative geometric structures,
including classification trees, $\ell_p$-SVMs, and models with discrete feature
selection. To address large-scale instances, we develop a dynamic clustering
matheuristic that leverages the proposed MIP formulation. Extensive
computational experiments demonstrate the efficiency of the proposed model and
dynamic clustering heuristic, and we report competitive classification
performance on both synthetic datasets and real-world benchmarks from the UCI
Machine Learning Repository, comparing our method with state-of-the-art
implementations available in scikit-learn.

---

## 20. Look-ahead Reasoning with a Learned Model in Imperfect Information Games

**Authors:** Ondřej Kubíček, Viliam Lisý

**Published:** 2025-10-06

**Categories:** cs.AI, cs.GT

**PDF:** [http://arxiv.org/pdf/2510.05048v1](http://arxiv.org/pdf/2510.05048v1)

**Abstract:**

Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

---

## 21. COLE: a Comprehensive Benchmark for French Language Understanding Evaluation

**Authors:** David Beauchemin, Yan Tremblay, Mohamed Amine Youssef, Richard Khoury

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05046v1](http://arxiv.org/pdf/2510.05046v1)

**Abstract:**

To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

---

## 22. Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts

**Authors:** Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05040v1](http://arxiv.org/pdf/2510.05040v1)

**Abstract:**

Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

---

## 23. Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization

**Authors:** Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05038v1](http://arxiv.org/pdf/2510.05038v1)

**Abstract:**

Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

---

## 24. Graph-Aware Diffusion for Signal Generation

**Authors:** Sergio Rozada, Vimal K. B., Andrea Cavallo, Antonio G. Marques, Hadi Jamali-Rad, Elvin Isufi

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05036v1](http://arxiv.org/pdf/2510.05036v1)

**Abstract:**

We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

---

## 25. Causal Abstractions, Categorically Unified

**Authors:** Markus Englberger, Devendra Singh Dhami

**Published:** 2025-10-06

**Categories:** stat.ML, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05033v1](http://arxiv.org/pdf/2510.05033v1)

**Abstract:**

We present a categorical framework for relating causal models that represent
the same system at different levels of abstraction. We define a causal
abstraction as natural transformations between appropriate Markov functors,
which concisely consolidate desirable properties a causal abstraction should
exhibit. Our approach unifies and generalizes previously considered causal
abstractions, and we obtain categorical proofs and generalizations of existing
results on causal abstractions. Using string diagrammatical tools, we can
explicitly describe the graphs that serve as consistent abstractions of a
low-level graph under interventions. We discuss how methods from mechanistic
interpretability, such as circuit analysis and sparse autoencoders, fit within
our categorical framework. We also show how applying do-calculus on a
high-level graphical abstraction of an acyclic-directed mixed graph (ADMG),
when unobserved confounders are present, gives valid results on the low-level
graph, thus generalizing an earlier statement by Anand et al. (2023). We argue
that our framework is more suitable for modeling causal abstractions compared
to existing categorical frameworks. Finally, we discuss how notions such as
$\tau$-consistency and constructive $\tau$-abstractions can be recovered with
our framework.

---

## 26. A Set of Quebec-French Corpus of Regional Expressions and Terms

**Authors:** David Beauchemin, Yan Tremblay, Mohamed Amine Youssef, Richard Khoury

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05026v1](http://arxiv.org/pdf/2510.05026v1)

**Abstract:**

The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

---

## 27. Imperceptible Jailbreaking against Large Language Models

**Authors:** Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, Shu-Tao Xia, Tianyu Pang

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.CR

**PDF:** [http://arxiv.org/pdf/2510.05025v1](http://arxiv.org/pdf/2510.05025v1)

**Abstract:**

Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

---

## 28. Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment

**Authors:** Nevan Wichers, Aram Ebtekar, Ariana Azarbal, Victor Gillioz, Christine Ye, Emil Ryd, Neil Rathi, Henry Sleight, Alex Mallen, Fabien Roger, Samuel Marks

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05024v1](http://arxiv.org/pdf/2510.05024v1)

**Abstract:**

Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

---

## 29. Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective

**Authors:** Weixin Wang, Haoyang Zheng, Guang Lin, Wei Deng, Pan Xu

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, stat.ML

**PDF:** [http://arxiv.org/pdf/2510.05023v1](http://arxiv.org/pdf/2510.05023v1)

**Abstract:**

Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

---

## 30. Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad

**Authors:** Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun

**Published:** 2025-10-06

**Categories:** astro-ph.IM, cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.05016v1](http://arxiv.org/pdf/2510.05016v1)

**Abstract:**

While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

---

## 31. Think Then Embed: Generative Context Improves Multimodal Embedding

**Authors:** Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla, Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Qi Guo, Ser-Nam Lim, Aashu Singh, Xiangjun Fan

**Published:** 2025-10-06

**Categories:** cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05014v1](http://arxiv.org/pdf/2510.05014v1)

**Abstract:**

There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

---

## 32. Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration

**Authors:** Theodore Jerome Tinker, Kenji Doya, Jun Tani

**Published:** 2025-10-06

**Categories:** stat.ML, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05013v1](http://arxiv.org/pdf/2510.05013v1)

**Abstract:**

Human infants acquire language and action co-developmentally, achieving
remarkable generalization capabilities from only a minimal number of learning
examples. In contrast, recent large language models require exposure to
billions of training tokens to achieve such generalization. What mechanisms
underlie such efficient developmental learning in humans? This study addresses
this question through simulation experiments in which robots learn to perform
various actions corresponding to imperative sentences (e.g., \textit{push red
cube}) via trials of self-guided exploration. Our approach integrates the
active inference framework with reinforcement learning, enabling
curiosity-driven developmental learning. The simulations yielded several
nontrivial findings: i) Curiosity-driven exploration combined with motor noise
substantially outperforms learning without curiosity. ii) Simpler,
prerequisite-like actions emerge earlier in development, while more complex
actions involving these prerequisites develop later. iii) Rote pairing of
sentences and actions occurs before the emergence of compositional
generalization. iv) Generalization is drastically improved as the number of
compositional elements increases. These results shed light into possible
mechanisms underlying efficient co-developmental learning in infants and
provide computational parallels to findings in developmental psychology.

---

## 33. Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition

**Authors:** Koen Vellenga, H. Joe Steinhauer, Jonas Andersson, Anders Sjögren

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.05006v1](http://arxiv.org/pdf/2510.05006v1)

**Abstract:**

Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

---

## 34. Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning

**Authors:** Imran Mansha

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.05003v1](http://arxiv.org/pdf/2510.05003v1)

**Abstract:**

Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

---

## 35. Bridging Text and Video Generation: A Survey

**Authors:** Nilay Kumar, Priyansh Bhandari, G. Maragatham

**Published:** 2025-10-06

**Categories:** cs.GR, cs.AI, cs.CV

**PDF:** [http://arxiv.org/pdf/2510.04999v1](http://arxiv.org/pdf/2510.04999v1)

**Abstract:**

Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

---

## 36. AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis

**Authors:** Jiongchi Yu, Weipeng Jiang, Xiaoyu Zhang, Qiang Hu, Xiaofei Xie, Chao Shen

**Published:** 2025-10-06

**Categories:** cs.SE, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04997v1](http://arxiv.org/pdf/2510.04997v1)

**Abstract:**

Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

---

## 37. Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training

**Authors:** Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, cs.CL, stat.ML

**PDF:** [http://arxiv.org/pdf/2510.04996v1](http://arxiv.org/pdf/2510.04996v1)

**Abstract:**

Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

---

## 38. Power Transform Revisited: Numerically Stable, and Federated

**Authors:** Xuefeng Xu, Graham Cormode

**Published:** 2025-10-06

**Categories:** cs.LG, cs.NA, math.NA

**PDF:** [http://arxiv.org/pdf/2510.04995v1](http://arxiv.org/pdf/2510.04995v1)

**Abstract:**

Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

---

## 39. Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization

**Authors:** Kristi Topollai, Anna Choromanska

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04988v1](http://arxiv.org/pdf/2510.04988v1)

**Abstract:**

The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

---

## 40. AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives

**Authors:** Khalid Mehtab Khan, Anagha Kulkarni

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.CY, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04983v1](http://arxiv.org/pdf/2510.04983v1)

**Abstract:**

Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

---

## 41. LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game

**Authors:** Fangzhou Liang, Tianshi Zheng, Chunkit Chan, Yauwai Yim, Yangqiu Song

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04980v1](http://arxiv.org/pdf/2510.04980v1)

**Abstract:**

Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

---

## 42. Federated Computation of ROC and PR Curves

**Authors:** Xuefeng Xu, Graham Cormode

**Published:** 2025-10-06

**Categories:** cs.LG, cs.CR

**PDF:** [http://arxiv.org/pdf/2510.04979v1](http://arxiv.org/pdf/2510.04979v1)

**Abstract:**

Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

---

## 43. Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI

**Authors:** Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04978v1](http://arxiv.org/pdf/2510.04978v1)

**Abstract:**

The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

---

## 44. StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R

**Authors:** Allen Daniel Sunny

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04974v1](http://arxiv.org/pdf/2510.04974v1)

**Abstract:**

We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

---

## 45. Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent Random Fields

**Authors:** Nabarun Deb

**Published:** 2025-10-06

**Categories:** math.ST, cs.LG, math.PR, stat.TH, 82B20, 82B26

**PDF:** [http://arxiv.org/pdf/2510.04972v1](http://arxiv.org/pdf/2510.04972v1)

**Abstract:**

In this paper, we study fluctuations of conditionally centered statistics of
the form $$N^{-1/2}\sum_{i=1}^N
c_i(g(\sigma_i)-\mathbb{E}_N[g(\sigma_i)|\sigma_j,j\neq i])$$ where
$(\sigma_1,\ldots ,\sigma_N)$ are sampled from a dependent random field, and
$g$ is some bounded function. Our first main result shows that under weak
smoothness assumptions on the conditional means (which cover both sparse and
dense interactions), the above statistic converges to a Gaussian \emph{scale
mixture} with a random scale determined by a \emph{quadratic variance} and an
\emph{interaction component}. We also show that under appropriate
studentization, the limit becomes a pivotal Gaussian. We leverage this theory
to develop a general asymptotic framework for maximum pseudolikelihood (MPLE)
inference in dependent random fields. We apply our results to Ising models with
pairwise as well as higher-order interactions and exponential random graph
models (ERGMs). In particular, we obtain a joint central limit theorem for the
inverse temperature and magnetization parameters via the joint MPLE (to our
knowledge, the first such result in dense, irregular regimes), and we derive
conditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without
restricting to the ``sub-critical" region. Our proof is based on a method of
moments approach via combinatorial decision-tree pruning, which may be of
independent interest.

---

## 46. Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning

**Authors:** Marcel Wienöbst, Leonard Henckel, Sebastian Weichwald

**Published:** 2025-10-06

**Categories:** stat.ML, cs.AI, cs.LG, stat.ME

**PDF:** [http://arxiv.org/pdf/2510.04970v1](http://arxiv.org/pdf/2510.04970v1)

**Abstract:**

We present FLOP (Fast Learning of Order and Parents), a score-based causal
discovery algorithm for linear models. It pairs fast parent selection with
iterative Cholesky-based score updates, cutting run-times over prior
algorithms. This makes it feasible to fully embrace discrete search, enabling
iterated local search with principled order initialization to find graphs with
scores at or close to the global optimum. The resulting structures are highly
accurate across benchmarks, with near-perfect recovery in standard settings.
This performance calls for revisiting discrete search over graphs as a
reasonable approach to causal discovery.

---

## 47. ActiveMark: on watermarking of visual foundation models via massive activations

**Authors:** Anna Chistyakova, Mikhail Pautov

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04966v1](http://arxiv.org/pdf/2510.04966v1)

**Abstract:**

Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

---

## 48. MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling

**Authors:** Bi-Cheng Yan, Ming-Kang Tsai, Berlin Chen

**Published:** 2025-10-06

**Categories:** eess.AS, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04956v1](http://arxiv.org/pdf/2510.04956v1)

**Abstract:**

Computer-assisted pronunciation training (CAPT) manages to facilitate
second-language (L2) learners to practice pronunciation skills by offering
timely and instructive feedback. To examine pronunciation proficiency from
multiple facets, existing methods for CAPT broadly fall into two categories:
mispronunciation detection and diagnosis (MDD) as well as automatic
pronunciation assessment (APA). The former aims to pinpoint phonetic
pronunciation errors and provide diagnostic feedback, while the latter seeks
instead to quantify pronunciation proficiency pertaining to various aspects.
Despite the natural complementarity between MDD and APA, researchers and
practitioners, however, often treat them as independent tasks with disparate
modeling paradigms. In light of this, we in this paper first introduce MuFFIN,
a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical
Neural architecture, to jointly address the tasks of MDD and APA. To better
capture the nuanced distinctions between phonemes in the feature space, a novel
phoneme-contrastive ordinal regularization mechanism is then put forward to
optimize the proposed model to generate more phoneme-discriminative features
while factoring in the ordinality of the aspect scores. In addition, to address
the intricate data imbalance problem in MDD, we design a simple yet effective
training objective, which is specifically tailored to perturb the outputs of a
phoneme classifier with the phoneme-specific variations, so as to better render
the distribution of predicted phonemes meanwhile considering their
mispronunciation characteristics. A series of experiments conducted on the
Speechocean762 benchmark dataset demonstrates the efficacy of our method in
relation to several cutting-edge baselines, showing state-of-the-art
performance on both the APA and MDD tasks.

---

## 49. Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits

**Authors:** Ailiya Borjigin, Cong He

**Published:** 2025-10-06

**Categories:** cs.AI, cs.DC

**PDF:** [http://arxiv.org/pdf/2510.04952v1](http://arxiv.org/pdf/2510.04952v1)

**Abstract:**

We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

---

## 50. Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints

**Authors:** Jayanta Mandi, Marianne Defresne, Senne Berden, Tias Guns

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04951v1](http://arxiv.org/pdf/2510.04951v1)

**Abstract:**

When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

---

## 51. Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)

**Authors:** Om Dobariya, Akhil Kumar

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.LG, cs.NE, stat.ME

**PDF:** [http://arxiv.org/pdf/2510.04950v1](http://arxiv.org/pdf/2510.04950v1)

**Abstract:**

The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

---

## 52. Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion

**Authors:** Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04947v1](http://arxiv.org/pdf/2510.04947v1)

**Abstract:**

Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

---

## 53. A First Context-Free Grammar Applied to Nawatl Corpora Augmentation

**Authors:** Juan-José Guzmán-Landa, Juan-Manuel Torres-Moreno, Miguel Figueroa-Saavedra, Ligia Quintana-Torres, Martha-Lorena Avendaño-Garrido, Graham Ranger

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04945v1](http://arxiv.org/pdf/2510.04945v1)

**Abstract:**

In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

---

## 54. On Structured State-Space Duality

**Authors:** Jerry Yao-Chieh Hu, Xiwen Zhang, Weimin Wu, Han Liu

**Published:** 2025-10-06

**Categories:** cs.LG, cs.CL, cs.CV, stat.ML

**PDF:** [http://arxiv.org/pdf/2510.04944v1](http://arxiv.org/pdf/2510.04944v1)

**Abstract:**

Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

---

## 55. Unsupervised Active Learning via Natural Feature Progressive Framework

**Authors:** Yuxi Liu, Catherine Lalman, Yimin Yang

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04939v1](http://arxiv.org/pdf/2510.04939v1)

**Abstract:**

The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

---

## 56. ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures

**Authors:** Shiwen Qin, Alexander Auras, Shay B. Cohen, Elliot J. Crowley, Michael Moeller, Linus Ericsson, Jovita Lukasik

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04938v1](http://arxiv.org/pdf/2510.04938v1)

**Abstract:**

Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

---

## 57. MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning

**Authors:** Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang

**Published:** 2025-10-06

**Categories:** cs.AI, cs.CL, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04935v1](http://arxiv.org/pdf/2510.04935v1)

**Abstract:**

Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

---

## 58. AURA Score: A Metric For Holistic Audio Question Answering Evaluation

**Authors:** Satvik Dixit, Soham Deshmukh, Bhiksha Raj

**Published:** 2025-10-06

**Categories:** eess.AS, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04934v1](http://arxiv.org/pdf/2510.04934v1)

**Abstract:**

Audio Question Answering (AQA) is a key task for evaluating Audio-Language
Models (ALMs), yet assessing open-ended responses remains challenging. Existing
metrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from
NLP and audio captioning, rely on surface similarity and fail to account for
question context, reasoning, and partial correctness. To address the gap in
literature, we make three contributions in this work. First, we introduce
AQEval to enable systematic benchmarking of AQA metrics. It is the first
benchmark of its kind, consisting of 10k model responses annotated by multiple
humans for their correctness and relevance. Second, we conduct a comprehensive
analysis of existing AQA metrics on AQEval, highlighting weak correlation with
human judgment, especially for longer answers. Third, we propose a new metric -
AURA score, to better evaluate open-ended model responses. On AQEval, AURA
achieves state-of-the-art correlation with human ratings, significantly
outperforming all baselines. Through this work, we aim to highlight the
limitations of current AQA evaluation methods and motivate better metrics. We
release both the AQEval benchmark and the AURA metric to support future
research in holistic AQA evaluation.

---

## 59. The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models

**Authors:** Amir Hameed Mir

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.IT, cs.LG, cs.NE, math.IT, 68T50, 68T07, 62H30, I.2.7; I.2.6; F.2.2; H.3.3

**PDF:** [http://arxiv.org/pdf/2510.04933v1](http://arxiv.org/pdf/2510.04933v1)

**Abstract:**

Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

---

## 60. Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking

**Authors:** Ali Saheb Pasand, Elvis Dohmatob

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04930v1](http://arxiv.org/pdf/2510.04930v1)

**Abstract:**

Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

---

## 61. Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data

**Authors:** Usman Akram, Yiyue Chen, Haris Vikalo

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI, eess.SP

**PDF:** [http://arxiv.org/pdf/2510.04927v1](http://arxiv.org/pdf/2510.04927v1)

**Abstract:**

Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

---

## 62. Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification

**Authors:** Eyal Cohen, Christophe Denis, Mohamed Hebiri

**Published:** 2025-10-06

**Categories:** stat.ML, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04926v1](http://arxiv.org/pdf/2510.04926v1)

**Abstract:**

Set-valued classification is used in multiclass settings where confusion
between classes can occur and lead to misleading predictions. However, its
application may amplify discriminatory bias motivating the development of
set-valued approaches under fairness constraints. In this paper, we address the
problem of set-valued classification under demographic parity and expected size
constraints. We propose two complementary strategies: an oracle-based method
that minimizes classification risk while satisfying both constraints, and a
computationally efficient proxy that prioritizes constraint satisfaction. For
both strategies, we derive closed-form expressions for the (optimal) fair
set-valued classifiers and use these to build plug-in, data-driven procedures
for empirical predictions. We establish distribution-free convergence rates for
violations of the size and fairness constraints for both methods, and under
mild assumptions we also provide excess-risk bounds for the oracle-based
approach. Empirical results demonstrate the effectiveness of both strategies
and highlight the efficiency of our proxy method.

---

## 63. REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis

**Authors:** Alec K. Peltekian, Halil Ertugrul Aktas, Gorkem Durak, Kevin Grudzinski, Bradford C. Bemiss, Carrie Richardson, Jane E. Dematte, G. R. Scott Budinger, Anthony J. Esposito, Alexander Misharin, Alok Choudhary, Ankit Agrawal, Ulas Bagci

**Published:** 2025-10-06

**Categories:** cs.CV, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04923v1](http://arxiv.org/pdf/2510.04923v1)

**Abstract:**

Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

---

## 64. Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment

**Authors:** Davood Rafiei, Morgan Lindsay Heisler, Weiwei Zhang, Mohammadreza Pourreza, Yong Zhang

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.DB

**PDF:** [http://arxiv.org/pdf/2510.04919v1](http://arxiv.org/pdf/2510.04919v1)

**Abstract:**

Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

---

## 65. Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context

**Authors:** Ngeyen Yinkfu, Sunday Nwovu, Jonathan Kayizzi, Angelique Uwamahoro

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04912v1](http://arxiv.org/pdf/2510.04912v1)

**Abstract:**

In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

---

## 66. Glocal Information Bottleneck for Time Series Imputation

**Authors:** Jie Yang, Kexin Zhang, Guibin Zhang, Philip S. Yu, Kaize Ding

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04910v1](http://arxiv.org/pdf/2510.04910v1)

**Abstract:**

Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

---

## 67. How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning

**Authors:** Haotian Gao, Zheng Dong, Jiawei Yong, Shintaro Fukushima, Kenjiro Taura, Renhe Jiang

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04908v1](http://arxiv.org/pdf/2510.04908v1)

**Abstract:**

Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

---

## 68. Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches

**Authors:** Yicheng Tao, Yao Qin, Yepang Liu

**Published:** 2025-10-06

**Categories:** cs.SE, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04905v1](http://arxiv.org/pdf/2510.04905v1)

**Abstract:**

Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

---

## 69. DP-HYPE: Distributed Differentially Private Hyperparameter Search

**Authors:** Johannes Liebenow, Thorsten Peinemann, Esfandiar Mohammadi

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04902v1](http://arxiv.org/pdf/2510.04902v1)

**Abstract:**

The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

---

## 70. Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects

**Authors:** Jonathan Colaço Carr, Qinyi Sun, Cameron Allen

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04901v1](http://arxiv.org/pdf/2510.04901v1)

**Abstract:**

Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

---

## 71. Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models

**Authors:** Nick Janßen, Melanie Schaller, Bodo Rosenhahn

**Published:** 2025-10-06

**Categories:** cs.LG, cs.SY, eess.SY

**PDF:** [http://arxiv.org/pdf/2510.04900v1](http://arxiv.org/pdf/2510.04900v1)

**Abstract:**

Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

---

## 72. Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding

**Authors:** Keane Ong, Wei Dai, Carol Li, Dewei Feng, Hengzhi Li, Jingyao Wu, Jiaee Cheong, Rui Mao, Gianmarco Mengaldo, Erik Cambria, Paul Pu Liang

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04899v1](http://arxiv.org/pdf/2510.04899v1)

**Abstract:**

Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

---

## 73. HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks

**Authors:** Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson

**Published:** 2025-10-06

**Categories:** cs.RO, cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04898v1](http://arxiv.org/pdf/2510.04898v1)

**Abstract:**

Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

---

## 74. SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests

**Authors:** Punya Syon Pandey, Hai Son Le, Devansh Bhardwaj, Rada Mihalcea, Zhijing Jin

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04891v1](http://arxiv.org/pdf/2510.04891v1)

**Abstract:**

Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

---

## 75. Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models

**Authors:** Alina Ermilova, Dmitrii Kornilov, Sofia Samoilova, Ekaterina Laptenkova, Anastasia Kolesnikova, Ekaterina Podplutova, Senotrusova Sofya, Maksim G. Sharaev

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04888v1](http://arxiv.org/pdf/2510.04888v1)

**Abstract:**

Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

---

## 76. Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution

**Authors:** Adi Banerjee, Anirudh Nair, Tarik Borogovac

**Published:** 2025-10-06

**Categories:** cs.AI, cs.MA

**PDF:** [http://arxiv.org/pdf/2510.04886v1](http://arxiv.org/pdf/2510.04886v1)

**Abstract:**

Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

---

## 77. RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection

**Authors:** Yuxin Wen, Arman Zharmagambetov, Ivan Evtimov, Narine Kokhlikyan, Tom Goldstein, Kamalika Chaudhuri, Chuan Guo

**Published:** 2025-10-06

**Categories:** cs.CR, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04885v1](http://arxiv.org/pdf/2510.04885v1)

**Abstract:**

Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

---

## 78. CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery

**Authors:** Nathan Shankar, Pawel Ladosz, Hujun Yin

**Published:** 2025-10-06

**Categories:** cs.RO, cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04883v1](http://arxiv.org/pdf/2510.04883v1)

**Abstract:**

This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

---

## 79. Flow-Matching Based Refiner for Molecular Conformer Generation

**Authors:** Xiangyang Xu, Hongyang Gao

**Published:** 2025-10-06

**Categories:** cs.LG, q-bio.QM

**PDF:** [http://arxiv.org/pdf/2510.04878v1](http://arxiv.org/pdf/2510.04878v1)

**Abstract:**

Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

---

## 80. BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping

**Authors:** Hayat Rajani, Valerio Franchi, Borja Martinez-Clavel Valles, Raimon Ramos, Rafael Garcia, Nuno Gracias

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG, I.2.6; I.4.6; I.5.1; I.5.4

**PDF:** [http://arxiv.org/pdf/2510.04876v1](http://arxiv.org/pdf/2510.04876v1)

**Abstract:**

Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

---

## 81. Less is More: Recursive Reasoning with Tiny Networks

**Authors:** Alexia Jolicoeur-Martineau

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04871v1](http://arxiv.org/pdf/2510.04871v1)

**Abstract:**

Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

---

## 82. Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing

**Authors:** Seyed Soroush Karimi Madahi, Kenneth Bruninx, Bert Claessens, Chris Develder

**Published:** 2025-10-06

**Categories:** eess.SY, cs.AI, cs.SY

**PDF:** [http://arxiv.org/pdf/2510.04868v1](http://arxiv.org/pdf/2510.04868v1)

**Abstract:**

In Europe, profit-seeking balance responsible parties can deviate in real
time from their day-ahead nominations to assist transmission system operators
in maintaining the supply-demand balance. Model predictive control (MPC)
strategies to exploit these implicit balancing strategies capture arbitrage
opportunities, but fail to accurately capture the price-formation process in
the European imbalance markets and face high computational costs. Model-free
reinforcement learning (RL) methods are fast to execute, but require
data-intensive training and usually rely on real-time and historical data for
decision-making. This paper proposes an MPC-guided RL method that combines the
complementary strengths of both MPC and RL. The proposed method can effectively
incorporate forecasts into the decision-making process (as in MPC), while
maintaining the fast inference capability of RL. The performance of the
proposed method is evaluated on the implicit balancing battery control problem
using Belgian balancing data from 2023. First, we analyze the performance of
the standalone state-of-the-art RL and MPC methods from various angles, to
highlight their individual strengths and limitations. Next, we show an
arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and
54.36%, compared to standalone RL and MPC.

---

## 83. Video Game Level Design as a Multi-Agent Reinforcement Learning Problem

**Authors:** Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius

**Published:** 2025-10-06

**Categories:** cs.AI, cs.LG, cs.MA, cs.NE

**PDF:** [http://arxiv.org/pdf/2510.04862v1](http://arxiv.org/pdf/2510.04862v1)

**Abstract:**

Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

---

## 84. A Clinical-grade Universal Foundation Model for Intraoperative Pathology

**Authors:** Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04861v1](http://arxiv.org/pdf/2510.04861v1)

**Abstract:**

Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

---

## 85. Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails

**Authors:** Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04860v1](http://arxiv.org/pdf/2510.04860v1)

**Abstract:**

As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

---

## 86. ERDE: Entropy-Regularized Distillation for Early-exit

**Authors:** Martial Guidez, Stefan Duffner, Yannick Alpou, Oscar Röth, Christophe Garcia

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04856v1](http://arxiv.org/pdf/2510.04856v1)

**Abstract:**

Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

---

## 87. Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders

**Authors:** Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni

**Published:** 2025-10-06

**Categories:** cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04855v1](http://arxiv.org/pdf/2510.04855v1)

**Abstract:**

Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

---

## 88. FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration

**Authors:** Victor May, Diganta Misra, Yanqi Luo, Anjali Sridhar, Justine Gehring, Silvio Soares Ribeiro Junior

**Published:** 2025-10-06

**Categories:** cs.SE, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04852v1](http://arxiv.org/pdf/2510.04852v1)

**Abstract:**

AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

---

## 89. LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation

**Authors:** Dongge Han, Camille Couturier, Daniel Madrigal Diaz, Xuchao Zhang, Victor Rühle, Saravan Rajmohan

**Published:** 2025-10-06

**Categories:** cs.AI, cs.LG, cs.MA

**PDF:** [http://arxiv.org/pdf/2510.04851v1](http://arxiv.org/pdf/2510.04851v1)

**Abstract:**

We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

---

## 90. Detecting Distillation Data from Reasoning Models

**Authors:** Hengxiang Zhang, Hyeong Kyu Choi, Yixuan Li, Hongxin Wei

**Published:** 2025-10-06

**Categories:** cs.CL, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04850v1](http://arxiv.org/pdf/2510.04850v1)

**Abstract:**

Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

---

## 91. When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA

**Authors:** Elisei Rykov, Kseniia Petrushina, Maksim Savkin, Valerii Olisov, Artem Vazhentsev, Kseniia Titova, Alexander Panchenko, Vasily Konovalov, Julia Belikova

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04849v1](http://arxiv.org/pdf/2510.04849v1)

**Abstract:**

Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

---

## 92. Instability in Downstream Task Performance During LLM Pretraining

**Authors:** Yuto Nishida, Masaru Isonuma, Yusuke Oda

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04848v1](http://arxiv.org/pdf/2510.04848v1)

**Abstract:**

When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

---

## 93. Distributionally Robust Causal Abstractions

**Authors:** Yorgos Felekis, Theodoros Damoulas, Paris Giampouras

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04842v1](http://arxiv.org/pdf/2510.04842v1)

**Abstract:**

Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

---

## 94. Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation

**Authors:** Muquan Li, Hang Gou, Dongyang Zhang, Shuang Liang, Xiurui Xie, Deqiang Ouyang, Ke Qin

**Published:** 2025-10-06

**Categories:** cs.CV, cs.LG

**PDF:** [http://arxiv.org/pdf/2510.04838v1](http://arxiv.org/pdf/2510.04838v1)

**Abstract:**

The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

---

## 95. Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study

**Authors:** Guillaume Godin

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04837v1](http://arxiv.org/pdf/2510.04837v1)

**Abstract:**

Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

---

## 96. On the Hardness of Learning Regular Expressions

**Authors:** Idan Attias, Lev Reyzin, Nathan Srebro, Gal Vardi

**Published:** 2025-10-06

**Categories:** cs.LG, cs.CC

**PDF:** [http://arxiv.org/pdf/2510.04834v1](http://arxiv.org/pdf/2510.04834v1)

**Abstract:**

Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

---

## 97. How I Built ASR for Endangered Languages with a Spoken Dictionary

**Authors:** Christopher Bartley, Anton Ragni

**Published:** 2025-10-06

**Categories:** cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04832v1](http://arxiv.org/pdf/2510.04832v1)

**Abstract:**

Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

---

## 98. Visual Representations inside the Language Model

**Authors:** Benlin Liu, Amita Kamath, Madeleine Grunde-McLaughlin, Winson Han, Ranjay Krishna

**Published:** 2025-10-06

**Categories:** cs.CV, cs.CL

**PDF:** [http://arxiv.org/pdf/2510.04819v1](http://arxiv.org/pdf/2510.04819v1)

**Abstract:**

Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

---

## 99. Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning

**Authors:** Abhinav Madahar

**Published:** 2025-10-06

**Categories:** cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04817v1](http://arxiv.org/pdf/2510.04817v1)

**Abstract:**

Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

---

## 100. On Predicting Post-Click Conversion Rate via Counterfactual Inference

**Authors:** Junhyung Ahn, Sanghack Lee

**Published:** 2025-10-06

**Categories:** cs.LG, cs.AI

**PDF:** [http://arxiv.org/pdf/2510.04816v1](http://arxiv.org/pdf/2510.04816v1)

**Abstract:**

Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

---

